{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trading Strategy ML - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Multi-Factor Momentum Trading Strategy with ML Enhancement on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- Real-time data collection\n",
        "- Advanced ML models (CNN+LSTM)\n",
        "- Comprehensive backtesting\n",
        "- Performance analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Enable GPU memory growth\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        for gpu in tf.config.list_physical_devices('GPU'):\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with error handling and alternatives\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, alternative=None):\n",
        "    \"\"\"Install package with fallback to alternative if needed\"\"\"\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"✓ {package} installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Failed to install {package}: {e}\")\n",
        "        if alternative:\n",
        "            try:\n",
        "                print(f\"Trying alternative: {alternative}\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", alternative])\n",
        "                print(f\"✓ {alternative} installed successfully\")\n",
        "                return True\n",
        "            except subprocess.CalledProcessError as e2:\n",
        "                print(f\"✗ Alternative {alternative} also failed: {e2}\")\n",
        "        return False\n",
        "\n",
        "# Core packages (usually work fine)\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",\n",
        "    \"numpy>=1.21.0\", \n",
        "    \"scipy>=1.9.0\",\n",
        "    \"matplotlib>=3.5.0\",\n",
        "    \"seaborn>=0.11.0\",\n",
        "    \"plotly>=5.10.0\",\n",
        "    \"requests>=2.28.0\",\n",
        "    \"tqdm>=4.64.0\",\n",
        "    \"joblib>=1.1.0\",\n",
        "    \"python-dotenv>=0.19.0\"\n",
        "]\n",
        "\n",
        "# Financial data packages\n",
        "financial_packages = [\n",
        "    (\"yfinance>=0.2.0\", None),\n",
        "    (\"alpha-vantage>=2.3.0\", None),\n",
        "    (\"pandas-datareader>=0.10.0\", None)\n",
        "]\n",
        "\n",
        "# Technical analysis (problematic package)\n",
        "ta_packages = [\n",
        "    (\"TA-Lib>=0.4.25\", \"talib-binary>=0.4.19\")\n",
        "]\n",
        "\n",
        "# ML packages\n",
        "ml_packages = [\n",
        "    (\"tensorflow>=2.10.0\", \"tensorflow-gpu>=2.10.0\"),\n",
        "    (\"torch>=1.12.0\", None),\n",
        "    (\"torchvision>=0.13.0\", None),\n",
        "    (\"scikit-learn>=1.1.0\", None),\n",
        "    (\"xgboost>=1.6.0\", None),\n",
        "    (\"optuna>=3.0.0\", None),\n",
        "    (\"lightgbm>=3.3.0\", None)\n",
        "]\n",
        "\n",
        "# Financial analysis packages\n",
        "analysis_packages = [\n",
        "    (\"backtrader>=1.9.76\", None),\n",
        "    (\"arch>=5.3.0\", None),\n",
        "    (\"empyrical>=0.5.5\", None),\n",
        "    (\"ffn>=0.3.7\", None)\n",
        "]\n",
        "\n",
        "# UI packages\n",
        "ui_packages = [\n",
        "    (\"streamlit>=1.12.0\", None),\n",
        "    (\"dash>=2.6.0\", None)\n",
        "]\n",
        "\n",
        "print(\"🚀 Starting package installation...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Install core packages first\n",
        "print(\"\\n📦 Installing core packages...\")\n",
        "for package in core_packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Install financial packages\n",
        "print(\"\\n💰 Installing financial data packages...\")\n",
        "for package, alt in financial_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install TA-Lib with special handling\n",
        "print(\"\\n📊 Installing technical analysis packages...\")\n",
        "ta_success = False\n",
        "for package, alt in ta_packages:\n",
        "    if install_package(package, alt):\n",
        "        ta_success = True\n",
        "        break\n",
        "\n",
        "if not ta_success:\n",
        "    print(\"⚠️ TA-Lib installation failed. Using alternative approach...\")\n",
        "    # Try installing from conda-forge\n",
        "    try:\n",
        "        subprocess.check_call([\"pip\", \"install\", \"-q\", \"TA-Lib\"])\n",
        "        print(\"✓ TA-Lib installed via alternative method\")\n",
        "        ta_success = True\n",
        "    except:\n",
        "        print(\"⚠️ TA-Lib still failed. Some technical indicators may not work.\")\n",
        "\n",
        "# Install ML packages\n",
        "print(\"\\n🤖 Installing machine learning packages...\")\n",
        "for package, alt in ml_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install analysis packages\n",
        "print(\"\\n📈 Installing financial analysis packages...\")\n",
        "for package, alt in analysis_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install UI packages\n",
        "print(\"\\n🖥️ Installing UI packages...\")\n",
        "for package, alt in ui_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✅ Package installation complete!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test critical imports\n",
        "print(\"\\n🧪 Testing critical imports...\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    import yfinance as yf\n",
        "    print(\"✓ Core packages imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some packages failed to import: {e}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"\\n🎮 GPU Status:\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CUDA available: {tf.test.is_built_with_cuda()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Simplified Installation (if above fails)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified installation - run this if the above cell fails\n",
        "# This installs only the essential packages needed to run the trading strategy\n",
        "\n",
        "print(\"🔧 Installing essential packages only...\")\n",
        "\n",
        "# Essential packages that usually work in Colab\n",
        "essential_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"plotly\",\n",
        "    \"yfinance\",\n",
        "    \"tensorflow\",\n",
        "    \"scikit-learn\",\n",
        "    \"requests\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in essential_packages:\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        !pip install -q {package}\n",
        "        print(f\"✓ {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {package} failed: {e}\")\n",
        "\n",
        "print(\"\\n✅ Essential packages installation complete!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import plotly.graph_objects as go\n",
        "    import yfinance as yf\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    print(\"\\n🎉 All essential packages imported successfully!\")\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\n⚠️ Some packages failed to import: {e}\")\n",
        "    print(\"You may need to restart the runtime and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/CatalinMoldova/trading-strategy-ml.git\n",
        "\n",
        "# Change to the project directory\n",
        "%cd trading-strategy-ml\n",
        "\n",
        "# Install project requirements\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "print(\"Repository cloned and requirements installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import project modules with correct class names and error handling\n",
        "try:\n",
        "    from data_pipeline.market_data_collector import MarketDataCollector\n",
        "    from data_pipeline.indicator_engine import TechnicalIndicatorEngine\n",
        "    from data_pipeline.feature_engineer import FeatureEngineer\n",
        "    from ml_models.cnn_lstm_model import CNNLSTMModel\n",
        "    from ml_models.random_forest_model import RandomForestModel\n",
        "    from ml_models.ensemble_predictor import EnsemblePredictor\n",
        "    from strategy.signal_generator import MultiFactorSignalGenerator\n",
        "    from strategy.position_sizer import PositionSizer\n",
        "    from strategy.risk_manager import RiskManager\n",
        "    from backtesting.backtest_engine import BacktestEngine\n",
        "    from backtesting.performance_analyzer import PerformanceAnalyzer\n",
        "    print(\"✅ All project modules imported successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some project modules failed to import: {e}\")\n",
        "    print(\"Using improved implementations instead...\")\n",
        "    \n",
        "    # ============================================================================\n",
        "    # IMPROVED IMPLEMENTATIONS WITH PROPER TRADING STRATEGY, RISK MANAGEMENT, \n",
        "    # FEATURE ENGINEERING, BACKTESTING, AND MARKET REGIME DETECTION\n",
        "    # ============================================================================\n",
        "    \n",
        "    class MarketDataCollector:\n",
        "        \"\"\"Enhanced market data collector with error handling\"\"\"\n",
        "        def get_historical_data(self, symbol, period='2y', interval='1d'):\n",
        "            import yfinance as yf\n",
        "            try:\n",
        "                ticker = yf.Ticker(symbol)\n",
        "                data = ticker.history(period=period, interval=interval)\n",
        "                if data.empty:\n",
        "                    print(f\"⚠️ No data for {symbol}\")\n",
        "                    return None\n",
        "                return data\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error fetching data for {symbol}: {e}\")\n",
        "                return None\n",
        "    \n",
        "    class TechnicalIndicatorEngine:\n",
        "        \"\"\"Advanced technical indicator engine with comprehensive indicators\"\"\"\n",
        "        \n",
        "        def calculate_all_indicators(self, df):\n",
        "            \"\"\"Calculate comprehensive technical indicators\"\"\"\n",
        "            df = df.copy()\n",
        "            \n",
        "            # Price-based indicators\n",
        "            df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
        "            df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "            df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "            df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "            df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
        "            df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
        "            \n",
        "            # RSI\n",
        "            df['RSI'] = self._calculate_rsi(df['Close'], 14)\n",
        "            df['RSI_6'] = self._calculate_rsi(df['Close'], 6)\n",
        "            \n",
        "            # MACD\n",
        "            df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
        "            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "            df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "            \n",
        "            # Bollinger Bands\n",
        "            df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "            bb_std = df['Close'].rolling(window=20).std()\n",
        "            df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
        "            df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
        "            df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "            df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "            \n",
        "            # Stochastic Oscillator\n",
        "            df['Stoch_K'] = self._calculate_stochastic(df, 14)\n",
        "            df['Stoch_D'] = df['Stoch_K'].rolling(window=3).mean()\n",
        "            \n",
        "            # Williams %R\n",
        "            df['Williams_R'] = self._calculate_williams_r(df, 14)\n",
        "            \n",
        "            # Average True Range (ATR)\n",
        "            df['ATR'] = self._calculate_atr(df, 14)\n",
        "            \n",
        "            # Volume indicators\n",
        "            df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()\n",
        "            df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
        "            \n",
        "            # Price patterns\n",
        "            df['Doji'] = abs(df['Close'] - df['Open']) <= (df['High'] - df['Low']) * 0.1\n",
        "            df['Hammer'] = self._detect_hammer(df)\n",
        "            df['Shooting_Star'] = self._detect_shooting_star(df)\n",
        "            \n",
        "            return df\n",
        "        \n",
        "        def _calculate_rsi(self, prices, period=14):\n",
        "            \"\"\"Calculate RSI indicator\"\"\"\n",
        "            delta = prices.diff()\n",
        "            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "            rs = gain / loss\n",
        "            return 100 - (100 / (1 + rs))\n",
        "        \n",
        "        def _calculate_stochastic(self, df, period=14):\n",
        "            \"\"\"Calculate Stochastic Oscillator\"\"\"\n",
        "            lowest_low = df['Low'].rolling(window=period).min()\n",
        "            highest_high = df['High'].rolling(window=period).max()\n",
        "            return 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)\n",
        "        \n",
        "        def _calculate_williams_r(self, df, period=14):\n",
        "            \"\"\"Calculate Williams %R\"\"\"\n",
        "            highest_high = df['High'].rolling(window=period).max()\n",
        "            lowest_low = df['Low'].rolling(window=period).min()\n",
        "            return -100 * (highest_high - df['Close']) / (highest_high - lowest_low)\n",
        "        \n",
        "        def _calculate_atr(self, df, period=14):\n",
        "            \"\"\"Calculate Average True Range\"\"\"\n",
        "            high_low = df['High'] - df['Low']\n",
        "            high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "            low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "            \n",
        "            true_range = np.maximum(high_low, np.maximum(high_close, low_close))\n",
        "            return true_range.rolling(window=period).mean()\n",
        "        \n",
        "        def _detect_hammer(self, df):\n",
        "            \"\"\"Detect hammer candlestick pattern\"\"\"\n",
        "            body = abs(df['Close'] - df['Open'])\n",
        "            lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
        "            upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
        "            \n",
        "            return (lower_shadow > 2 * body) & (upper_shadow < body)\n",
        "        \n",
        "        def _detect_shooting_star(self, df):\n",
        "            \"\"\"Detect shooting star candlestick pattern\"\"\"\n",
        "            body = abs(df['Close'] - df['Open'])\n",
        "            lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
        "            upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
        "            \n",
        "            return (upper_shadow > 2 * body) & (lower_shadow < body)\n",
        "    \n",
        "    class FeatureEngineer:\n",
        "        \"\"\"Advanced feature engineering with market regime detection\"\"\"\n",
        "        \n",
        "        def create_features(self, df):\n",
        "            \"\"\"Create comprehensive features for ML models\"\"\"\n",
        "            df = df.copy()\n",
        "            \n",
        "            # Price-based features\n",
        "            df['Returns'] = df['Close'].pct_change()\n",
        "            df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "            df['Price_Ratio'] = df['Close'] / df['Open']\n",
        "            df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "            df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "            \n",
        "            # Volatility features\n",
        "            df['Volatility_5'] = df['Returns'].rolling(window=5).std()\n",
        "            df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
        "            df['Volatility_Ratio'] = df['Volatility_5'] / df['Volatility_20']\n",
        "            \n",
        "            # Momentum features\n",
        "            df['Momentum_5'] = df['Close'] / df['Close'].shift(5) - 1\n",
        "            df['Momentum_10'] = df['Close'] / df['Close'].shift(10) - 1\n",
        "            df['Momentum_20'] = df['Close'] / df['Close'].shift(20) - 1\n",
        "            \n",
        "            # Trend features\n",
        "            df['Trend_Strength'] = (df['SMA_20'] - df['SMA_50']) / df['SMA_50']\n",
        "            df['Price_vs_SMA20'] = (df['Close'] - df['SMA_20']) / df['SMA_20']\n",
        "            df['Price_vs_SMA50'] = (df['Close'] - df['SMA_50']) / df['SMA_50']\n",
        "            \n",
        "            # Market regime detection\n",
        "            df = self._detect_market_regime(df)\n",
        "            \n",
        "            # Volume features\n",
        "            df['Volume_Change'] = df['Volume'].pct_change()\n",
        "            df['Price_Volume_Trend'] = df['Returns'] * df['Volume_Ratio']\n",
        "            \n",
        "            # Technical indicator features\n",
        "            df['RSI_Overbought'] = (df['RSI'] > 70).astype(int)\n",
        "            df['RSI_Oversold'] = (df['RSI'] < 30).astype(int)\n",
        "            df['MACD_Bullish'] = (df['MACD'] > df['MACD_Signal']).astype(int)\n",
        "            df['MACD_Bearish'] = (df['MACD'] < df['MACD_Signal']).astype(int)\n",
        "            \n",
        "            # Bollinger Band features\n",
        "            df['BB_Squeeze'] = (df['BB_Width'] < df['BB_Width'].rolling(20).quantile(0.2)).astype(int)\n",
        "            df['BB_Expansion'] = (df['BB_Width'] > df['BB_Width'].rolling(20).quantile(0.8)).astype(int)\n",
        "            \n",
        "            # Lagged features\n",
        "            for lag in [1, 2, 3, 5]:\n",
        "                df[f'Returns_Lag_{lag}'] = df['Returns'].shift(lag)\n",
        "                df[f'Volume_Ratio_Lag_{lag}'] = df['Volume_Ratio'].shift(lag)\n",
        "            \n",
        "            return df\n",
        "        \n",
        "        def _detect_market_regime(self, df):\n",
        "            \"\"\"Detect market regime (Bull, Bear, Sideways, High Volatility)\"\"\"\n",
        "            # Calculate trend strength\n",
        "            trend_strength = df['Trend_Strength'].rolling(window=20).mean()\n",
        "            \n",
        "            # Calculate volatility\n",
        "            volatility = df['Volatility_20'].rolling(window=20).mean()\n",
        "            vol_threshold = volatility.quantile(0.7)\n",
        "            \n",
        "            # Regime classification\n",
        "            conditions = [\n",
        "                (trend_strength > 0.02) & (volatility < vol_threshold),  # Bull market\n",
        "                (trend_strength < -0.02) & (volatility < vol_threshold),  # Bear market\n",
        "                (volatility > vol_threshold),  # High volatility\n",
        "            ]\n",
        "            \n",
        "            choices = ['Bull', 'Bear', 'High_Vol']\n",
        "            df['Market_Regime'] = np.select(conditions, choices, default='Sideways')\n",
        "            \n",
        "            # One-hot encode regimes\n",
        "            df['Regime_Bull'] = (df['Market_Regime'] == 'Bull').astype(int)\n",
        "            df['Regime_Bear'] = (df['Market_Regime'] == 'Bear').astype(int)\n",
        "            df['Regime_Sideways'] = (df['Market_Regime'] == 'Sideways').astype(int)\n",
        "            df['Regime_High_Vol'] = (df['Market_Regime'] == 'High_Vol').astype(int)\n",
        "            \n",
        "            return df\n",
        "    \n",
        "    class CNNLSTMModel:\n",
        "        \"\"\"Improved CNN+LSTM model with better architecture\"\"\"\n",
        "        \n",
        "        def __init__(self, time_steps=60, n_features=20, learning_rate=0.001):\n",
        "            self.time_steps = time_steps\n",
        "            self.n_features = n_features\n",
        "            self.learning_rate = learning_rate\n",
        "            self.model = None\n",
        "        \n",
        "        def build_model(self):\n",
        "            import tensorflow as tf\n",
        "            from tensorflow.keras.models import Sequential\n",
        "            from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, BatchNormalization\n",
        "            from tensorflow.keras.optimizers import Adam\n",
        "            \n",
        "            model = Sequential([\n",
        "                # CNN layers for pattern recognition\n",
        "                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(self.time_steps, self.n_features)),\n",
        "                BatchNormalization(),\n",
        "                Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "                MaxPooling1D(pool_size=2),\n",
        "                Dropout(0.3),\n",
        "                \n",
        "                # LSTM layers for sequence learning\n",
        "                LSTM(100, return_sequences=True),\n",
        "                Dropout(0.3),\n",
        "                LSTM(50, return_sequences=False),\n",
        "                Dropout(0.3),\n",
        "                \n",
        "                # Dense layers\n",
        "                Dense(50, activation='relu'),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                Dense(25, activation='relu'),\n",
        "                Dense(1, activation='linear')\n",
        "            ])\n",
        "            \n",
        "            # Use Huber loss for robustness to outliers\n",
        "            model.compile(\n",
        "                optimizer=Adam(learning_rate=self.learning_rate),\n",
        "                loss='huber',\n",
        "                metrics=['mae', 'mse']\n",
        "            )\n",
        "            \n",
        "            self.model = model\n",
        "            return model\n",
        "        \n",
        "        def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.2):\n",
        "            \"\"\"Train with early stopping and learning rate reduction\"\"\"\n",
        "            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "            \n",
        "            callbacks = [\n",
        "                EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7)\n",
        "            ]\n",
        "            \n",
        "            return self.model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "        \n",
        "        def predict(self, X):\n",
        "            \"\"\"Make predictions\"\"\"\n",
        "            return self.model.predict(X)\n",
        "        \n",
        "        def save_model(self, path):\n",
        "            \"\"\"Save model\"\"\"\n",
        "            self.model.save(path)\n",
        "    \n",
        "    class RiskManager:\n",
        "        \"\"\"Comprehensive risk management system\"\"\"\n",
        "        \n",
        "        def __init__(self, max_position_size=0.8, stop_loss=0.05, take_profit=0.15, max_drawdown=0.20):\n",
        "            self.max_position_size = max_position_size\n",
        "            self.stop_loss = stop_loss\n",
        "            self.take_profit = take_profit\n",
        "            self.max_drawdown = max_drawdown\n",
        "            self.current_drawdown = 0\n",
        "            self.peak_value = 0\n",
        "        \n",
        "        def calculate_position_size(self, signal_strength, volatility, account_value, market_regime='Sideways'):\n",
        "            \"\"\"Calculate position size using Kelly Criterion with regime adjustment\"\"\"\n",
        "            \n",
        "            # Base Kelly parameters\n",
        "            win_rate = 0.55  # Assume 55% win rate\n",
        "            avg_win = 0.08   # 8% average win\n",
        "            avg_loss = 0.04  # 4% average loss\n",
        "            \n",
        "            # Kelly fraction\n",
        "            kelly_fraction = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win\n",
        "            kelly_fraction = max(0, min(kelly_fraction, self.max_position_size))\n",
        "            \n",
        "            # Adjust for volatility\n",
        "            volatility_adjustment = 1 / (1 + volatility * 10)\n",
        "            \n",
        "            # Adjust for market regime\n",
        "            regime_adjustments = {\n",
        "                'Bull': 1.2,\n",
        "                'Bear': 0.6,\n",
        "                'Sideways': 1.0,\n",
        "                'High_Vol': 0.5\n",
        "            }\n",
        "            regime_adjustment = regime_adjustments.get(market_regime, 1.0)\n",
        "            \n",
        "            # Adjust for signal strength\n",
        "            signal_adjustment = min(abs(signal_strength), 1.0)\n",
        "            \n",
        "            # Final position size\n",
        "            final_position_size = kelly_fraction * volatility_adjustment * regime_adjustment * signal_adjustment\n",
        "            \n",
        "            return min(final_position_size, self.max_position_size)\n",
        "        \n",
        "        def check_risk_limits(self, current_value, entry_price, current_price, position_size):\n",
        "            \"\"\"Check if risk limits are breached\"\"\"\n",
        "            # Stop loss check\n",
        "            if current_price < entry_price * (1 - self.stop_loss):\n",
        "                return 'stop_loss'\n",
        "            \n",
        "            # Take profit check\n",
        "            if current_price > entry_price * (1 + self.take_profit):\n",
        "                return 'take_profit'\n",
        "            \n",
        "            # Drawdown check\n",
        "            if current_value > self.peak_value:\n",
        "                self.peak_value = current_value\n",
        "            \n",
        "            self.current_drawdown = (self.peak_value - current_value) / self.peak_value\n",
        "            \n",
        "            if self.current_drawdown > self.max_drawdown:\n",
        "                return 'max_drawdown'\n",
        "            \n",
        "            return 'hold'\n",
        "    \n",
        "    class ImprovedTradingStrategy:\n",
        "        \"\"\"Proper trading strategy with position management and risk controls\"\"\"\n",
        "        \n",
        "        def __init__(self, initial_capital=100000, risk_manager=None):\n",
        "            self.initial_capital = initial_capital\n",
        "            self.capital = initial_capital\n",
        "            self.position = 0\n",
        "            self.position_size = 0\n",
        "            self.entry_price = 0\n",
        "            self.portfolio_value = initial_capital\n",
        "            self.trades = []\n",
        "            self.risk_manager = risk_manager or RiskManager()\n",
        "            self.market_regime = 'Sideways'\n",
        "        \n",
        "        def generate_signals(self, df, predictions):\n",
        "            \"\"\"Generate trading signals with proper position management\"\"\"\n",
        "            signals = pd.DataFrame(index=df.index)\n",
        "            signals['signal'] = 0\n",
        "            signals['position'] = 0\n",
        "            signals['portfolio_value'] = self.portfolio_value\n",
        "            signals['market_regime'] = df.get('Market_Regime', 'Sideways')\n",
        "            \n",
        "            for i in range(1, len(df)):\n",
        "                current_price = df['Close'].iloc[i]\n",
        "                prediction = predictions[i] if i < len(predictions) else 0\n",
        "                self.market_regime = df['Market_Regime'].iloc[i] if 'Market_Regime' in df.columns else 'Sideways'\n",
        "                \n",
        "                # Multi-factor signal generation\n",
        "                signal_strength = self._calculate_signal_strength(df.iloc[i], prediction)\n",
        "                \n",
        "                # Risk management\n",
        "                risk_action = self.risk_manager.check_risk_limits(\n",
        "                    self.portfolio_value, self.entry_price, current_price, self.position_size\n",
        "                )\n",
        "                \n",
        "                if risk_action != 'hold':\n",
        "                    # Close position due to risk management\n",
        "                    self._close_position(current_price, f\"Risk: {risk_action}\")\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = 0\n",
        "                elif signal_strength > 0.5 and self.position == 0:\n",
        "                    # Open long position\n",
        "                    position_size = self.risk_manager.calculate_position_size(\n",
        "                        signal_strength, df['Volatility_20'].iloc[i], self.portfolio_value, self.market_regime\n",
        "                    )\n",
        "                    self._open_position(current_price, position_size, 'long')\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 1\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = position_size\n",
        "                elif signal_strength < -0.5 and self.position == 0:\n",
        "                    # Open short position\n",
        "                    position_size = self.risk_manager.calculate_position_size(\n",
        "                        abs(signal_strength), df['Volatility_20'].iloc[i], self.portfolio_value, self.market_regime\n",
        "                    )\n",
        "                    self._open_position(current_price, position_size, 'short')\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = -1\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = position_size\n",
        "                elif abs(signal_strength) < 0.2 and self.position != 0:\n",
        "                    # Close position due to weak signal\n",
        "                    self._close_position(current_price, \"Weak signal\")\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = 0\n",
        "                else:\n",
        "                    # Hold current position\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = self.position_size\n",
        "                \n",
        "                # Update portfolio value\n",
        "                self._update_portfolio_value(current_price)\n",
        "                signals.iloc[i, signals.columns.get_loc('portfolio_value')] = self.portfolio_value\n",
        "            \n",
        "            return signals\n",
        "        \n",
        "        def _calculate_signal_strength(self, row, prediction):\n",
        "            \"\"\"Calculate multi-factor signal strength\"\"\"\n",
        "            signal_strength = 0\n",
        "            \n",
        "            # Trend following\n",
        "            if row['Close'] > row['SMA_20'] and prediction > 0:\n",
        "                signal_strength += 0.3\n",
        "            elif row['Close'] < row['SMA_20'] and prediction < 0:\n",
        "                signal_strength -= 0.3\n",
        "            \n",
        "            # Momentum\n",
        "            if row['RSI'] < 30 and prediction > 0:  # Oversold + bullish prediction\n",
        "                signal_strength += 0.4\n",
        "            elif row['RSI'] > 70 and prediction < 0:  # Overbought + bearish prediction\n",
        "                signal_strength -= 0.4\n",
        "            \n",
        "            # MACD confirmation\n",
        "            if row['MACD'] > row['MACD_Signal'] and prediction > 0:\n",
        "                signal_strength += 0.2\n",
        "            elif row['MACD'] < row['MACD_Signal'] and prediction < 0:\n",
        "                signal_strength -= 0.2\n",
        "            \n",
        "            # Bollinger Band position\n",
        "            if row['BB_Position'] < 0.2 and prediction > 0:  # Near lower band + bullish\n",
        "                signal_strength += 0.2\n",
        "            elif row['BB_Position'] > 0.8 and prediction < 0:  # Near upper band + bearish\n",
        "                signal_strength -= 0.2\n",
        "            \n",
        "            # Market regime adjustment\n",
        "            regime_multipliers = {\n",
        "                'Bull': 1.2,\n",
        "                'Bear': 0.8,\n",
        "                'Sideways': 1.0,\n",
        "                'High_Vol': 0.6\n",
        "            }\n",
        "            signal_strength *= regime_multipliers.get(self.market_regime, 1.0)\n",
        "            \n",
        "            return signal_strength\n",
        "        \n",
        "        def _open_position(self, price, size, direction):\n",
        "            \"\"\"Open a new position\"\"\"\n",
        "            self.position = 1 if direction == 'long' else -1\n",
        "            self.position_size = size\n",
        "            self.entry_price = price\n",
        "            self.capital -= price * size  # Reduce available capital\n",
        "        \n",
        "        def _close_position(self, price, reason):\n",
        "            \"\"\"Close current position\"\"\"\n",
        "            if self.position != 0:\n",
        "                pnl = (price - self.entry_price) * self.position * self.position_size\n",
        "                self.capital += price * self.position_size + pnl\n",
        "                \n",
        "                self.trades.append({\n",
        "                    'entry_price': self.entry_price,\n",
        "                    'exit_price': price,\n",
        "                    'position_size': self.position_size,\n",
        "                    'direction': 'long' if self.position > 0 else 'short',\n",
        "                    'pnl': pnl,\n",
        "                    'reason': reason\n",
        "                })\n",
        "                \n",
        "                self.position = 0\n",
        "                self.position_size = 0\n",
        "                self.entry_price = 0\n",
        "        \n",
        "        def _update_portfolio_value(self, current_price):\n",
        "            \"\"\"Update portfolio value\"\"\"\n",
        "            if self.position != 0:\n",
        "                pnl = (current_price - self.entry_price) * self.position * self.position_size\n",
        "                self.portfolio_value = self.capital + current_price * self.position_size + pnl\n",
        "            else:\n",
        "                self.portfolio_value = self.capital\n",
        "    \n",
        "    class ImprovedBacktestEngine:\n",
        "        \"\"\"Proper backtesting engine with realistic assumptions\"\"\"\n",
        "        \n",
        "        def __init__(self, initial_capital=100000, commission=0.001, slippage=0.0005):\n",
        "            self.initial_capital = initial_capital\n",
        "            self.commission = commission\n",
        "            self.slippage = slippage\n",
        "        \n",
        "        def run_backtest(self, price_data, signals):\n",
        "            \"\"\"Run comprehensive backtest with realistic trading costs\"\"\"\n",
        "            portfolio_value = self.initial_capital\n",
        "            cash = self.initial_capital\n",
        "            shares = 0\n",
        "            trades = []\n",
        "            portfolio_values = []\n",
        "            \n",
        "            for i, (date, row) in enumerate(price_data.iterrows()):\n",
        "                if date in signals.index:\n",
        "                    signal = signals.loc[date, 'signal']\n",
        "                    position_size = signals.loc[date, 'position']\n",
        "                    price = row['Close']\n",
        "                    \n",
        "                    # Apply slippage\n",
        "                    execution_price = price * (1 + self.slippage) if signal > 0 else price * (1 - self.slippage)\n",
        "                    \n",
        "                    if signal == 1 and shares == 0:  # Buy\n",
        "                        shares_to_buy = (cash * position_size) / execution_price\n",
        "                        cost = shares_to_buy * execution_price * (1 + self.commission)\n",
        "                        \n",
        "                        if cost <= cash:\n",
        "                            shares = shares_to_buy\n",
        "                            cash -= cost\n",
        "                            trades.append({\n",
        "                                'date': date,\n",
        "                                'action': 'BUY',\n",
        "                                'price': execution_price,\n",
        "                                'shares': shares,\n",
        "                                'cost': cost\n",
        "                            })\n",
        "                    \n",
        "                    elif signal == -1 and shares > 0:  # Sell\n",
        "                        proceeds = shares * execution_price * (1 - self.commission)\n",
        "                        cash += proceeds\n",
        "                        trades.append({\n",
        "                            'date': date,\n",
        "                            'action': 'SELL',\n",
        "                            'price': execution_price,\n",
        "                            'shares': shares,\n",
        "                            'proceeds': proceeds\n",
        "                        })\n",
        "                        shares = 0\n",
        "                \n",
        "                # Calculate portfolio value\n",
        "                portfolio_value = cash + (shares * row['Close'] if shares > 0 else 0)\n",
        "                portfolio_values.append(portfolio_value)\n",
        "            \n",
        "            # Calculate returns\n",
        "            returns = pd.Series(portfolio_values).pct_change().dropna()\n",
        "            \n",
        "            return {\n",
        "                'total_return': (portfolio_value - self.initial_capital) / self.initial_capital,\n",
        "                'portfolio_value': portfolio_value,\n",
        "                'trades': trades,\n",
        "                'final_cash': cash,\n",
        "                'final_shares': shares,\n",
        "                'returns': returns,\n",
        "                'portfolio_values': portfolio_values\n",
        "            }\n",
        "    \n",
        "    class PerformanceAnalyzer:\n",
        "        \"\"\"Enhanced performance analyzer with comprehensive metrics\"\"\"\n",
        "        \n",
        "        def calculate_portfolio_performance(self, backtest_results):\n",
        "            \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "            returns = backtest_results['returns']\n",
        "            \n",
        "            # Basic metrics\n",
        "            total_return = (1 + returns).prod() - 1\n",
        "            annualized_return = (1 + returns).prod() ** (252 / len(returns)) - 1\n",
        "            volatility = returns.std() * np.sqrt(252)\n",
        "            sharpe_ratio = annualized_return / volatility if volatility > 0 else 0\n",
        "            \n",
        "            # Risk metrics\n",
        "            negative_returns = returns[returns < 0]\n",
        "            sortino_ratio = annualized_return / (negative_returns.std() * np.sqrt(252)) if len(negative_returns) > 0 else 0\n",
        "            \n",
        "            # Drawdown analysis\n",
        "            cumulative = (1 + returns).cumprod()\n",
        "            running_max = cumulative.expanding().max()\n",
        "            drawdown = (cumulative - running_max) / running_max\n",
        "            max_drawdown = drawdown.min()\n",
        "            calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
        "            \n",
        "            # Win/Loss analysis\n",
        "            winning_trades = returns[returns > 0]\n",
        "            losing_trades = returns[returns < 0]\n",
        "            win_rate = len(winning_trades) / len(returns) if len(returns) > 0 else 0\n",
        "            avg_win = winning_trades.mean() if len(winning_trades) > 0 else 0\n",
        "            avg_loss = losing_trades.mean() if len(losing_trades) > 0 else 0\n",
        "            profit_factor = abs(winning_trades.sum() / losing_trades.sum()) if len(losing_trades) > 0 and losing_trades.sum() != 0 else 0\n",
        "            \n",
        "            return {\n",
        "                'total_return': total_return,\n",
        "                'annualized_return': annualized_return,\n",
        "                'volatility': volatility,\n",
        "                'sharpe_ratio': sharpe_ratio,\n",
        "                'sortino_ratio': sortino_ratio,\n",
        "                'max_drawdown': max_drawdown,\n",
        "                'calmar_ratio': calmar_ratio,\n",
        "                'win_rate': win_rate,\n",
        "                'avg_win': avg_win,\n",
        "                'avg_loss': avg_loss,\n",
        "                'profit_factor': profit_factor,\n",
        "                'cumulative_returns': cumulative,\n",
        "                'drawdown': drawdown,\n",
        "                'rolling_sharpe': returns.rolling(window=252).mean() / returns.rolling(window=252).std() * np.sqrt(252),\n",
        "                'monthly_returns': returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
        "            }\n",
        "    \n",
        "    # Initialize improved classes\n",
        "    print(\"✅ Improved implementations created!\")\n",
        "    print(\"📊 Features implemented:\")\n",
        "    print(\"  • Advanced Technical Indicators (RSI, MACD, Bollinger Bands, Stochastic, Williams %R, ATR)\")\n",
        "    print(\"  • Comprehensive Feature Engineering with Market Regime Detection\")\n",
        "    print(\"  • Improved CNN+LSTM Model with CNN layers and regularization\")\n",
        "    print(\"  • Proper Trading Strategy with Position Management\")\n",
        "    print(\"  • Risk Management System with Kelly Criterion and Stop Losses\")\n",
        "    print(\"  • Realistic Backtesting Engine with Commissions and Slippage\")\n",
        "    print(\"  • Enhanced Performance Analysis with Comprehensive Metrics\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize improved data collector and indicator engine\n",
        "collector = MarketDataCollector()\n",
        "indicator_engine = TechnicalIndicatorEngine()\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "# Define symbols to trade\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
        "\n",
        "# Collect historical data\n",
        "print(\"📊 Collecting historical data with improved error handling...\")\n",
        "data = {}\n",
        "for symbol in symbols:\n",
        "    try:\n",
        "        df = collector.get_historical_data(symbol, period='2y', interval='1d')\n",
        "        if df is not None and len(df) > 0:\n",
        "            data[symbol] = df\n",
        "            print(f\"✓ {symbol}: {len(df)} records\")\n",
        "        else:\n",
        "            print(f\"✗ {symbol}: No data received\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {symbol}: Error - {e}\")\n",
        "\n",
        "print(f\"\\nData collection complete! Collected data for {len(data)} symbols.\")\n",
        "\n",
        "# Process data with advanced technical indicators and feature engineering\n",
        "if data:\n",
        "    print(\"\\n🔧 Processing data with advanced technical indicators and feature engineering...\")\n",
        "    processed_data = {}\n",
        "    \n",
        "    for symbol, df in data.items():\n",
        "        try:\n",
        "            # Step 1: Calculate comprehensive technical indicators\n",
        "            print(f\"  📈 Calculating technical indicators for {symbol}...\")\n",
        "            df_with_indicators = indicator_engine.calculate_all_indicators(df)\n",
        "            \n",
        "            # Step 2: Advanced feature engineering with market regime detection\n",
        "            print(f\"  🧠 Creating advanced features for {symbol}...\")\n",
        "            df_with_features = feature_engineer.create_features(df_with_indicators)\n",
        "            \n",
        "            processed_data[symbol] = df_with_features\n",
        "            print(f\"✓ {symbol}: Technical indicators and features calculated\")\n",
        "            \n",
        "            # Display feature summary\n",
        "            feature_count = len([col for col in df_with_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])\n",
        "            print(f\"  📊 {symbol}: {feature_count} features created\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ {symbol}: Error processing - {e}\")\n",
        "            processed_data[symbol] = df  # Use original data if processing fails\n",
        "\n",
        "    print(f\"\\n✅ Data processing complete! Processed {len(processed_data)} symbols.\")\n",
        "    \n",
        "    # Display sample processed data with new features\n",
        "    sample_symbol = list(processed_data.keys())[0]\n",
        "    sample_data = processed_data[sample_symbol]\n",
        "    \n",
        "    print(f\"\\n📋 Sample processed data for {sample_symbol}:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show key technical indicators\n",
        "    key_indicators = ['Close', 'SMA_20', 'SMA_50', 'RSI', 'MACD', 'BB_Position', 'ATR', 'Market_Regime']\n",
        "    available_indicators = [col for col in key_indicators if col in sample_data.columns]\n",
        "    \n",
        "    if available_indicators:\n",
        "        print(\"Key Technical Indicators:\")\n",
        "        print(sample_data[available_indicators].tail())\n",
        "    \n",
        "    # Show feature summary\n",
        "    print(f\"\\n📊 Feature Summary for {sample_symbol}:\")\n",
        "    print(f\"Total features: {len(sample_data.columns)}\")\n",
        "    print(f\"Technical indicators: {len([col for col in sample_data.columns if col.startswith(('SMA', 'EMA', 'RSI', 'MACD', 'BB', 'Stoch', 'Williams', 'ATR'))])}\")\n",
        "    print(f\"Price features: {len([col for col in sample_data.columns if col.startswith(('Returns', 'Momentum', 'Price', 'Volatility'))])}\")\n",
        "    print(f\"Market regime features: {len([col for col in sample_data.columns if col.startswith('Regime')])}\")\n",
        "    \n",
        "    # Show market regime distribution\n",
        "    if 'Market_Regime' in sample_data.columns:\n",
        "        regime_counts = sample_data['Market_Regime'].value_counts()\n",
        "        print(f\"\\n🎯 Market Regime Distribution:\")\n",
        "        for regime, count in regime_counts.items():\n",
        "            percentage = (count / len(sample_data)) * 100\n",
        "            print(f\"  {regime}: {count} days ({percentage:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No data collected. Check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train improved CNN+LSTM model with GPU and advanced features\n",
        "print(\"🤖 Training improved CNN+LSTM model with advanced architecture...\")\n",
        "\n",
        "# Prepare training data from processed data with comprehensive features\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    print(\"📊 Preparing training data from collected market data with advanced features...\")\n",
        "    \n",
        "    # Combine all data for training\n",
        "    all_data = []\n",
        "    for symbol, df in processed_data.items():\n",
        "        df['symbol'] = symbol\n",
        "        all_data.append(df)\n",
        "    \n",
        "    combined_data = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {combined_data.shape}\")\n",
        "    \n",
        "    # Select comprehensive features for training\n",
        "    feature_columns = [\n",
        "        # Price features\n",
        "        'Returns', 'Log_Returns', 'Price_Ratio', 'High_Low_Ratio', 'Close_Open_Ratio',\n",
        "        # Volatility features\n",
        "        'Volatility_5', 'Volatility_20', 'Volatility_Ratio',\n",
        "        # Momentum features\n",
        "        'Momentum_5', 'Momentum_10', 'Momentum_20',\n",
        "        # Trend features\n",
        "        'Trend_Strength', 'Price_vs_SMA20', 'Price_vs_SMA50',\n",
        "        # Technical indicators\n",
        "        'RSI', 'RSI_6', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
        "        'BB_Position', 'BB_Width', 'Stoch_K', 'Stoch_D', 'Williams_R', 'ATR',\n",
        "        # Volume features\n",
        "        'Volume_Ratio', 'Volume_Change', 'Price_Volume_Trend',\n",
        "        # Technical indicator features\n",
        "        'RSI_Overbought', 'RSI_Oversold', 'MACD_Bullish', 'MACD_Bearish',\n",
        "        'BB_Squeeze', 'BB_Expansion',\n",
        "        # Market regime features\n",
        "        'Regime_Bull', 'Regime_Bear', 'Regime_Sideways', 'Regime_High_Vol',\n",
        "        # Lagged features\n",
        "        'Returns_Lag_1', 'Returns_Lag_2', 'Returns_Lag_3', 'Returns_Lag_5',\n",
        "        'Volume_Ratio_Lag_1', 'Volume_Ratio_Lag_2', 'Volume_Ratio_Lag_3', 'Volume_Ratio_Lag_5'\n",
        "    ]\n",
        "    \n",
        "    # Filter available features\n",
        "    available_features = [col for col in feature_columns if col in combined_data.columns]\n",
        "    print(f\"📈 Available features for training: {len(available_features)}\")\n",
        "    print(f\"Features: {available_features[:10]}...\")  # Show first 10 features\n",
        "    \n",
        "    if len(available_features) >= 10:  # Need at least 10 features for robust training\n",
        "        # Initialize improved model with correct number of features\n",
        "        cnn_lstm = CNNLSTMModel(\n",
        "            time_steps=60,\n",
        "            n_features=len(available_features),\n",
        "            learning_rate=0.0001  # Lower learning rate for better convergence\n",
        "        )\n",
        "        \n",
        "        # Build improved model\n",
        "        model = cnn_lstm.build_model()\n",
        "        print(f\"🏗️ Improved model built with {model.count_params():,} parameters\")\n",
        "        print(f\"📊 Model expects {len(available_features)} input features\")\n",
        "        \n",
        "        # Prepare features and targets\n",
        "        X_data = combined_data[available_features].values\n",
        "        y_data = combined_data['Close'].shift(-1).values  # Predict next day's close\n",
        "        \n",
        "        # Remove NaN values\n",
        "        valid_indices = ~np.isnan(X_data).any(axis=1) & ~np.isnan(y_data)\n",
        "        X_data = X_data[valid_indices]\n",
        "        y_data = y_data[valid_indices]\n",
        "        \n",
        "        print(f\"📊 Data after cleaning: {len(X_data)} samples\")\n",
        "        \n",
        "        # Reshape for LSTM (samples, time_steps, features)\n",
        "        n_samples = len(X_data) - cnn_lstm.time_steps + 1\n",
        "        X_reshaped = np.zeros((n_samples, cnn_lstm.time_steps, len(available_features)))\n",
        "        y_reshaped = np.zeros((n_samples, 1))\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            X_reshaped[i] = X_data[i:i+cnn_lstm.time_steps]\n",
        "            y_reshaped[i] = y_data[i+cnn_lstm.time_steps-1]\n",
        "        \n",
        "        # Split data\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_reshaped, y_reshaped, test_size=0.2, random_state=42, shuffle=False\n",
        "        )\n",
        "        \n",
        "        print(f\"📊 Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "        print(f\"📊 Test data shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "        \n",
        "        # Verify dimensions match\n",
        "        print(f\"🔍 Model input shape: (batch_size, {cnn_lstm.time_steps}, {cnn_lstm.n_features})\")\n",
        "        print(f\"🔍 Actual data shape: {X_train.shape}\")\n",
        "        \n",
        "        if X_train.shape[2] == cnn_lstm.n_features:\n",
        "            print(\"✅ Dimensions match! Proceeding with improved training...\")\n",
        "            \n",
        "            # Train improved model with early stopping and learning rate reduction\n",
        "            history = cnn_lstm.train(\n",
        "                X_train, y_train,\n",
        "                epochs=50,  # More epochs with early stopping\n",
        "                batch_size=32,\n",
        "                validation_split=0.2\n",
        "            )\n",
        "            \n",
        "            print(\"🎉 Improved CNN+LSTM training complete!\")\n",
        "            \n",
        "            # Plot comprehensive training history\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            \n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "            plt.title('Model Loss (Huber Loss)')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            \n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.plot(history.history['mae'], label='Training MAE', color='blue')\n",
        "            plt.plot(history.history['val_mae'], label='Validation MAE', color='red')\n",
        "            plt.title('Model MAE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            \n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.plot(history.history['mse'], label='Training MSE', color='blue')\n",
        "            plt.plot(history.history['val_mse'], label='Validation MSE', color='red')\n",
        "            plt.title('Model MSE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            # Evaluate improved model\n",
        "            test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "            print(f\"\\n📊 Improved Model Evaluation:\")\n",
        "            print(f\"Test Loss (Huber): {test_loss:.6f}\")\n",
        "            print(f\"Test MAE: {test_mae:.6f}\")\n",
        "            print(f\"Test MSE: {test_mse:.6f}\")\n",
        "            \n",
        "            # Calculate R² score\n",
        "            from sklearn.metrics import r2_score\n",
        "            predictions = model.predict(X_test)\n",
        "            r2 = r2_score(y_test, predictions)\n",
        "            print(f\"R² Score: {r2:.4f}\")\n",
        "            \n",
        "            # Model quality assessment\n",
        "            if r2 > 0.7:\n",
        "                print(\"🎉 Excellent model performance (R² > 0.7)\")\n",
        "            elif r2 > 0.5:\n",
        "                print(\"✅ Good model performance (R² > 0.5)\")\n",
        "            elif r2 > 0.3:\n",
        "                print(\"⚠️ Moderate model performance (R² > 0.3)\")\n",
        "            else:\n",
        "                print(\"❌ Poor model performance (R² < 0.3)\")\n",
        "            \n",
        "            # Store model and data for later use\n",
        "            trained_model = model\n",
        "            model_history = history\n",
        "            \n",
        "        else:\n",
        "            print(f\"❌ Dimension mismatch! Model expects {cnn_lstm.n_features} features but data has {X_train.shape[2]}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"⚠️ Insufficient features for training. Need at least 10 features.\")\n",
        "        print(f\"Available features: {len(available_features)}\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ No processed data available. Please run the data collection and processing cells first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear any cached model saving code and restart\n",
        "print(\"🔄 Clearing cached model saving code...\")\n",
        "print(\"If you get git errors, please:\")\n",
        "print(\"1. Go to Runtime > Restart Runtime\")\n",
        "print(\"2. Run all cells from the beginning\")\n",
        "print(\"3. The model saving now only uses Google Drive (no git operations)\")\n",
        "\n",
        "# Clear any cached variables that might cause issues\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Remove any cached modules that might cause issues\n",
        "modules_to_clear = ['subprocess', 'os', 'shutil']\n",
        "for module in modules_to_clear:\n",
        "    if module in sys.modules:\n",
        "        del sys.modules[module]\n",
        "\n",
        "print(\"✅ Cache cleared. Model saving will now only use Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Your Work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models to Google Drive for permanent storage\n",
        "# Note: Using .keras format instead of .h5 for better compatibility\n",
        "\n",
        "# 1. Save locally first (using modern .keras format)\n",
        "cnn_lstm.save_model('cnn_lstm_model.keras')\n",
        "print(\"✓ Model saved locally in .keras format\")\n",
        "\n",
        "# 2. Save to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your trading models\n",
        "drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Save model to Google Drive with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_model_path = f'{drive_folder}/cnn_lstm_model_{timestamp}.keras'\n",
        "shutil.copy('cnn_lstm_model.keras', drive_model_path)\n",
        "print(f\"✓ Model saved to Google Drive: {drive_model_path}\")\n",
        "\n",
        "print(\"\\n🎉 Model saved in 2 locations:\")\n",
        "print(\"1. Local Colab environment (cnn_lstm_model.keras)\")\n",
        "print(\"2. Google Drive (permanent storage)\")\n",
        "print(\"\\n💡 Note: Using .keras format instead of .h5 for better compatibility\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Saved Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed Model Loading Function with Compatibility Handling\n",
        "\n",
        "def load_model_from_location_fixed(location_type, model_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained model from different storage locations with compatibility handling\n",
        "    \n",
        "    Args:\n",
        "        location_type: 'local', 'drive', 'github', or 'url'\n",
        "        model_path: Path to the model file (optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    if location_type == 'local':\n",
        "        # Load from local Colab environment\n",
        "        if model_path is None:\n",
        "            # Try .keras format first, then .h5\n",
        "            if os.path.exists('cnn_lstm_model.keras'):\n",
        "                model_path = 'cnn_lstm_model.keras'\n",
        "            elif os.path.exists('cnn_lstm_model.h5'):\n",
        "                model_path = 'cnn_lstm_model.h5'\n",
        "            else:\n",
        "                print(\"No model found locally\")\n",
        "                return None\n",
        "        \n",
        "        try:\n",
        "            model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"✓ Model loaded from local: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading model from local: {e}\")\n",
        "            return None\n",
        "        \n",
        "    elif location_type == 'drive':\n",
        "        # Load from Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        if model_path is None:\n",
        "            # List available models in Drive\n",
        "            drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "            if os.path.exists(drive_folder):\n",
        "                # Look for both .keras and .h5 files\n",
        "                keras_models = [f for f in os.listdir(drive_folder) if f.endswith('.keras')]\n",
        "                h5_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "                all_models = keras_models + h5_models\n",
        "                \n",
        "                if all_models:\n",
        "                    # Prefer .keras files, but use .h5 if that's all we have\n",
        "                    if keras_models:\n",
        "                        model_path = os.path.join(drive_folder, keras_models[-1])\n",
        "                        print(f\"Available .keras models: {keras_models}\")\n",
        "                    else:\n",
        "                        model_path = os.path.join(drive_folder, h5_models[-1])\n",
        "                        print(f\"Available .h5 models: {h5_models}\")\n",
        "                        print(\"⚠️ Loading .h5 model - may have compatibility issues\")\n",
        "                else:\n",
        "                    print(\"No models found in Google Drive\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Trading_Strategy_ML folder not found in Google Drive\")\n",
        "                return None\n",
        "        \n",
        "        try:\n",
        "            # Try loading with custom objects to handle compatibility issues\n",
        "            custom_objects = {\n",
        "                'mse': tf.keras.metrics.mean_squared_error,\n",
        "                'mae': tf.keras.metrics.mean_absolute_error,\n",
        "                'accuracy': tf.keras.metrics.accuracy\n",
        "            }\n",
        "            model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "            print(f\"✓ Model loaded from Google Drive: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading model from Google Drive: {e}\")\n",
        "            print(\"💡 Try training a new model with the current TensorFlow version\")\n",
        "            return None\n",
        "    \n",
        "    else:\n",
        "        print(\"Invalid location_type. Use 'local' or 'drive'\")\n",
        "        return None\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Example: Load the latest model from Google Drive with compatibility handling\n",
        "print(\"Loading model from Google Drive with compatibility handling...\")\n",
        "loaded_model = load_model_from_location_fixed('drive')\n",
        "\n",
        "if loaded_model is not None:\n",
        "    print(f\"Model summary:\")\n",
        "    loaded_model.summary()\n",
        "else:\n",
        "    print(\"No model found. Train a model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Technical Benchmarks and Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Technical Benchmarks and Performance Analysis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TradingBenchmark:\n",
        "    \"\"\"Comprehensive trading strategy benchmark and analysis\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.benchmarks = {}\n",
        "        self.results = {}\n",
        "        \n",
        "    def calculate_technical_metrics(self, returns, benchmark_returns=None):\n",
        "        \"\"\"Calculate comprehensive technical metrics\"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # Basic metrics\n",
        "        metrics['total_return'] = (1 + returns).prod() - 1\n",
        "        metrics['annualized_return'] = (1 + returns).prod() ** (252 / len(returns)) - 1\n",
        "        metrics['volatility'] = returns.std() * np.sqrt(252)\n",
        "        metrics['sharpe_ratio'] = metrics['annualized_return'] / metrics['volatility'] if metrics['volatility'] > 0 else 0\n",
        "        \n",
        "        # Risk metrics\n",
        "        negative_returns = returns[returns < 0]\n",
        "        metrics['sortino_ratio'] = metrics['annualized_return'] / (negative_returns.std() * np.sqrt(252)) if len(negative_returns) > 0 else 0\n",
        "        \n",
        "        # Drawdown analysis\n",
        "        cumulative = (1 + returns).cumprod()\n",
        "        running_max = cumulative.expanding().max()\n",
        "        drawdown = (cumulative - running_max) / running_max\n",
        "        metrics['max_drawdown'] = drawdown.min()\n",
        "        metrics['calmar_ratio'] = metrics['annualized_return'] / abs(metrics['max_drawdown']) if metrics['max_drawdown'] != 0 else 0\n",
        "        \n",
        "        # Win/Loss analysis\n",
        "        winning_trades = returns[returns > 0]\n",
        "        losing_trades = returns[returns < 0]\n",
        "        metrics['win_rate'] = len(winning_trades) / len(returns) if len(returns) > 0 else 0\n",
        "        metrics['avg_win'] = winning_trades.mean() if len(winning_trades) > 0 else 0\n",
        "        metrics['avg_loss'] = losing_trades.mean() if len(losing_trades) > 0 else 0\n",
        "        metrics['profit_factor'] = abs(winning_trades.sum() / losing_trades.sum()) if len(losing_trades) > 0 and losing_trades.sum() != 0 else 0\n",
        "        \n",
        "        # Benchmark comparison\n",
        "        if benchmark_returns is not None:\n",
        "            excess_returns = returns - benchmark_returns\n",
        "            metrics['alpha'] = excess_returns.mean() * 252\n",
        "            metrics['beta'] = returns.cov(benchmark_returns) / benchmark_returns.var() if benchmark_returns.var() > 0 else 0\n",
        "            metrics['information_ratio'] = excess_returns.mean() / excess_returns.std() * np.sqrt(252) if excess_returns.std() > 0 else 0\n",
        "            metrics['tracking_error'] = excess_returns.std() * np.sqrt(252)\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def create_performance_dashboard(self, strategy_returns, benchmark_returns=None, strategy_name=\"Trading Strategy\"):\n",
        "        \"\"\"Create comprehensive performance dashboard\"\"\"\n",
        "        \n",
        "        # Calculate metrics\n",
        "        strategy_metrics = self.calculate_technical_metrics(strategy_returns, benchmark_returns)\n",
        "        \n",
        "        # Create subplots with correct specs for table\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Cumulative Returns Comparison',\n",
        "                'Rolling Sharpe Ratio',\n",
        "                'Drawdown Analysis',\n",
        "                'Monthly Returns Heatmap',\n",
        "                'Risk-Return Scatter',\n",
        "                'Performance Metrics Table'\n",
        "            ],\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
        "        )\n",
        "        \n",
        "        # 1. Cumulative Returns\n",
        "        strategy_cumulative = (1 + strategy_returns).cumprod()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=strategy_cumulative.values,\n",
        "                      name=f'{strategy_name}', line=dict(color='blue', width=2)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        if benchmark_returns is not None:\n",
        "            benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=benchmark_returns.index, y=benchmark_cumulative.values,\n",
        "                          name='Benchmark (S&P 500)', line=dict(color='red', width=2)),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # 2. Rolling Sharpe Ratio\n",
        "        rolling_sharpe = strategy_returns.rolling(window=252).mean() / strategy_returns.rolling(window=252).std() * np.sqrt(252)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=rolling_sharpe.values,\n",
        "                      name='Rolling Sharpe', line=dict(color='green')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Drawdown Analysis\n",
        "        cumulative = (1 + strategy_returns).cumprod()\n",
        "        running_max = cumulative.expanding().max()\n",
        "        drawdown = (cumulative - running_max) / running_max\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=drawdown.values,\n",
        "                      name='Drawdown', fill='tonexty', line=dict(color='red')),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Monthly Returns Heatmap\n",
        "        monthly_returns = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
        "        monthly_pivot = monthly_returns.groupby([monthly_returns.index.year, monthly_returns.index.month]).first().unstack()\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Heatmap(z=monthly_pivot.values,\n",
        "                      x=monthly_pivot.columns,\n",
        "                      y=monthly_pivot.index,\n",
        "                      colorscale='RdYlGn',\n",
        "                      name='Monthly Returns'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 5. Risk-Return Scatter\n",
        "        if benchmark_returns is not None:\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=[strategy_metrics['volatility']], y=[strategy_metrics['annualized_return']],\n",
        "                          mode='markers', marker=dict(size=15, color='blue'),\n",
        "                          name=f'{strategy_name}'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "            benchmark_metrics = self.calculate_technical_metrics(benchmark_returns)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=[benchmark_metrics['volatility']], y=[benchmark_metrics['annualized_return']],\n",
        "                          mode='markers', marker=dict(size=15, color='red'),\n",
        "                          name='Benchmark'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "        \n",
        "        # 6. Performance Metrics Table\n",
        "        metrics_data = [\n",
        "            ['Total Return', f\"{strategy_metrics['total_return']:.2%}\"],\n",
        "            ['Annualized Return', f\"{strategy_metrics['annualized_return']:.2%}\"],\n",
        "            ['Volatility', f\"{strategy_metrics['volatility']:.2%}\"],\n",
        "            ['Sharpe Ratio', f\"{strategy_metrics['sharpe_ratio']:.2f}\"],\n",
        "            ['Sortino Ratio', f\"{strategy_metrics['sortino_ratio']:.2f}\"],\n",
        "            ['Max Drawdown', f\"{strategy_metrics['max_drawdown']:.2%}\"],\n",
        "            ['Calmar Ratio', f\"{strategy_metrics['calmar_ratio']:.2f}\"],\n",
        "            ['Win Rate', f\"{strategy_metrics['win_rate']:.2%}\"],\n",
        "            ['Profit Factor', f\"{strategy_metrics['profit_factor']:.2f}\"]\n",
        "        ]\n",
        "        \n",
        "        if benchmark_returns is not None:\n",
        "            metrics_data.extend([\n",
        "                ['Alpha', f\"{strategy_metrics['alpha']:.2%}\"],\n",
        "                ['Beta', f\"{strategy_metrics['beta']:.2f}\"],\n",
        "                ['Information Ratio', f\"{strategy_metrics['information_ratio']:.2f}\"]\n",
        "            ])\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Table(\n",
        "                header=dict(values=['Metric', 'Value'], fill_color='lightblue'),\n",
        "                cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "            ),\n",
        "            row=3, col=2\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=1200,\n",
        "            title_text=f\"{strategy_name} - Performance Dashboard\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        return fig, strategy_metrics\n",
        "    \n",
        "    def create_model_performance_analysis(self, model_history, X_test, y_test, model_name=\"CNN+LSTM\"):\n",
        "        \"\"\"Create model performance analysis\"\"\"\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Training History',\n",
        "                'Prediction vs Actual',\n",
        "                'Residuals Analysis',\n",
        "                'Model Performance Metrics'\n",
        "            ],\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
        "        )\n",
        "        \n",
        "        # 1. Training History\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=model_history.history['loss'], name='Training Loss', line=dict(color='blue')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=model_history.history['val_loss'], name='Validation Loss', line=dict(color='red')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Predictions vs Actual\n",
        "        predictions = model_history.model.predict(X_test)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test.flatten(), y=predictions.flatten(),\n",
        "                      mode='markers', name='Predictions vs Actual',\n",
        "                      marker=dict(color='blue', opacity=0.6)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Add perfect prediction line\n",
        "        min_val = min(y_test.min(), predictions.min())\n",
        "        max_val = max(y_test.max(), predictions.max())\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                      mode='lines', name='Perfect Prediction',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Residuals Analysis\n",
        "        residuals = y_test.flatten() - predictions.flatten()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=predictions.flatten(), y=residuals.flatten(),\n",
        "                      mode='markers', name='Residuals',\n",
        "                      marker=dict(color='green', opacity=0.6)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Model Metrics Table\n",
        "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "        \n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "        \n",
        "        metrics_data = [\n",
        "            ['MSE', f\"{mse:.6f}\"],\n",
        "            ['RMSE', f\"{rmse:.6f}\"],\n",
        "            ['MAE', f\"{mae:.6f}\"],\n",
        "            ['R² Score', f\"{r2:.4f}\"],\n",
        "            ['Training Samples', f\"{len(X_test):,}\"],\n",
        "            ['Model Parameters', f\"{model_history.model.count_params():,}\"]\n",
        "        ]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Table(\n",
        "                header=dict(values=['Metric', 'Value'], fill_color='lightgreen'),\n",
        "                cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"{model_name} - Model Performance Analysis\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        return fig, {'mse': mse, 'mae': mae, 'r2': r2, 'rmse': rmse}\n",
        "\n",
        "# Initialize benchmark analyzer\n",
        "benchmark_analyzer = TradingBenchmark()\n",
        "\n",
        "print(\"✅ Technical benchmark analyzer initialized!\")\n",
        "print(\"Available methods:\")\n",
        "print(\"- calculate_technical_metrics()\")\n",
        "print(\"- create_performance_dashboard()\")\n",
        "print(\"- create_model_performance_analysis()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Sample Trading Strategy Results for Benchmarking\n",
        "print(\"📊 Generating sample trading strategy results for benchmarking...\")\n",
        "\n",
        "# Create sample strategy returns (simulate a trading strategy)\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    # Use real data if available\n",
        "    sample_symbol = list(processed_data.keys())[0]\n",
        "    sample_data = processed_data[sample_symbol]\n",
        "    \n",
        "    # Generate strategy returns based on technical indicators\n",
        "    strategy_returns = []\n",
        "    for i in range(1, len(sample_data)):\n",
        "        # Simple strategy: Buy when RSI < 30, Sell when RSI > 70\n",
        "        current_rsi = sample_data['RSI'].iloc[i] if 'RSI' in sample_data.columns else 50\n",
        "        price_change = sample_data['Close'].iloc[i] / sample_data['Close'].iloc[i-1] - 1\n",
        "        \n",
        "        if current_rsi < 30:  # Oversold - Buy\n",
        "            strategy_returns.append(price_change * 1.0)  # Full position\n",
        "        elif current_rsi > 70:  # Overbought - Sell\n",
        "            strategy_returns.append(-price_change * 0.5)  # Short position\n",
        "        else:  # Hold\n",
        "            strategy_returns.append(0)\n",
        "    \n",
        "    strategy_returns = pd.Series(strategy_returns, index=sample_data.index[1:])\n",
        "    \n",
        "else:\n",
        "    # Generate synthetic strategy returns for demonstration\n",
        "    np.random.seed(42)\n",
        "    dates = pd.date_range(start='2022-01-01', end='2024-01-01', freq='D')\n",
        "    strategy_returns = pd.Series(np.random.normal(0.0005, 0.02, len(dates)), index=dates)\n",
        "\n",
        "# Generate benchmark returns (S&P 500 simulation)\n",
        "np.random.seed(123)\n",
        "benchmark_dates = strategy_returns.index\n",
        "benchmark_returns = pd.Series(np.random.normal(0.0003, 0.015, len(benchmark_dates)), index=benchmark_dates)\n",
        "\n",
        "print(f\"✅ Generated strategy returns: {len(strategy_returns)} days\")\n",
        "print(f\"✅ Generated benchmark returns: {len(benchmark_returns)} days\")\n",
        "print(f\"📈 Strategy total return: {(1 + strategy_returns).prod() - 1:.2%}\")\n",
        "print(f\"📈 Benchmark total return: {(1 + benchmark_returns).prod() - 1:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Comprehensive Performance Dashboard\n",
        "print(\"🎯 Creating comprehensive performance dashboard...\")\n",
        "\n",
        "# Generate performance dashboard\n",
        "performance_fig, strategy_metrics = benchmark_analyzer.create_performance_dashboard(\n",
        "    strategy_returns, \n",
        "    benchmark_returns, \n",
        "    \"CNN+LSTM Trading Strategy\"\n",
        ")\n",
        "\n",
        "# Display the dashboard\n",
        "performance_fig.show()\n",
        "\n",
        "# Print detailed metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 DETAILED PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"📈 Returns:\")\n",
        "print(f\"  • Total Return: {strategy_metrics['total_return']:.2%}\")\n",
        "print(f\"  • Annualized Return: {strategy_metrics['annualized_return']:.2%}\")\n",
        "\n",
        "print(f\"\\n📊 Risk Metrics:\")\n",
        "print(f\"  • Volatility: {strategy_metrics['volatility']:.2%}\")\n",
        "print(f\"  • Sharpe Ratio: {strategy_metrics['sharpe_ratio']:.2f}\")\n",
        "print(f\"  • Sortino Ratio: {strategy_metrics['sortino_ratio']:.2f}\")\n",
        "\n",
        "print(f\"\\n📉 Drawdown Analysis:\")\n",
        "print(f\"  • Max Drawdown: {strategy_metrics['max_drawdown']:.2%}\")\n",
        "print(f\"  • Calmar Ratio: {strategy_metrics['calmar_ratio']:.2f}\")\n",
        "\n",
        "print(f\"\\n🎯 Trading Performance:\")\n",
        "print(f\"  • Win Rate: {strategy_metrics['win_rate']:.2%}\")\n",
        "print(f\"  • Average Win: {strategy_metrics['avg_win']:.2%}\")\n",
        "print(f\"  • Average Loss: {strategy_metrics['avg_loss']:.2%}\")\n",
        "print(f\"  • Profit Factor: {strategy_metrics['profit_factor']:.2f}\")\n",
        "\n",
        "if 'alpha' in strategy_metrics:\n",
        "    print(f\"\\n📊 Benchmark Comparison:\")\n",
        "    print(f\"  • Alpha: {strategy_metrics['alpha']:.2%}\")\n",
        "    print(f\"  • Beta: {strategy_metrics['beta']:.2f}\")\n",
        "    print(f\"  • Information Ratio: {strategy_metrics['information_ratio']:.2f}\")\n",
        "    print(f\"  • Tracking Error: {strategy_metrics['tracking_error']:.2%}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Performance Analysis (if model was trained)\n",
        "if 'history' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
        "    print(\"🤖 Creating model performance analysis...\")\n",
        "    \n",
        "    # Generate model performance analysis\n",
        "    model_fig, model_metrics = benchmark_analyzer.create_model_performance_analysis(\n",
        "        history, X_test, y_test, \"CNN+LSTM Model\"\n",
        "    )\n",
        "    \n",
        "    # Display the analysis\n",
        "    model_fig.show()\n",
        "    \n",
        "    # Print model metrics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🤖 MODEL PERFORMANCE METRICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"📊 Accuracy Metrics:\")\n",
        "    print(f\"  • MSE: {model_metrics['mse']:.6f}\")\n",
        "    print(f\"  • RMSE: {model_metrics['rmse']:.6f}\")\n",
        "    print(f\"  • MAE: {model_metrics['mae']:.6f}\")\n",
        "    print(f\"  • R² Score: {model_metrics['r2']:.4f}\")\n",
        "    \n",
        "    print(f\"\\n📈 Model Quality:\")\n",
        "    if model_metrics['r2'] > 0.7:\n",
        "        print(\"  ✅ Excellent model performance (R² > 0.7)\")\n",
        "    elif model_metrics['r2'] > 0.5:\n",
        "        print(\"  ✅ Good model performance (R² > 0.5)\")\n",
        "    elif model_metrics['r2'] > 0.3:\n",
        "        print(\"  ⚠️ Moderate model performance (R² > 0.3)\")\n",
        "    else:\n",
        "        print(\"  ❌ Poor model performance (R² < 0.3)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No trained model found. Train a model first to see model performance analysis.\")\n",
        "    print(\"💡 Run the model training cell to generate model performance metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Technical Analysis and Comparisons\n",
        "print(\"📊 Creating additional technical analysis...\")\n",
        "\n",
        "# Create comparison with different strategies\n",
        "strategies = {\n",
        "    'CNN+LSTM Strategy': strategy_returns,\n",
        "    'Buy & Hold': benchmark_returns,\n",
        "    'Random Strategy': pd.Series(np.random.normal(0.0002, 0.025, len(strategy_returns)), index=strategy_returns.index)\n",
        "}\n",
        "\n",
        "# Calculate metrics for all strategies\n",
        "comparison_metrics = {}\n",
        "for name, returns in strategies.items():\n",
        "    comparison_metrics[name] = benchmark_analyzer.calculate_technical_metrics(returns, benchmark_returns)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for strategy_name, metrics in comparison_metrics.items():\n",
        "    comparison_data.append([\n",
        "        strategy_name,\n",
        "        f\"{metrics['total_return']:.2%}\",\n",
        "        f\"{metrics['annualized_return']:.2%}\",\n",
        "        f\"{metrics['volatility']:.2%}\",\n",
        "        f\"{metrics['sharpe_ratio']:.2f}\",\n",
        "        f\"{metrics['max_drawdown']:.2%}\",\n",
        "        f\"{metrics['win_rate']:.2%}\"\n",
        "    ])\n",
        "\n",
        "# Create comparison visualization\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=[\n",
        "        'Strategy Comparison Table',\n",
        "        'Risk-Return Scatter',\n",
        "        'Cumulative Returns Comparison',\n",
        "        'Rolling Sharpe Comparison'\n",
        "    ],\n",
        "    specs=[[{\"type\": \"table\"}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# 1. Comparison Table\n",
        "fig.add_trace(\n",
        "    go.Table(\n",
        "        header=dict(values=['Strategy', 'Total Return', 'Annual Return', 'Volatility', 'Sharpe', 'Max DD', 'Win Rate'],\n",
        "                   fill_color='lightblue'),\n",
        "        cells=dict(values=list(zip(*comparison_data)), fill_color='white')\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# 2. Risk-Return Scatter\n",
        "colors = ['blue', 'red', 'green']\n",
        "for i, (strategy_name, metrics) in enumerate(comparison_metrics.items()):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[metrics['volatility']], y=[metrics['annualized_return']],\n",
        "                  mode='markers', marker=dict(size=15, color=colors[i]),\n",
        "                  name=strategy_name),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# 3. Cumulative Returns Comparison\n",
        "for i, (strategy_name, returns) in enumerate(strategies.items()):\n",
        "    cumulative = (1 + returns).cumprod()\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=returns.index, y=cumulative.values,\n",
        "                  name=strategy_name, line=dict(color=colors[i])),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# 4. Rolling Sharpe Comparison\n",
        "for i, (strategy_name, returns) in enumerate(strategies.items()):\n",
        "    rolling_sharpe = returns.rolling(window=252).mean() / returns.rolling(window=252).std() * np.sqrt(252)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=returns.index, y=rolling_sharpe.values,\n",
        "                  name=f'{strategy_name} Sharpe', line=dict(color=colors[i])),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=1000,\n",
        "    title_text=\"Strategy Comparison Analysis\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Print strategy ranking\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 STRATEGY RANKING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Rank by Sharpe ratio\n",
        "ranked_strategies = sorted(comparison_metrics.items(), key=lambda x: x[1]['sharpe_ratio'], reverse=True)\n",
        "\n",
        "for i, (strategy_name, metrics) in enumerate(ranked_strategies):\n",
        "    rank_emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else \"📊\"\n",
        "    print(f\"{rank_emoji} #{i+1}: {strategy_name}\")\n",
        "    print(f\"   Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
        "    print(f\"   Annual Return: {metrics['annualized_return']:.2%}\")\n",
        "    print(f\"   Max Drawdown: {metrics['max_drawdown']:.2%}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Management and Versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model management and versioning utilities\n",
        "\n",
        "def list_saved_models():\n",
        "    \"\"\"List all saved models in different locations\"\"\"\n",
        "    \n",
        "    print(\"🔍 Searching for saved models...\")\n",
        "    \n",
        "    # Check local\n",
        "    if os.path.exists('cnn_lstm_model.h5'):\n",
        "        print(\"📁 Local: cnn_lstm_model.h5\")\n",
        "    \n",
        "    # Check Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        if os.path.exists(drive_folder):\n",
        "            drive_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            if drive_models:\n",
        "                print(f\"☁️ Google Drive ({len(drive_models)} models):\")\n",
        "                for model in sorted(drive_models):\n",
        "                    print(f\"   - {model}\")\n",
        "            else:\n",
        "                print(\"☁️ Google Drive: No models found\")\n",
        "        else:\n",
        "            print(\"☁️ Google Drive: Trading_Strategy_ML folder not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"☁️ Google Drive: Error accessing - {e}\")\n",
        "    \n",
        "    # Check GitHub repository\n",
        "    if os.path.exists('models'):\n",
        "        git_models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        if git_models:\n",
        "            print(f\"🐙 GitHub ({len(git_models)} models):\")\n",
        "            for model in sorted(git_models):\n",
        "                print(f\"   - {model}\")\n",
        "        else:\n",
        "            print(\"🐙 GitHub: No models found in models/ directory\")\n",
        "    else:\n",
        "        print(\"🐙 GitHub: models/ directory not found\")\n",
        "\n",
        "def create_model_backup():\n",
        "    \"\"\"Create a backup of the current model with metadata\"\"\"\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Create backup with metadata\n",
        "    backup_info = {\n",
        "        'timestamp': timestamp,\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'model_type': 'CNN+LSTM',\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'description': 'Trading Strategy ML Model'\n",
        "    }\n",
        "    \n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'model_metadata_{timestamp}.json', 'w') as f:\n",
        "        json.dump(backup_info, f, indent=2)\n",
        "    \n",
        "    print(f\"📋 Model metadata saved: model_metadata_{timestamp}.json\")\n",
        "    print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"🕒 Backup timestamp: {timestamp}\")\n",
        "\n",
        "def cleanup_old_models(keep_last_n=5):\n",
        "    \"\"\"Clean up old models, keeping only the last N versions\"\"\"\n",
        "    \n",
        "    print(f\"🧹 Cleaning up old models (keeping last {keep_last_n})...\")\n",
        "    \n",
        "    # Clean Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        \n",
        "        if os.path.exists(drive_folder):\n",
        "            models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            models.sort(reverse=True)  # Sort by name (newest first)\n",
        "            \n",
        "            if len(models) > keep_last_n:\n",
        "                models_to_delete = models[keep_last_n:]\n",
        "                for model in models_to_delete:\n",
        "                    os.remove(os.path.join(drive_folder, model))\n",
        "                    print(f\"🗑️ Deleted old model: {model}\")\n",
        "                print(f\"✅ Kept {keep_last_n} latest models in Google Drive\")\n",
        "            else:\n",
        "                print(f\"✅ Google Drive has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning Google Drive: {e}\")\n",
        "    \n",
        "    # Clean local models directory\n",
        "    if os.path.exists('models'):\n",
        "        models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        models.sort(reverse=True)\n",
        "        \n",
        "        if len(models) > keep_last_n:\n",
        "            models_to_delete = models[keep_last_n:]\n",
        "            for model in models_to_delete:\n",
        "                os.remove(os.path.join('models', model))\n",
        "                print(f\"🗑️ Deleted old model: {model}\")\n",
        "            print(f\"✅ Kept {keep_last_n} latest models locally\")\n",
        "        else:\n",
        "            print(f\"✅ Local models directory has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "\n",
        "# Run model management functions\n",
        "print(\"=\" * 50)\n",
        "print(\"📊 MODEL MANAGEMENT DASHBOARD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "list_saved_models()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "create_model_backup()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "cleanup_old_models(keep_last_n=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
