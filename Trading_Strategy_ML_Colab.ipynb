{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trading Strategy ML - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Multi-Factor Momentum Trading Strategy with ML Enhancement on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- Real-time data collection\n",
        "- Advanced ML models (CNN+LSTM)\n",
        "- Comprehensive backtesting\n",
        "- Performance analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Enable GPU memory growth\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        for gpu in tf.config.list_physical_devices('GPU'):\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with error handling and alternatives\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, alternative=None):\n",
        "    \"\"\"Install package with fallback to alternative if needed\"\"\"\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"✓ {package} installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Failed to install {package}: {e}\")\n",
        "        if alternative:\n",
        "            try:\n",
        "                print(f\"Trying alternative: {alternative}\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", alternative])\n",
        "                print(f\"✓ {alternative} installed successfully\")\n",
        "                return True\n",
        "            except subprocess.CalledProcessError as e2:\n",
        "                print(f\"✗ Alternative {alternative} also failed: {e2}\")\n",
        "        return False\n",
        "\n",
        "# Core packages (usually work fine)\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",\n",
        "    \"numpy>=1.21.0\", \n",
        "    \"scipy>=1.9.0\",\n",
        "    \"matplotlib>=3.5.0\",\n",
        "    \"seaborn>=0.11.0\",\n",
        "    \"plotly>=5.10.0\",\n",
        "    \"requests>=2.28.0\",\n",
        "    \"tqdm>=4.64.0\",\n",
        "    \"joblib>=1.1.0\",\n",
        "    \"python-dotenv>=0.19.0\"\n",
        "]\n",
        "\n",
        "# Financial data packages\n",
        "financial_packages = [\n",
        "    (\"yfinance>=0.2.0\", None),\n",
        "    (\"alpha-vantage>=2.3.0\", None),\n",
        "    (\"pandas-datareader>=0.10.0\", None)\n",
        "]\n",
        "\n",
        "# Technical analysis (problematic package)\n",
        "ta_packages = [\n",
        "    (\"TA-Lib>=0.4.25\", \"talib-binary>=0.4.19\")\n",
        "]\n",
        "\n",
        "# ML packages\n",
        "ml_packages = [\n",
        "    (\"tensorflow>=2.10.0\", \"tensorflow-gpu>=2.10.0\"),\n",
        "    (\"torch>=1.12.0\", None),\n",
        "    (\"torchvision>=0.13.0\", None),\n",
        "    (\"scikit-learn>=1.1.0\", None),\n",
        "    (\"xgboost>=1.6.0\", None),\n",
        "    (\"optuna>=3.0.0\", None),\n",
        "    (\"lightgbm>=3.3.0\", None)\n",
        "]\n",
        "\n",
        "# Financial analysis packages\n",
        "analysis_packages = [\n",
        "    (\"backtrader>=1.9.76\", None),\n",
        "    (\"arch>=5.3.0\", None),\n",
        "    (\"empyrical>=0.5.5\", None),\n",
        "    (\"ffn>=0.3.7\", None)\n",
        "]\n",
        "\n",
        "# UI packages\n",
        "ui_packages = [\n",
        "    (\"streamlit>=1.12.0\", None),\n",
        "    (\"dash>=2.6.0\", None)\n",
        "]\n",
        "\n",
        "print(\"🚀 Starting package installation...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Install core packages first\n",
        "print(\"\\n📦 Installing core packages...\")\n",
        "for package in core_packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Install financial packages\n",
        "print(\"\\n💰 Installing financial data packages...\")\n",
        "for package, alt in financial_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install TA-Lib with special handling\n",
        "print(\"\\n📊 Installing technical analysis packages...\")\n",
        "ta_success = False\n",
        "for package, alt in ta_packages:\n",
        "    if install_package(package, alt):\n",
        "        ta_success = True\n",
        "        break\n",
        "\n",
        "if not ta_success:\n",
        "    print(\"⚠️ TA-Lib installation failed. Using alternative approach...\")\n",
        "    # Try installing from conda-forge\n",
        "    try:\n",
        "        subprocess.check_call([\"pip\", \"install\", \"-q\", \"TA-Lib\"])\n",
        "        print(\"✓ TA-Lib installed via alternative method\")\n",
        "        ta_success = True\n",
        "    except:\n",
        "        print(\"⚠️ TA-Lib still failed. Some technical indicators may not work.\")\n",
        "\n",
        "# Install ML packages\n",
        "print(\"\\n🤖 Installing machine learning packages...\")\n",
        "for package, alt in ml_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install analysis packages\n",
        "print(\"\\n📈 Installing financial analysis packages...\")\n",
        "for package, alt in analysis_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install UI packages\n",
        "print(\"\\n🖥️ Installing UI packages...\")\n",
        "for package, alt in ui_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✅ Package installation complete!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test critical imports\n",
        "print(\"\\n🧪 Testing critical imports...\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    import yfinance as yf\n",
        "    print(\"✓ Core packages imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some packages failed to import: {e}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"\\n🎮 GPU Status:\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CUDA available: {tf.test.is_built_with_cuda()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Simplified Installation (if above fails)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified installation - run this if the above cell fails\n",
        "# This installs only the essential packages needed to run the trading strategy\n",
        "\n",
        "print(\"🔧 Installing essential packages only...\")\n",
        "\n",
        "# Essential packages that usually work in Colab\n",
        "essential_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"plotly\",\n",
        "    \"yfinance\",\n",
        "    \"tensorflow\",\n",
        "    \"scikit-learn\",\n",
        "    \"requests\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in essential_packages:\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        !pip install -q {package}\n",
        "        print(f\"✓ {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {package} failed: {e}\")\n",
        "\n",
        "print(\"\\n✅ Essential packages installation complete!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import plotly.graph_objects as go\n",
        "    import yfinance as yf\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    print(\"\\n🎉 All essential packages imported successfully!\")\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\n⚠️ Some packages failed to import: {e}\")\n",
        "    print(\"You may need to restart the runtime and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/CatalinMoldova/trading-strategy-ml.git\n",
        "\n",
        "# Change to the project directory\n",
        "%cd trading-strategy-ml\n",
        "\n",
        "# Install project requirements\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "print(\"Repository cloned and requirements installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import project modules with correct class names and error handling\n",
        "try:\n",
        "    from data_pipeline.market_data_collector import MarketDataCollector\n",
        "    from data_pipeline.indicator_engine import TechnicalIndicatorEngine\n",
        "    from data_pipeline.feature_engineer import FeatureEngineer\n",
        "    from ml_models.cnn_lstm_model import CNNLSTMModel\n",
        "    from ml_models.random_forest_model import RandomForestModel\n",
        "    from ml_models.ensemble_predictor import EnsemblePredictor\n",
        "    from strategy.signal_generator import MultiFactorSignalGenerator\n",
        "    from strategy.position_sizer import PositionSizer\n",
        "    from strategy.risk_manager import RiskManager\n",
        "    from backtesting.backtest_engine import BacktestEngine\n",
        "    from backtesting.performance_analyzer import PerformanceAnalyzer\n",
        "    print(\"✅ All project modules imported successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some project modules failed to import: {e}\")\n",
        "    print(\"Using simplified implementations instead...\")\n",
        "    \n",
        "    # Create simplified fallback classes\n",
        "    class MarketDataCollector:\n",
        "        def get_historical_data(self, symbol, period='2y', interval='1d'):\n",
        "            import yfinance as yf\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            return ticker.history(period=period, interval=interval)\n",
        "    \n",
        "    class TechnicalIndicatorEngine:\n",
        "        def calculate_all_indicators(self, df):\n",
        "            # Simple technical indicators without TA-Lib\n",
        "            df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "            df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "            df['RSI'] = self._calculate_rsi(df['Close'])\n",
        "            df['MACD'] = df['Close'].ewm(span=12).mean() - df['Close'].ewm(span=26).mean()\n",
        "            return df\n",
        "        \n",
        "        def _calculate_rsi(self, prices, period=14):\n",
        "            delta = prices.diff()\n",
        "            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "            rs = gain / loss\n",
        "            return 100 - (100 / (1 + rs))\n",
        "    \n",
        "    class FeatureEngineer:\n",
        "        def create_features(self, df):\n",
        "            # Simple feature engineering\n",
        "            df['Price_Change'] = df['Close'].pct_change()\n",
        "            df['Volume_Change'] = df['Volume'].pct_change()\n",
        "            df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "            df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "            return df\n",
        "    \n",
        "    class CNNLSTMModel:\n",
        "        def __init__(self, time_steps=60, n_features=20, learning_rate=0.001):\n",
        "            self.time_steps = time_steps\n",
        "            self.n_features = n_features\n",
        "            self.learning_rate = learning_rate\n",
        "            self.model = None\n",
        "        \n",
        "        def build_model(self):\n",
        "            import tensorflow as tf\n",
        "            from tensorflow.keras.models import Sequential\n",
        "            from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "            \n",
        "            model = Sequential([\n",
        "                LSTM(50, return_sequences=True, input_shape=(self.time_steps, self.n_features)),\n",
        "                Dropout(0.2),\n",
        "                LSTM(50, return_sequences=False),\n",
        "                Dropout(0.2),\n",
        "                Dense(25),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            \n",
        "            model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "            self.model = model\n",
        "            return model\n",
        "        \n",
        "        def train(self, X_train, y_train, epochs=10, batch_size=32, validation_split=0.2):\n",
        "            return self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
        "                                validation_split=validation_split, verbose=1)\n",
        "        \n",
        "        def save_model(self, path):\n",
        "            self.model.save(path)\n",
        "    \n",
        "    class RandomForestModel:\n",
        "        def __init__(self):\n",
        "            self.model = None\n",
        "        \n",
        "        def train(self, X_train, y_train):\n",
        "            from sklearn.ensemble import RandomForestRegressor\n",
        "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "            self.model.fit(X_train, y_train)\n",
        "        \n",
        "        def get_feature_importance(self):\n",
        "            if self.model:\n",
        "                return list(zip(range(len(self.model.feature_importances_)), \n",
        "                              self.model.feature_importances_))\n",
        "            return []\n",
        "        \n",
        "        def save_model(self, path):\n",
        "            import joblib\n",
        "            joblib.dump(self.model, path)\n",
        "    \n",
        "    class EnsemblePredictor:\n",
        "        def __init__(self):\n",
        "            self.models = {}\n",
        "            self.weights = {}\n",
        "        \n",
        "        def add_model(self, name, model, weight=1.0):\n",
        "            self.models[name] = model\n",
        "            self.weights[name] = weight\n",
        "        \n",
        "        def predict(self, X):\n",
        "            predictions = []\n",
        "            for name, model in self.models.items():\n",
        "                if hasattr(model, 'predict'):\n",
        "                    pred = model.predict(X)\n",
        "                    predictions.append(pred * self.weights[name])\n",
        "            return np.mean(predictions, axis=0) if predictions else np.zeros(len(X))\n",
        "    \n",
        "    class MultiFactorSignalGenerator:\n",
        "        def generate_signals(self, df, predictions):\n",
        "            # Simple signal generation\n",
        "            signals = pd.DataFrame(index=df.index)\n",
        "            signals['signal'] = 0\n",
        "            signals.loc[predictions > 0.01, 'signal'] = 1  # Buy\n",
        "            signals.loc[predictions < -0.01, 'signal'] = -1  # Sell\n",
        "            return signals\n",
        "    \n",
        "    class PositionSizer:\n",
        "        def __init__(self):\n",
        "            pass\n",
        "    \n",
        "    class RiskManager:\n",
        "        def __init__(self):\n",
        "            pass\n",
        "    \n",
        "    class BacktestEngine:\n",
        "        def __init__(self, initial_capital=100000, commission=0.001, slippage=0.0005):\n",
        "            self.initial_capital = initial_capital\n",
        "            self.commission = commission\n",
        "            self.slippage = slippage\n",
        "        \n",
        "        def run_backtest(self, price_data, signals):\n",
        "            # Simple backtest implementation\n",
        "            portfolio_value = self.initial_capital\n",
        "            positions = 0\n",
        "            \n",
        "            results = []\n",
        "            for date, row in price_data.iterrows():\n",
        "                if date in signals.index:\n",
        "                    signal = signals.loc[date, 'signal']\n",
        "                    price = row['Close']\n",
        "                    \n",
        "                    if signal == 1 and positions == 0:  # Buy\n",
        "                        positions = portfolio_value / price\n",
        "                        portfolio_value = 0\n",
        "                    elif signal == -1 and positions > 0:  # Sell\n",
        "                        portfolio_value = positions * price\n",
        "                        positions = 0\n",
        "                \n",
        "                current_value = portfolio_value + (positions * row['Close'] if positions > 0 else 0)\n",
        "                results.append(current_value)\n",
        "            \n",
        "            return {\n",
        "                'total_return': (results[-1] - self.initial_capital) / self.initial_capital,\n",
        "                'portfolio_value': results[-1]\n",
        "            }\n",
        "    \n",
        "    class PerformanceAnalyzer:\n",
        "        def calculate_portfolio_performance(self, backtest_results):\n",
        "            # Simple performance calculation\n",
        "            total_return = sum(result['total_return'] for result in backtest_results.values())\n",
        "            return {\n",
        "                'total_return': total_return,\n",
        "                'annualized_return': total_return * 0.5,  # Assuming 2-year period\n",
        "                'volatility': 0.15,  # Placeholder\n",
        "                'sharpe_ratio': total_return / 0.15 if total_return > 0 else 0,\n",
        "                'sortino_ratio': total_return / 0.10 if total_return > 0 else 0,\n",
        "                'max_drawdown': -0.05,  # Placeholder\n",
        "                'calmar_ratio': total_return / 0.05 if total_return > 0 else 0,\n",
        "                'cumulative_returns': pd.Series([0, total_return]),\n",
        "                'drawdown': pd.Series([0, -0.05]),\n",
        "                'rolling_sharpe': pd.Series([0, total_return / 0.15]),\n",
        "                'monthly_returns': pd.Series([0, total_return / 24])\n",
        "            }\n",
        "    \n",
        "    print(\"✅ Fallback implementations created!\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data collector and indicator engine\n",
        "collector = MarketDataCollector()\n",
        "indicator_engine = TechnicalIndicatorEngine()\n",
        "\n",
        "# Define symbols to trade\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
        "\n",
        "# Collect historical data\n",
        "print(\"Collecting historical data...\")\n",
        "data = {}\n",
        "for symbol in symbols:\n",
        "    try:\n",
        "        df = collector.get_historical_data(symbol, period='2y', interval='1d')\n",
        "        if df is not None and len(df) > 0:\n",
        "            data[symbol] = df\n",
        "            print(f\"✓ {symbol}: {len(df)} records\")\n",
        "        else:\n",
        "            print(f\"✗ {symbol}: No data received\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {symbol}: Error - {e}\")\n",
        "\n",
        "print(f\"\\nData collection complete! Collected data for {len(data)} symbols.\")\n",
        "\n",
        "# Process data with technical indicators\n",
        "if data:\n",
        "    print(\"\\nProcessing data with technical indicators...\")\n",
        "    processed_data = {}\n",
        "    for symbol, df in data.items():\n",
        "        try:\n",
        "            # Calculate technical indicators\n",
        "            df_with_indicators = indicator_engine.calculate_all_indicators(df)\n",
        "            processed_data[symbol] = df_with_indicators\n",
        "            print(f\"✓ {symbol}: Technical indicators calculated\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {symbol}: Error calculating indicators - {e}\")\n",
        "            processed_data[symbol] = df  # Use original data if indicators fail\n",
        "    \n",
        "    print(f\"\\nData processing complete! Processed {len(processed_data)} symbols.\")\n",
        "    \n",
        "    # Display sample data\n",
        "    sample_symbol = list(processed_data.keys())[0]\n",
        "    print(f\"\\nSample processed data for {sample_symbol}:\")\n",
        "    print(processed_data[sample_symbol][['Close', 'SMA_20', 'RSI', 'MACD']].head())\n",
        "else:\n",
        "    print(\"⚠️ No data collected. Check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN+LSTM model with GPU\n",
        "print(\"Training CNN+LSTM model...\")\n",
        "\n",
        "# Initialize model\n",
        "cnn_lstm = CNNLSTMModel(\n",
        "    time_steps=60,\n",
        "    n_features=8,  # Adjusted based on available features\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Build model\n",
        "model = cnn_lstm.build_model()\n",
        "print(f\"Model built with {model.count_params()} parameters\")\n",
        "\n",
        "# Prepare training data from processed data\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    print(\"Preparing training data from collected market data...\")\n",
        "    \n",
        "    # Combine all data for training\n",
        "    all_data = []\n",
        "    for symbol, df in processed_data.items():\n",
        "        df['symbol'] = symbol\n",
        "        all_data.append(df)\n",
        "    \n",
        "    combined_data = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {combined_data.shape}\")\n",
        "    \n",
        "    # Select features for training\n",
        "    feature_columns = ['Close', 'SMA_20', 'RSI', 'MACD', 'Price_Change', 'Volume_Change', 'High_Low_Ratio', 'Close_Open_Ratio']\n",
        "    available_features = [col for col in feature_columns if col in combined_data.columns]\n",
        "    \n",
        "    if len(available_features) >= 4:  # Need at least 4 features\n",
        "        # Prepare features and targets\n",
        "        X_data = combined_data[available_features].values\n",
        "        y_data = combined_data['Close'].shift(-1).values  # Predict next day's close\n",
        "        \n",
        "        # Remove NaN values\n",
        "        valid_indices = ~np.isnan(X_data).any(axis=1) & ~np.isnan(y_data)\n",
        "        X_data = X_data[valid_indices]\n",
        "        y_data = y_data[valid_indices]\n",
        "        \n",
        "        # Reshape for LSTM (samples, time_steps, features)\n",
        "        n_samples = len(X_data) - cnn_lstm.time_steps + 1\n",
        "        X_reshaped = np.zeros((n_samples, cnn_lstm.time_steps, len(available_features)))\n",
        "        y_reshaped = np.zeros((n_samples, 1))\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            X_reshaped[i] = X_data[i:i+cnn_lstm.time_steps]\n",
        "            y_reshaped[i] = y_data[i+cnn_lstm.time_steps-1]\n",
        "        \n",
        "        # Split data\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_reshaped, y_reshaped, test_size=0.2, random_state=42, shuffle=False\n",
        "        )\n",
        "        \n",
        "        print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "        print(f\"Test data shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "        \n",
        "        # Train model\n",
        "        history = cnn_lstm.train(\n",
        "            X_train, y_train,\n",
        "            epochs=20,  # Increased for better training\n",
        "            batch_size=32,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "        \n",
        "        print(\"CNN+LSTM training complete!\")\n",
        "        \n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE')\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "        plt.title('Model MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('MAE')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Evaluate model\n",
        "        test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"\\nModel Evaluation:\")\n",
        "        print(f\"Test Loss: {test_loss:.6f}\")\n",
        "        print(f\"Test MAE: {test_mae:.6f}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"⚠️ Insufficient features for training. Using example data...\")\n",
        "        # Fallback to example data\n",
        "        X_train = np.random.randn(1000, 60, 8)\n",
        "        y_train = np.random.randn(1000, 1)\n",
        "        \n",
        "        history = cnn_lstm.train(\n",
        "            X_train, y_train,\n",
        "            epochs=10,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "        \n",
        "        print(\"CNN+LSTM training complete with example data!\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ No processed data available. Using example data...\")\n",
        "    # Fallback to example data\n",
        "    X_train = np.random.randn(1000, 60, 8)\n",
        "    y_train = np.random.randn(1000, 1)\n",
        "    \n",
        "    history = cnn_lstm.train(\n",
        "        X_train, y_train,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "    \n",
        "    print(\"CNN+LSTM training complete with example data!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Your Work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models to multiple locations for permanent storage\n",
        "\n",
        "# 1. Save locally first\n",
        "cnn_lstm.save_model('cnn_lstm_model.h5')\n",
        "print(\"✓ Model saved locally\")\n",
        "\n",
        "# 2. Save to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your trading models\n",
        "drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Save model to Google Drive with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_model_path = f'{drive_folder}/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', drive_model_path)\n",
        "print(f\"✓ Model saved to Google Drive: {drive_model_path}\")\n",
        "\n",
        "# 3. Save to GitHub (push to repository)\n",
        "import subprocess\n",
        "\n",
        "# Configure git (if not already done)\n",
        "subprocess.run(['git', 'config', '--global', 'user.email', 'your-email@example.com'], check=False)\n",
        "subprocess.run(['git', 'config', '--global', 'user.name', 'CatalinMoldova'], check=False)\n",
        "\n",
        "# Create models directory in git\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Copy model to git directory\n",
        "git_model_path = f'models/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', git_model_path)\n",
        "\n",
        "# Add, commit, and push to GitHub\n",
        "subprocess.run(['git', 'add', git_model_path], check=True)\n",
        "subprocess.run(['git', 'commit', '-m', f'Add trained CNN+LSTM model - {timestamp}'], check=True)\n",
        "subprocess.run(['git', 'push', 'origin', 'main'], check=True)\n",
        "print(f\"✓ Model pushed to GitHub: {git_model_path}\")\n",
        "\n",
        "# 4. Download to local machine as backup\n",
        "from google.colab import files\n",
        "files.download('cnn_lstm_model.h5')\n",
        "print(\"✓ Model downloaded to your local machine\")\n",
        "\n",
        "print(\"\\n🎉 Model saved in 4 locations:\")\n",
        "print(\"1. Local Colab environment\")\n",
        "print(\"2. Google Drive (permanent)\")\n",
        "print(\"3. GitHub repository (permanent)\")\n",
        "print(\"4. Your local machine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Saved Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models from different storage locations\n",
        "\n",
        "def load_model_from_location(location_type, model_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained model from different storage locations\n",
        "    \n",
        "    Args:\n",
        "        location_type: 'local', 'drive', 'github', or 'url'\n",
        "        model_path: Path to the model file (optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    if location_type == 'local':\n",
        "        # Load from local Colab environment\n",
        "        if model_path is None:\n",
        "            model_path = 'cnn_lstm_model.h5'\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from local: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'drive':\n",
        "        # Load from Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        if model_path is None:\n",
        "            # List available models in Drive\n",
        "            drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "            if os.path.exists(drive_folder):\n",
        "                models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(drive_folder, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in Google Drive\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Trading_Strategy_ML folder not found in Google Drive\")\n",
        "                return None\n",
        "        \n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from Google Drive: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'github':\n",
        "        # Load from GitHub (if model is in repository)\n",
        "        if model_path is None:\n",
        "            models_dir = 'models'\n",
        "            if os.path.exists(models_dir):\n",
        "                models = [f for f in os.listdir(models_dir) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(models_dir, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in GitHub repository\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Models directory not found\")\n",
        "                return None\n",
        "        \n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from GitHub: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'url':\n",
        "        # Load from URL (if you have a direct link)\n",
        "        if model_path is None:\n",
        "            print(\"Please provide a URL to the model file\")\n",
        "            return None\n",
        "        \n",
        "        import urllib.request\n",
        "        local_path = 'downloaded_model.h5'\n",
        "        urllib.request.urlretrieve(model_path, local_path)\n",
        "        model = tf.keras.models.load_model(local_path)\n",
        "        print(f\"✓ Model loaded from URL: {model_path}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"Invalid location_type. Use 'local', 'drive', 'github', or 'url'\")\n",
        "        return None\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Example: Load the latest model from Google Drive\n",
        "print(\"Loading model from Google Drive...\")\n",
        "loaded_model = load_model_from_location('drive')\n",
        "\n",
        "if loaded_model is not None:\n",
        "    print(f\"Model summary:\")\n",
        "    loaded_model.summary()\n",
        "else:\n",
        "    print(\"No model found. Train a model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Management and Versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model management and versioning utilities\n",
        "\n",
        "def list_saved_models():\n",
        "    \"\"\"List all saved models in different locations\"\"\"\n",
        "    \n",
        "    print(\"🔍 Searching for saved models...\")\n",
        "    \n",
        "    # Check local\n",
        "    if os.path.exists('cnn_lstm_model.h5'):\n",
        "        print(\"📁 Local: cnn_lstm_model.h5\")\n",
        "    \n",
        "    # Check Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        if os.path.exists(drive_folder):\n",
        "            drive_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            if drive_models:\n",
        "                print(f\"☁️ Google Drive ({len(drive_models)} models):\")\n",
        "                for model in sorted(drive_models):\n",
        "                    print(f\"   - {model}\")\n",
        "            else:\n",
        "                print(\"☁️ Google Drive: No models found\")\n",
        "        else:\n",
        "            print(\"☁️ Google Drive: Trading_Strategy_ML folder not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"☁️ Google Drive: Error accessing - {e}\")\n",
        "    \n",
        "    # Check GitHub repository\n",
        "    if os.path.exists('models'):\n",
        "        git_models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        if git_models:\n",
        "            print(f\"🐙 GitHub ({len(git_models)} models):\")\n",
        "            for model in sorted(git_models):\n",
        "                print(f\"   - {model}\")\n",
        "        else:\n",
        "            print(\"🐙 GitHub: No models found in models/ directory\")\n",
        "    else:\n",
        "        print(\"🐙 GitHub: models/ directory not found\")\n",
        "\n",
        "def create_model_backup():\n",
        "    \"\"\"Create a backup of the current model with metadata\"\"\"\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Create backup with metadata\n",
        "    backup_info = {\n",
        "        'timestamp': timestamp,\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'model_type': 'CNN+LSTM',\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'description': 'Trading Strategy ML Model'\n",
        "    }\n",
        "    \n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'model_metadata_{timestamp}.json', 'w') as f:\n",
        "        json.dump(backup_info, f, indent=2)\n",
        "    \n",
        "    print(f\"📋 Model metadata saved: model_metadata_{timestamp}.json\")\n",
        "    print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"🕒 Backup timestamp: {timestamp}\")\n",
        "\n",
        "def cleanup_old_models(keep_last_n=5):\n",
        "    \"\"\"Clean up old models, keeping only the last N versions\"\"\"\n",
        "    \n",
        "    print(f\"🧹 Cleaning up old models (keeping last {keep_last_n})...\")\n",
        "    \n",
        "    # Clean Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        \n",
        "        if os.path.exists(drive_folder):\n",
        "            models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            models.sort(reverse=True)  # Sort by name (newest first)\n",
        "            \n",
        "            if len(models) > keep_last_n:\n",
        "                models_to_delete = models[keep_last_n:]\n",
        "                for model in models_to_delete:\n",
        "                    os.remove(os.path.join(drive_folder, model))\n",
        "                    print(f\"🗑️ Deleted old model: {model}\")\n",
        "                print(f\"✅ Kept {keep_last_n} latest models in Google Drive\")\n",
        "            else:\n",
        "                print(f\"✅ Google Drive has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning Google Drive: {e}\")\n",
        "    \n",
        "    # Clean local models directory\n",
        "    if os.path.exists('models'):\n",
        "        models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        models.sort(reverse=True)\n",
        "        \n",
        "        if len(models) > keep_last_n:\n",
        "            models_to_delete = models[keep_last_n:]\n",
        "            for model in models_to_delete:\n",
        "                os.remove(os.path.join('models', model))\n",
        "                print(f\"🗑️ Deleted old model: {model}\")\n",
        "            print(f\"✅ Kept {keep_last_n} latest models locally\")\n",
        "        else:\n",
        "            print(f\"✅ Local models directory has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "\n",
        "# Run model management functions\n",
        "print(\"=\" * 50)\n",
        "print(\"📊 MODEL MANAGEMENT DASHBOARD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "list_saved_models()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "create_model_backup()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "cleanup_old_models(keep_last_n=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
