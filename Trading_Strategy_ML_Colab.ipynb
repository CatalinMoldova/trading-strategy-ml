{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trading Strategy ML - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Multi-Factor Momentum Trading Strategy with ML Enhancement on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- Real-time data collection\n",
        "- Advanced ML models (CNN+LSTM)\n",
        "- Comprehensive backtesting\n",
        "- Performance analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Enable GPU memory growth\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        for gpu in tf.config.list_physical_devices('GPU'):\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with error handling and alternatives\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package, alternative=None):\n",
        "    \"\"\"Install package with fallback to alternative if needed\"\"\"\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"✓ {package} installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Failed to install {package}: {e}\")\n",
        "        if alternative:\n",
        "            try:\n",
        "                print(f\"Trying alternative: {alternative}\")\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", alternative])\n",
        "                print(f\"✓ {alternative} installed successfully\")\n",
        "                return True\n",
        "            except subprocess.CalledProcessError as e2:\n",
        "                print(f\"✗ Alternative {alternative} also failed: {e2}\")\n",
        "        return False\n",
        "\n",
        "# Core packages (usually work fine)\n",
        "core_packages = [\n",
        "    \"pandas>=1.5.0\",\n",
        "    \"numpy>=1.21.0\", \n",
        "    \"scipy>=1.9.0\",\n",
        "    \"matplotlib>=3.5.0\",\n",
        "    \"seaborn>=0.11.0\",\n",
        "    \"plotly>=5.10.0\",\n",
        "    \"requests>=2.28.0\",\n",
        "    \"tqdm>=4.64.0\",\n",
        "    \"joblib>=1.1.0\",\n",
        "    \"python-dotenv>=0.19.0\"\n",
        "]\n",
        "\n",
        "# Financial data packages\n",
        "financial_packages = [\n",
        "    (\"yfinance>=0.2.0\", None),\n",
        "    (\"alpha-vantage>=2.3.0\", None),\n",
        "    (\"pandas-datareader>=0.10.0\", None)\n",
        "]\n",
        "\n",
        "# Technical analysis (problematic package)\n",
        "ta_packages = [\n",
        "    (\"TA-Lib>=0.4.25\", \"talib-binary>=0.4.19\")\n",
        "]\n",
        "\n",
        "# ML packages\n",
        "ml_packages = [\n",
        "    (\"tensorflow>=2.10.0\", \"tensorflow-gpu>=2.10.0\"),\n",
        "    (\"torch>=1.12.0\", None),\n",
        "    (\"torchvision>=0.13.0\", None),\n",
        "    (\"scikit-learn>=1.1.0\", None),\n",
        "    (\"xgboost>=1.6.0\", None),\n",
        "    (\"optuna>=3.0.0\", None),\n",
        "    (\"lightgbm>=3.3.0\", None)\n",
        "]\n",
        "\n",
        "# Financial analysis packages\n",
        "analysis_packages = [\n",
        "    (\"backtrader>=1.9.76\", None),\n",
        "    (\"arch>=5.3.0\", None),\n",
        "    (\"empyrical>=0.5.5\", None),\n",
        "    (\"ffn>=0.3.7\", None)\n",
        "]\n",
        "\n",
        "# UI packages\n",
        "ui_packages = [\n",
        "    (\"streamlit>=1.12.0\", None),\n",
        "    (\"dash>=2.6.0\", None)\n",
        "]\n",
        "\n",
        "print(\"🚀 Starting package installation...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Install core packages first\n",
        "print(\"\\n📦 Installing core packages...\")\n",
        "for package in core_packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Install financial packages\n",
        "print(\"\\n💰 Installing financial data packages...\")\n",
        "for package, alt in financial_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install TA-Lib with special handling\n",
        "print(\"\\n📊 Installing technical analysis packages...\")\n",
        "ta_success = False\n",
        "for package, alt in ta_packages:\n",
        "    if install_package(package, alt):\n",
        "        ta_success = True\n",
        "        break\n",
        "\n",
        "if not ta_success:\n",
        "    print(\"⚠️ TA-Lib installation failed. Using alternative approach...\")\n",
        "    # Try installing from conda-forge\n",
        "    try:\n",
        "        subprocess.check_call([\"pip\", \"install\", \"-q\", \"TA-Lib\"])\n",
        "        print(\"✓ TA-Lib installed via alternative method\")\n",
        "        ta_success = True\n",
        "    except:\n",
        "        print(\"⚠️ TA-Lib still failed. Some technical indicators may not work.\")\n",
        "\n",
        "# Install ML packages\n",
        "print(\"\\n🤖 Installing machine learning packages...\")\n",
        "for package, alt in ml_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install analysis packages\n",
        "print(\"\\n📈 Installing financial analysis packages...\")\n",
        "for package, alt in analysis_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "# Install UI packages\n",
        "print(\"\\n🖥️ Installing UI packages...\")\n",
        "for package, alt in ui_packages:\n",
        "    install_package(package, alt)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✅ Package installation complete!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test critical imports\n",
        "print(\"\\n🧪 Testing critical imports...\")\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    import yfinance as yf\n",
        "    print(\"✓ Core packages imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some packages failed to import: {e}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"\\n🎮 GPU Status:\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CUDA available: {tf.test.is_built_with_cuda()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Simplified Installation (if above fails)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified installation - run this if the above cell fails\n",
        "# This installs only the essential packages needed to run the trading strategy\n",
        "\n",
        "print(\"🔧 Installing essential packages only...\")\n",
        "\n",
        "# Essential packages that usually work in Colab\n",
        "essential_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"plotly\",\n",
        "    \"yfinance\",\n",
        "    \"tensorflow\",\n",
        "    \"scikit-learn\",\n",
        "    \"requests\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in essential_packages:\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        !pip install -q {package}\n",
        "        print(f\"✓ {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {package} failed: {e}\")\n",
        "\n",
        "print(\"\\n✅ Essential packages installation complete!\")\n",
        "\n",
        "# Test imports\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import plotly.graph_objects as go\n",
        "    import yfinance as yf\n",
        "    import tensorflow as tf\n",
        "    import sklearn\n",
        "    print(\"\\n🎉 All essential packages imported successfully!\")\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\n⚠️ Some packages failed to import: {e}\")\n",
        "    print(\"You may need to restart the runtime and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/CatalinMoldova/trading-strategy-ml.git\n",
        "\n",
        "# Change to the project directory\n",
        "%cd trading-strategy-ml\n",
        "\n",
        "# Install project requirements\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "print(\"Repository cloned and requirements installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import project modules with correct class names and error handling\n",
        "try:\n",
        "    from data_pipeline.market_data_collector import MarketDataCollector\n",
        "    from data_pipeline.indicator_engine import TechnicalIndicatorEngine\n",
        "    from data_pipeline.feature_engineer import FeatureEngineer\n",
        "    from ml_models.cnn_lstm_model import CNNLSTMModel\n",
        "    from ml_models.random_forest_model import RandomForestModel\n",
        "    from ml_models.ensemble_predictor import EnsemblePredictor\n",
        "    from strategy.signal_generator import MultiFactorSignalGenerator\n",
        "    from strategy.position_sizer import PositionSizer\n",
        "    from strategy.risk_manager import RiskManager\n",
        "    from backtesting.backtest_engine import BacktestEngine\n",
        "    from backtesting.performance_analyzer import PerformanceAnalyzer\n",
        "    print(\"✅ All project modules imported successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Some project modules failed to import: {e}\")\n",
        "    print(\"Using improved implementations instead...\")\n",
        "    \n",
        "    # ============================================================================\n",
        "    # IMPROVED IMPLEMENTATIONS WITH PROPER TRADING STRATEGY, RISK MANAGEMENT, \n",
        "    # FEATURE ENGINEERING, BACKTESTING, AND MARKET REGIME DETECTION\n",
        "    # ============================================================================\n",
        "    \n",
        "    class MarketDataCollector:\n",
        "        \"\"\"Enhanced market data collector with error handling\"\"\"\n",
        "        def get_historical_data(self, symbol, period='2y', interval='1d'):\n",
        "            import yfinance as yf\n",
        "            try:\n",
        "                ticker = yf.Ticker(symbol)\n",
        "                data = ticker.history(period=period, interval=interval)\n",
        "                if data.empty:\n",
        "                    print(f\"⚠️ No data for {symbol}\")\n",
        "                    return None\n",
        "                return data\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error fetching data for {symbol}: {e}\")\n",
        "                return None\n",
        "    \n",
        "    class TechnicalIndicatorEngine:\n",
        "        \"\"\"Advanced technical indicator engine with comprehensive indicators\"\"\"\n",
        "        \n",
        "        def calculate_all_indicators(self, df):\n",
        "            \"\"\"Calculate comprehensive technical indicators\"\"\"\n",
        "            df = df.copy()\n",
        "            \n",
        "            # Price-based indicators\n",
        "            df['SMA_5'] = df['Close'].rolling(window=5).mean()\n",
        "            df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "            df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "            df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "            df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
        "            df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
        "            \n",
        "            # RSI\n",
        "            df['RSI'] = self._calculate_rsi(df['Close'], 14)\n",
        "            df['RSI_6'] = self._calculate_rsi(df['Close'], 6)\n",
        "            \n",
        "            # MACD\n",
        "            df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
        "            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "            df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "            \n",
        "            # Bollinger Bands with advanced analysis\n",
        "            df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "            bb_std = df['Close'].rolling(window=20).std()\n",
        "            df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
        "            df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
        "            df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle']\n",
        "            df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "            df['BB_Squeeze'] = (df['BB_Width'] < df['BB_Width'].rolling(20).quantile(0.2)).astype(int)\n",
        "            df['BB_Expansion'] = (df['BB_Width'] > df['BB_Width'].rolling(20).quantile(0.8)).astype(int)\n",
        "            \n",
        "            # Stochastic Oscillator\n",
        "            df['Stoch_K'] = self._calculate_stochastic(df, 14)\n",
        "            df['Stoch_D'] = df['Stoch_K'].rolling(window=3).mean()\n",
        "            \n",
        "            # Williams %R\n",
        "            df['Williams_R'] = self._calculate_williams_r(df, 14)\n",
        "            \n",
        "            # Average True Range (ATR)\n",
        "            df['ATR'] = self._calculate_atr(df, 14)\n",
        "            df['ATR_Ratio'] = df['ATR'] / df['Close']  # Normalized ATR\n",
        "            \n",
        "            # Commodity Channel Index (CCI)\n",
        "            df['CCI'] = self._calculate_cci(df, 20)\n",
        "            \n",
        "            # Money Flow Index (MFI)\n",
        "            df['MFI'] = self._calculate_mfi(df, 14)\n",
        "            \n",
        "            # Volume indicators\n",
        "            df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()\n",
        "            df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
        "            df['OBV'] = self._calculate_obv(df)  # On-Balance Volume\n",
        "            \n",
        "            # Price patterns\n",
        "            df['Doji'] = abs(df['Close'] - df['Open']) <= (df['High'] - df['Low']) * 0.1\n",
        "            df['Hammer'] = self._detect_hammer(df)\n",
        "            df['Shooting_Star'] = self._detect_shooting_star(df)\n",
        "            df['Engulfing'] = self._detect_engulfing(df)\n",
        "            \n",
        "            return df\n",
        "        \n",
        "        def _calculate_rsi(self, prices, period=14):\n",
        "            \"\"\"Calculate RSI indicator\"\"\"\n",
        "            delta = prices.diff()\n",
        "            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "            rs = gain / loss\n",
        "            return 100 - (100 / (1 + rs))\n",
        "        \n",
        "        def _calculate_stochastic(self, df, period=14):\n",
        "            \"\"\"Calculate Stochastic Oscillator\"\"\"\n",
        "            lowest_low = df['Low'].rolling(window=period).min()\n",
        "            highest_high = df['High'].rolling(window=period).max()\n",
        "            return 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)\n",
        "        \n",
        "        def _calculate_williams_r(self, df, period=14):\n",
        "            \"\"\"Calculate Williams %R\"\"\"\n",
        "            highest_high = df['High'].rolling(window=period).max()\n",
        "            lowest_low = df['Low'].rolling(window=period).min()\n",
        "            return -100 * (highest_high - df['Close']) / (highest_high - lowest_low)\n",
        "        \n",
        "        def _calculate_atr(self, df, period=14):\n",
        "            \"\"\"Calculate Average True Range\"\"\"\n",
        "            high_low = df['High'] - df['Low']\n",
        "            high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "            low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "            \n",
        "            true_range = np.maximum(high_low, np.maximum(high_close, low_close))\n",
        "            return true_range.rolling(window=period).mean()\n",
        "        \n",
        "        def _detect_hammer(self, df):\n",
        "            \"\"\"Detect hammer candlestick pattern\"\"\"\n",
        "            body = abs(df['Close'] - df['Open'])\n",
        "            lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
        "            upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
        "            \n",
        "            return (lower_shadow > 2 * body) & (upper_shadow < body)\n",
        "        \n",
        "        def _detect_shooting_star(self, df):\n",
        "            \"\"\"Detect shooting star candlestick pattern\"\"\"\n",
        "            body = abs(df['Close'] - df['Open'])\n",
        "            lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
        "            upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
        "            \n",
        "            return (upper_shadow > 2 * body) & (lower_shadow < body)\n",
        "        \n",
        "        def _detect_engulfing(self, df):\n",
        "            \"\"\"Detect bullish/bearish engulfing patterns\"\"\"\n",
        "            prev_body = abs(df['Close'].shift(1) - df['Open'].shift(1))\n",
        "            curr_body = abs(df['Close'] - df['Open'])\n",
        "            \n",
        "            # Bullish engulfing\n",
        "            bullish_engulfing = (\n",
        "                (df['Close'].shift(1) < df['Open'].shift(1)) &  # Previous candle bearish\n",
        "                (df['Close'] > df['Open']) &  # Current candle bullish\n",
        "                (df['Open'] < df['Close'].shift(1)) &  # Current open below previous close\n",
        "                (df['Close'] > df['Open'].shift(1)) &  # Current close above previous open\n",
        "                (curr_body > prev_body)  # Current body larger than previous\n",
        "            )\n",
        "            \n",
        "            # Bearish engulfing\n",
        "            bearish_engulfing = (\n",
        "                (df['Close'].shift(1) > df['Open'].shift(1)) &  # Previous candle bullish\n",
        "                (df['Close'] < df['Open']) &  # Current candle bearish\n",
        "                (df['Open'] > df['Close'].shift(1)) &  # Current open above previous close\n",
        "                (df['Close'] < df['Open'].shift(1)) &  # Current close below previous open\n",
        "                (curr_body > prev_body)  # Current body larger than previous\n",
        "            )\n",
        "            \n",
        "            return bullish_engulfing.astype(int) - bearish_engulfing.astype(int)\n",
        "        \n",
        "        def _calculate_cci(self, df, period=20):\n",
        "            \"\"\"Calculate Commodity Channel Index\"\"\"\n",
        "            typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "            sma_tp = typical_price.rolling(window=period).mean()\n",
        "            mad = typical_price.rolling(window=period).apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
        "            return (typical_price - sma_tp) / (0.015 * mad)\n",
        "        \n",
        "        def _calculate_mfi(self, df, period=14):\n",
        "            \"\"\"Calculate Money Flow Index\"\"\"\n",
        "            typical_price = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "            money_flow = typical_price * df['Volume']\n",
        "            \n",
        "            positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(window=period).sum()\n",
        "            negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(window=period).sum()\n",
        "            \n",
        "            mfi = 100 - (100 / (1 + positive_flow / negative_flow))\n",
        "            return mfi\n",
        "        \n",
        "        def _calculate_obv(self, df):\n",
        "            \"\"\"Calculate On-Balance Volume\"\"\"\n",
        "            obv = np.zeros(len(df))\n",
        "            obv[0] = df['Volume'].iloc[0]\n",
        "            \n",
        "            for i in range(1, len(df)):\n",
        "                if df['Close'].iloc[i] > df['Close'].iloc[i-1]:\n",
        "                    obv[i] = obv[i-1] + df['Volume'].iloc[i]\n",
        "                elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:\n",
        "                    obv[i] = obv[i-1] - df['Volume'].iloc[i]\n",
        "                else:\n",
        "                    obv[i] = obv[i-1]\n",
        "            \n",
        "            return pd.Series(obv, index=df.index)\n",
        "    \n",
        "    class FeatureEngineer:\n",
        "        \"\"\"Advanced feature engineering with market regime detection\"\"\"\n",
        "        \n",
        "        def create_features(self, df):\n",
        "            \"\"\"Create comprehensive features for ML models\"\"\"\n",
        "            df = df.copy()\n",
        "            \n",
        "            # Price-based features\n",
        "            df['Returns'] = df['Close'].pct_change()\n",
        "            df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "            df['Price_Ratio'] = df['Close'] / df['Open']\n",
        "            df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "            df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "            \n",
        "            # Volatility features\n",
        "            df['Volatility_5'] = df['Returns'].rolling(window=5).std()\n",
        "            df['Volatility_20'] = df['Returns'].rolling(window=20).std()\n",
        "            df['Volatility_Ratio'] = df['Volatility_5'] / df['Volatility_20']\n",
        "            \n",
        "            # Momentum features\n",
        "            df['Momentum_5'] = df['Close'] / df['Close'].shift(5) - 1\n",
        "            df['Momentum_10'] = df['Close'] / df['Close'].shift(10) - 1\n",
        "            df['Momentum_20'] = df['Close'] / df['Close'].shift(20) - 1\n",
        "            \n",
        "            # Trend features\n",
        "            df['Trend_Strength'] = (df['SMA_20'] - df['SMA_50']) / df['SMA_50']\n",
        "            df['Price_vs_SMA20'] = (df['Close'] - df['SMA_20']) / df['SMA_20']\n",
        "            df['Price_vs_SMA50'] = (df['Close'] - df['SMA_50']) / df['SMA_50']\n",
        "            \n",
        "            # Market regime detection\n",
        "            df = self._detect_market_regime(df)\n",
        "            \n",
        "            # Volume features\n",
        "            df['Volume_Change'] = df['Volume'].pct_change()\n",
        "            df['Price_Volume_Trend'] = df['Returns'] * df['Volume_Ratio']\n",
        "            \n",
        "            # Technical indicator features\n",
        "            df['RSI_Overbought'] = (df['RSI'] > 70).astype(int)\n",
        "            df['RSI_Oversold'] = (df['RSI'] < 30).astype(int)\n",
        "            df['MACD_Bullish'] = (df['MACD'] > df['MACD_Signal']).astype(int)\n",
        "            df['MACD_Bearish'] = (df['MACD'] < df['MACD_Signal']).astype(int)\n",
        "            \n",
        "            # Bollinger Band features\n",
        "            df['BB_Squeeze'] = (df['BB_Width'] < df['BB_Width'].rolling(20).quantile(0.2)).astype(int)\n",
        "            df['BB_Expansion'] = (df['BB_Width'] > df['BB_Width'].rolling(20).quantile(0.8)).astype(int)\n",
        "            \n",
        "            # Lagged features\n",
        "            for lag in [1, 2, 3, 5]:\n",
        "                df[f'Returns_Lag_{lag}'] = df['Returns'].shift(lag)\n",
        "                df[f'Volume_Ratio_Lag_{lag}'] = df['Volume_Ratio'].shift(lag)\n",
        "            \n",
        "            return df\n",
        "        \n",
        "        def _detect_market_regime(self, df):\n",
        "            \"\"\"Detect market regime (Bull, Bear, Sideways, High Volatility)\"\"\"\n",
        "            # Calculate trend strength using multiple timeframes\n",
        "            trend_5 = (df['SMA_5'] - df['SMA_20']) / df['SMA_20']\n",
        "            trend_20 = (df['SMA_20'] - df['SMA_50']) / df['SMA_50']\n",
        "            trend_strength = (trend_5 + trend_20) / 2\n",
        "            \n",
        "            # Calculate volatility using ATR and rolling standard deviation\n",
        "            volatility_short = df['ATR_Ratio'].rolling(window=10).mean()\n",
        "            volatility_long = df['ATR_Ratio'].rolling(window=30).mean()\n",
        "            volatility_regime = volatility_short / volatility_long\n",
        "            \n",
        "            # Calculate momentum using RSI and MACD\n",
        "            momentum_rsi = df['RSI'].rolling(window=10).mean()\n",
        "            momentum_macd = df['MACD'].rolling(window=10).mean()\n",
        "            \n",
        "            # Regime classification with multiple conditions\n",
        "            conditions = [\n",
        "                # Bull market: Strong uptrend, moderate volatility, positive momentum\n",
        "                (trend_strength > 0.02) & (volatility_regime < 1.5) & (momentum_rsi > 50) & (momentum_macd > 0),\n",
        "                \n",
        "                # Bear market: Strong downtrend, moderate volatility, negative momentum  \n",
        "                (trend_strength < -0.02) & (volatility_regime < 1.5) & (momentum_rsi < 50) & (momentum_macd < 0),\n",
        "                \n",
        "                # High volatility: Elevated volatility regardless of trend\n",
        "                (volatility_regime > 1.8),\n",
        "                \n",
        "                # Sideways: Weak trend, moderate volatility\n",
        "                (abs(trend_strength) < 0.01) & (volatility_regime < 1.5)\n",
        "            ]\n",
        "            \n",
        "            choices = ['Bull', 'Bear', 'High_Vol', 'Sideways']\n",
        "            df['Market_Regime'] = np.select(conditions, choices, default='Sideways')\n",
        "            \n",
        "            # Add regime strength (confidence in regime classification)\n",
        "            regime_strength = np.abs(trend_strength) + (1 / volatility_regime) + np.abs(momentum_rsi - 50) / 50\n",
        "            df['Regime_Strength'] = regime_strength\n",
        "            \n",
        "            # One-hot encode regimes\n",
        "            df['Regime_Bull'] = (df['Market_Regime'] == 'Bull').astype(int)\n",
        "            df['Regime_Bear'] = (df['Market_Regime'] == 'Bear').astype(int)\n",
        "            df['Regime_Sideways'] = (df['Market_Regime'] == 'Sideways').astype(int)\n",
        "            df['Regime_High_Vol'] = (df['Market_Regime'] == 'High_Vol').astype(int)\n",
        "            \n",
        "            # Add regime transition signals\n",
        "            df['Regime_Change'] = (df['Market_Regime'] != df['Market_Regime'].shift(1)).astype(int)\n",
        "            \n",
        "            return df\n",
        "    \n",
        "    class CNNLSTMModel:\n",
        "        \"\"\"Improved CNN+LSTM model with better architecture and regularization\"\"\"\n",
        "        \n",
        "        def __init__(self, time_steps=60, n_features=20, learning_rate=0.001):\n",
        "            self.time_steps = time_steps\n",
        "            self.n_features = n_features\n",
        "            self.learning_rate = learning_rate\n",
        "            self.model = None\n",
        "        \n",
        "        def build_model(self):\n",
        "            import tensorflow as tf\n",
        "            from tensorflow.keras.models import Sequential\n",
        "            from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, BatchNormalization, GlobalAveragePooling1D\n",
        "            from tensorflow.keras.optimizers import Adam\n",
        "            from tensorflow.keras.regularizers import l1_l2\n",
        "            \n",
        "            model = Sequential([\n",
        "                # CNN layers for pattern recognition\n",
        "                Conv1D(filters=64, kernel_size=3, activation='relu', \n",
        "                       input_shape=(self.time_steps, self.n_features),\n",
        "                       kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Conv1D(filters=32, kernel_size=3, activation='relu',\n",
        "                       kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                MaxPooling1D(pool_size=2),\n",
        "                Dropout(0.3),\n",
        "                \n",
        "                # Additional CNN layer for more pattern recognition\n",
        "                Conv1D(filters=16, kernel_size=2, activation='relu',\n",
        "                       kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                \n",
        "                # LSTM layers for sequence learning\n",
        "                LSTM(128, return_sequences=True, \n",
        "                     kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
        "                     recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.3),\n",
        "                \n",
        "                LSTM(64, return_sequences=True,\n",
        "                     kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
        "                     recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.3),\n",
        "                \n",
        "                LSTM(32, return_sequences=False,\n",
        "                     kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
        "                     recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                \n",
        "                # Dense layers with regularization\n",
        "                Dense(64, activation='relu', \n",
        "                      kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                \n",
        "                Dense(32, activation='relu',\n",
        "                      kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.1),\n",
        "                \n",
        "                Dense(16, activation='relu',\n",
        "                      kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
        "                Dense(1, activation='linear')\n",
        "            ])\n",
        "            \n",
        "            # Use Huber loss for robustness to outliers\n",
        "            model.compile(\n",
        "                optimizer=Adam(learning_rate=self.learning_rate, clipnorm=1.0),\n",
        "                loss='huber',\n",
        "                metrics=['mae', 'mse']\n",
        "            )\n",
        "            \n",
        "            self.model = model\n",
        "            return model\n",
        "        \n",
        "        def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.2):\n",
        "            \"\"\"Train with early stopping and learning rate reduction\"\"\"\n",
        "            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "            \n",
        "            callbacks = [\n",
        "                EarlyStopping(patience=10, restore_best_weights=True),\n",
        "                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7)\n",
        "            ]\n",
        "            \n",
        "            return self.model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "        \n",
        "        def predict(self, X):\n",
        "            \"\"\"Make predictions\"\"\"\n",
        "            return self.model.predict(X)\n",
        "        \n",
        "        def save_model(self, path):\n",
        "            \"\"\"Save model\"\"\"\n",
        "            self.model.save(path)\n",
        "    \n",
        "    class RiskManager:\n",
        "        \"\"\"Comprehensive risk management system with Kelly Criterion\"\"\"\n",
        "        \n",
        "        def __init__(self, max_position_size=0.8, stop_loss=0.05, take_profit=0.15, max_drawdown=0.20):\n",
        "            self.max_position_size = max_position_size\n",
        "            self.stop_loss = stop_loss\n",
        "            self.take_profit = take_profit\n",
        "            self.max_drawdown = max_drawdown\n",
        "            self.current_drawdown = 0\n",
        "            self.peak_value = 0\n",
        "            self.trade_history = []\n",
        "        \n",
        "        def calculate_kelly_position_size(self, signal_strength, volatility, account_value, market_regime='Sideways', win_rate=0.55, avg_win=0.08, avg_loss=0.04):\n",
        "            \"\"\"Calculate position size using Kelly Criterion with regime adjustment\"\"\"\n",
        "            \n",
        "            # Base Kelly parameters (can be optimized based on historical performance)\n",
        "            kelly_fraction = (win_rate * avg_win - (1 - win_rate) * avg_loss) / avg_win\n",
        "            kelly_fraction = max(0, min(kelly_fraction, self.max_position_size))\n",
        "            \n",
        "            # Volatility adjustment - reduce size in high volatility\n",
        "            if volatility > 0:\n",
        "                volatility_adjustment = 1 / (1 + volatility * 8)  # More conservative than before\n",
        "            else:\n",
        "                volatility_adjustment = 1\n",
        "            \n",
        "            # Market regime adjustment\n",
        "            regime_adjustments = {\n",
        "                'Bull': 1.3,      # Increase size in bull markets\n",
        "                'Bear': 0.6,      # Reduce size in bear markets\n",
        "                'Sideways': 1.0,   # Normal size in sideways markets\n",
        "                'High_Vol': 0.4   # Significantly reduce in high volatility\n",
        "            }\n",
        "            regime_adjustment = regime_adjustments.get(market_regime, 1.0)\n",
        "            \n",
        "            # Signal strength adjustment - only trade on strong signals\n",
        "            signal_adjustment = min(abs(signal_strength), 1.0)\n",
        "            if abs(signal_strength) < 0.3:  # Weak signals get minimal allocation\n",
        "                signal_adjustment *= 0.2\n",
        "            \n",
        "            # Drawdown adjustment - reduce size if in drawdown\n",
        "            drawdown_adjustment = 1.0\n",
        "            if self.current_drawdown > 0.05:  # 5% drawdown\n",
        "                drawdown_adjustment = 1 - (self.current_drawdown * 2)  # Reduce by 2x drawdown\n",
        "                drawdown_adjustment = max(0.1, drawdown_adjustment)  # Minimum 10% allocation\n",
        "            \n",
        "            # Final position size calculation\n",
        "            final_position_size = (kelly_fraction * volatility_adjustment * \n",
        "                                 regime_adjustment * signal_adjustment * drawdown_adjustment)\n",
        "            \n",
        "            return min(final_position_size, self.max_position_size)\n",
        "        \n",
        "        def calculate_dynamic_stop_loss(self, entry_price, volatility, market_regime):\n",
        "            \"\"\"Calculate dynamic stop loss based on volatility and regime\"\"\"\n",
        "            base_stop = self.stop_loss\n",
        "            \n",
        "            # Adjust stop loss based on volatility\n",
        "            if volatility > 0.03:  # High volatility\n",
        "                stop_multiplier = 1.5\n",
        "            elif volatility < 0.01:  # Low volatility\n",
        "                stop_multiplier = 0.7\n",
        "            else:\n",
        "                stop_multiplier = 1.0\n",
        "            \n",
        "            # Adjust based on market regime\n",
        "            regime_multipliers = {\n",
        "                'Bull': 1.2,      # Wider stops in bull markets\n",
        "                'Bear': 0.8,       # Tighter stops in bear markets\n",
        "                'Sideways': 1.0,   # Normal stops\n",
        "                'High_Vol': 1.8   # Much wider stops in high volatility\n",
        "            }\n",
        "            regime_multiplier = regime_multipliers.get(market_regime, 1.0)\n",
        "            \n",
        "            dynamic_stop = base_stop * stop_multiplier * regime_multiplier\n",
        "            return min(dynamic_stop, 0.15)  # Cap at 15%\n",
        "        \n",
        "        def calculate_dynamic_take_profit(self, entry_price, volatility, market_regime):\n",
        "            \"\"\"Calculate dynamic take profit based on volatility and regime\"\"\"\n",
        "            base_take_profit = self.take_profit\n",
        "            \n",
        "            # Adjust take profit based on volatility\n",
        "            if volatility > 0.03:  # High volatility\n",
        "                profit_multiplier = 1.8\n",
        "            elif volatility < 0.01:  # Low volatility\n",
        "                profit_multiplier = 1.2\n",
        "            else:\n",
        "                profit_multiplier = 1.5\n",
        "            \n",
        "            # Adjust based on market regime\n",
        "            regime_multipliers = {\n",
        "                'Bull': 1.5,      # Higher targets in bull markets\n",
        "                'Bear': 1.0,      # Normal targets in bear markets\n",
        "                'Sideways': 1.2,   # Slightly higher in sideways\n",
        "                'High_Vol': 2.0   # Much higher targets in high volatility\n",
        "            }\n",
        "            regime_multiplier = regime_multipliers.get(market_regime, 1.0)\n",
        "            \n",
        "            dynamic_take_profit = base_take_profit * profit_multiplier * regime_multiplier\n",
        "            return min(dynamic_take_profit, 0.30)  # Cap at 30%\n",
        "        \n",
        "        def check_risk_limits(self, current_value, entry_price, current_price, position_size, market_regime='Sideways'):\n",
        "            \"\"\"Check if risk limits are breached with dynamic stops\"\"\"\n",
        "            \n",
        "            # Calculate dynamic stops\n",
        "            volatility = abs(current_price - entry_price) / entry_price if entry_price > 0 else 0.02\n",
        "            dynamic_stop_loss = self.calculate_dynamic_stop_loss(entry_price, volatility, market_regime)\n",
        "            dynamic_take_profit = self.calculate_dynamic_take_profit(entry_price, volatility, market_regime)\n",
        "            \n",
        "            # Stop loss check\n",
        "            if current_price < entry_price * (1 - dynamic_stop_loss):\n",
        "                return 'stop_loss'\n",
        "            \n",
        "            # Take profit check\n",
        "            if current_price > entry_price * (1 + dynamic_take_profit):\n",
        "                return 'take_profit'\n",
        "            \n",
        "            # Drawdown check\n",
        "            if current_value > self.peak_value:\n",
        "                self.peak_value = current_value\n",
        "            \n",
        "            self.current_drawdown = (self.peak_value - current_value) / self.peak_value\n",
        "            \n",
        "            if self.current_drawdown > self.max_drawdown:\n",
        "                return 'max_drawdown'\n",
        "            \n",
        "            return 'hold'\n",
        "        \n",
        "        def update_trade_history(self, trade_info):\n",
        "            \"\"\"Update trade history for performance tracking\"\"\"\n",
        "            self.trade_history.append(trade_info)\n",
        "            \n",
        "            # Keep only last 100 trades for memory efficiency\n",
        "            if len(self.trade_history) > 100:\n",
        "                self.trade_history = self.trade_history[-100:]\n",
        "        \n",
        "        def get_performance_stats(self):\n",
        "            \"\"\"Get performance statistics from trade history\"\"\"\n",
        "            if not self.trade_history:\n",
        "                return {'win_rate': 0.5, 'avg_win': 0.05, 'avg_loss': 0.03}\n",
        "            \n",
        "            wins = [t for t in self.trade_history if t.get('pnl', 0) > 0]\n",
        "            losses = [t for t in self.trade_history if t.get('pnl', 0) < 0]\n",
        "            \n",
        "            win_rate = len(wins) / len(self.trade_history) if self.trade_history else 0.5\n",
        "            avg_win = np.mean([t['pnl'] for t in wins]) if wins else 0.05\n",
        "            avg_loss = abs(np.mean([t['pnl'] for t in losses])) if losses else 0.03\n",
        "            \n",
        "            return {\n",
        "                'win_rate': win_rate,\n",
        "                'avg_win': avg_win,\n",
        "                'avg_loss': avg_loss,\n",
        "                'total_trades': len(self.trade_history)\n",
        "            }\n",
        "    \n",
        "    class MultiFactorTradingStrategy:\n",
        "        \"\"\"Advanced multi-factor ensemble trading strategy with proper position management\"\"\"\n",
        "        \n",
        "        def __init__(self, initial_capital=100000, risk_manager=None):\n",
        "            self.initial_capital = initial_capital\n",
        "            self.capital = initial_capital\n",
        "            self.position = 0\n",
        "            self.position_size = 0\n",
        "            self.entry_price = 0\n",
        "            self.portfolio_value = initial_capital\n",
        "            self.trades = []\n",
        "            self.risk_manager = risk_manager or RiskManager()\n",
        "            self.market_regime = 'Sideways'\n",
        "            self.prediction_history = []\n",
        "        \n",
        "        def calculate_ensemble_prediction(self, row, model_predictions=None):\n",
        "            \"\"\"Calculate multi-factor ensemble prediction with weighted scoring\"\"\"\n",
        "            prediction_score = 0\n",
        "            confidence = 0\n",
        "            \n",
        "            # 1. TREND FOLLOWING (25% weight)\n",
        "            trend_score = 0\n",
        "            if 'SMA_20' in row and 'SMA_50' in row:\n",
        "                # Price vs moving averages\n",
        "                if row['Close'] > row['SMA_20'] and row['SMA_20'] > row['SMA_50']:\n",
        "                    trend_score += 0.4  # Strong uptrend\n",
        "                elif row['Close'] < row['SMA_20'] and row['SMA_20'] < row['SMA_50']:\n",
        "                    trend_score -= 0.4  # Strong downtrend\n",
        "                elif row['Close'] > row['SMA_20']:\n",
        "                    trend_score += 0.2  # Weak uptrend\n",
        "                elif row['Close'] < row['SMA_20']:\n",
        "                    trend_score -= 0.2  # Weak downtrend\n",
        "            \n",
        "            # EMA trend confirmation\n",
        "            if 'EMA_12' in row and 'EMA_26' in row:\n",
        "                if row['EMA_12'] > row['EMA_26']:\n",
        "                    trend_score += 0.2\n",
        "                else:\n",
        "                    trend_score -= 0.2\n",
        "            \n",
        "            prediction_score += trend_score * 0.25\n",
        "            confidence += 0.25\n",
        "            \n",
        "            # 2. MOMENTUM ANALYSIS (25% weight)\n",
        "            momentum_score = 0\n",
        "            if 'RSI' in row:\n",
        "                if row['RSI'] < 30:  # Oversold\n",
        "                    momentum_score += 0.3\n",
        "                elif row['RSI'] > 70:  # Overbought\n",
        "                    momentum_score -= 0.3\n",
        "                elif row['RSI'] < 20:  # Extreme oversold - potential reversal\n",
        "                    momentum_score += 0.1\n",
        "                elif row['RSI'] > 80:  # Extreme overbought - potential reversal\n",
        "                    momentum_score -= 0.1\n",
        "            \n",
        "            # MACD momentum\n",
        "            if 'MACD' in row and 'MACD_Signal' in row:\n",
        "                macd_diff = row['MACD'] - row['MACD_Signal']\n",
        "                if macd_diff > 0:\n",
        "                    momentum_score += 0.2\n",
        "                else:\n",
        "                    momentum_score -= 0.2\n",
        "            \n",
        "            # Stochastic momentum\n",
        "            if 'Stoch_K' in row and 'Stoch_D' in row:\n",
        "                if row['Stoch_K'] > row['Stoch_D'] and row['Stoch_K'] < 80:\n",
        "                    momentum_score += 0.1\n",
        "                elif row['Stoch_K'] < row['Stoch_D'] and row['Stoch_K'] > 20:\n",
        "                    momentum_score -= 0.1\n",
        "            \n",
        "            # Williams %R momentum\n",
        "            if 'Williams_R' in row:\n",
        "                if row['Williams_R'] < -80:  # Oversold\n",
        "                    momentum_score += 0.1\n",
        "                elif row['Williams_R'] > -20:  # Overbought\n",
        "                    momentum_score -= 0.1\n",
        "            \n",
        "            prediction_score += momentum_score * 0.25\n",
        "            confidence += 0.25\n",
        "            \n",
        "            # 3. VOLATILITY ANALYSIS (20% weight)\n",
        "            volatility_score = 0\n",
        "            if 'BB_Position' in row and 'BB_Width' in row:\n",
        "                # Bollinger Band analysis\n",
        "                if row['BB_Position'] < 0.2:  # Near lower band\n",
        "                    volatility_score += 0.3\n",
        "                elif row['BB_Position'] > 0.8:  # Near upper band\n",
        "                    volatility_score -= 0.3\n",
        "                elif 0.3 < row['BB_Position'] < 0.7:  # Middle range\n",
        "                    volatility_score += 0.1\n",
        "                \n",
        "                # Volatility expansion/contraction\n",
        "                if 'BB_Squeeze' in row and row['BB_Squeeze'] == 1:\n",
        "                    volatility_score += 0.2  # Volatility squeeze - potential breakout\n",
        "                elif 'BB_Expansion' in row and row['BB_Expansion'] == 1:\n",
        "                    volatility_score -= 0.1  # High volatility - be cautious\n",
        "            \n",
        "            # ATR analysis\n",
        "            if 'ATR_Ratio' in row:\n",
        "                if row['ATR_Ratio'] > 0.03:  # High volatility\n",
        "                    volatility_score -= 0.1\n",
        "                elif row['ATR_Ratio'] < 0.01:  # Low volatility\n",
        "                    volatility_score += 0.1\n",
        "            \n",
        "            prediction_score += volatility_score * 0.2\n",
        "            confidence += 0.2\n",
        "            \n",
        "            # 4. VOLUME ANALYSIS (10% weight)\n",
        "            volume_score = 0\n",
        "            if 'Volume_Ratio' in row and 'OBV' in row:\n",
        "                # Volume confirmation\n",
        "                if row['Volume_Ratio'] > 1.5:  # High volume\n",
        "                    if row['Close'] > row['Close']:  # Price up with volume\n",
        "                        volume_score += 0.3\n",
        "                    else:  # Price down with volume\n",
        "                        volume_score -= 0.2\n",
        "                elif row['Volume_Ratio'] < 0.5:  # Low volume\n",
        "                    volume_score -= 0.1  # Weak conviction\n",
        "                \n",
        "                # OBV trend confirmation\n",
        "                if 'OBV' in row and len(self.prediction_history) > 0:\n",
        "                    obv_trend = row['OBV'] - self.prediction_history[-1].get('obv', row['OBV'])\n",
        "                    if obv_trend > 0:\n",
        "                        volume_score += 0.1\n",
        "                    else:\n",
        "                        volume_score -= 0.1\n",
        "            \n",
        "            prediction_score += volume_score * 0.1\n",
        "            confidence += 0.1\n",
        "            \n",
        "            # 5. MARKET REGIME ADJUSTMENT (5% weight)\n",
        "            regime_score = 0\n",
        "            if 'Market_Regime' in row:\n",
        "                regime_multipliers = {\n",
        "                    'Bull': 1.2,\n",
        "                    'Bear': 0.8,\n",
        "                    'Sideways': 1.0,\n",
        "                    'High_Vol': 0.6\n",
        "                }\n",
        "                regime_multiplier = regime_multipliers.get(row['Market_Regime'], 1.0)\n",
        "                prediction_score *= regime_multiplier\n",
        "            \n",
        "            # 6. ML MODEL PREDICTION INTEGRATION (15% weight)\n",
        "            if model_predictions is not None:\n",
        "                # Use model predictions as additional factor\n",
        "                ml_weight = 0.15\n",
        "                prediction_score += model_predictions * ml_weight\n",
        "                confidence += ml_weight\n",
        "            \n",
        "            # Normalize prediction score\n",
        "            if confidence > 0:\n",
        "                prediction_score = prediction_score / confidence\n",
        "            \n",
        "            return prediction_score, confidence\n",
        "        \n",
        "        def generate_advanced_signals(self, df, model_predictions=None):\n",
        "            \"\"\"Generate advanced trading signals with ensemble predictions\"\"\"\n",
        "            signals = pd.DataFrame(index=df.index)\n",
        "            signals['signal'] = 0\n",
        "            signals['position'] = 0\n",
        "            signals['prediction_score'] = 0\n",
        "            signals['confidence'] = 0\n",
        "            signals['portfolio_value'] = self.portfolio_value\n",
        "            signals['market_regime'] = df.get('Market_Regime', 'Sideways')\n",
        "            \n",
        "            for i in range(1, len(df)):\n",
        "                current_price = df['Close'].iloc[i]\n",
        "                row = df.iloc[i]\n",
        "                \n",
        "                # Get model prediction if available\n",
        "                ml_prediction = None\n",
        "                if model_predictions is not None and i < len(model_predictions):\n",
        "                    ml_prediction = model_predictions[i]\n",
        "                \n",
        "                # Calculate ensemble prediction\n",
        "                prediction_score, confidence = self.calculate_ensemble_prediction(row, ml_prediction)\n",
        "                \n",
        "                # Store prediction history\n",
        "                self.prediction_history.append({\n",
        "                    'date': df.index[i],\n",
        "                    'prediction_score': prediction_score,\n",
        "                    'confidence': confidence,\n",
        "                    'price': current_price,\n",
        "                    'obv': row.get('OBV', 0)\n",
        "                })\n",
        "                \n",
        "                # Get performance stats for Kelly Criterion\n",
        "                perf_stats = self.risk_manager.get_performance_stats()\n",
        "                \n",
        "                # Calculate position size using Kelly Criterion\n",
        "                volatility = row.get('ATR_Ratio', 0.02)\n",
        "                market_regime = row.get('Market_Regime', 'Sideways')\n",
        "                position_size = self.risk_manager.calculate_kelly_position_size(\n",
        "                    prediction_score, volatility, self.portfolio_value, market_regime,\n",
        "                    perf_stats['win_rate'], perf_stats['avg_win'], perf_stats['avg_loss']\n",
        "                )\n",
        "                \n",
        "                # Risk management check\n",
        "                risk_action = self.risk_manager.check_risk_limits(\n",
        "                    self.portfolio_value, self.entry_price, current_price, self.position_size, market_regime\n",
        "                )\n",
        "                \n",
        "                if risk_action != 'hold':\n",
        "                    # Close position due to risk management\n",
        "                    self._close_position(current_price, f\"Risk: {risk_action}\")\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = 0\n",
        "                elif prediction_score > 0.4 and self.position == 0:  # Strong buy signal\n",
        "                    self._open_position(current_price, position_size, 'long')\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 1\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = position_size\n",
        "                elif prediction_score < -0.4 and self.position == 0:  # Strong sell signal\n",
        "                    self._open_position(current_price, position_size, 'short')\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = -1\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = position_size\n",
        "                elif abs(prediction_score) < 0.2 and self.position != 0:  # Weak signal - close position\n",
        "                    self._close_position(current_price, \"Weak signal\")\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = 0\n",
        "                else:  # Hold current position\n",
        "                    signals.iloc[i, signals.columns.get_loc('signal')] = 0\n",
        "                    signals.iloc[i, signals.columns.get_loc('position')] = self.position_size\n",
        "                \n",
        "                # Update prediction and confidence\n",
        "                signals.iloc[i, signals.columns.get_loc('prediction_score')] = prediction_score\n",
        "                signals.iloc[i, signals.columns.get_loc('confidence')] = confidence\n",
        "                \n",
        "                # Update portfolio value\n",
        "                self._update_portfolio_value(current_price)\n",
        "                signals.iloc[i, signals.columns.get_loc('portfolio_value')] = self.portfolio_value\n",
        "            \n",
        "            return signals\n",
        "        \n",
        "        def _open_position(self, price, size, direction):\n",
        "            \"\"\"Open a new position\"\"\"\n",
        "            self.position = 1 if direction == 'long' else -1\n",
        "            self.position_size = size\n",
        "            self.entry_price = price\n",
        "            self.capital -= price * size  # Reduce available capital\n",
        "        \n",
        "        def _close_position(self, price, reason):\n",
        "            \"\"\"Close current position\"\"\"\n",
        "            if self.position != 0:\n",
        "                pnl = (price - self.entry_price) * self.position * self.position_size\n",
        "                self.capital += price * self.position_size + pnl\n",
        "                \n",
        "                trade_info = {\n",
        "                    'entry_price': self.entry_price,\n",
        "                    'exit_price': price,\n",
        "                    'position_size': self.position_size,\n",
        "                    'direction': 'long' if self.position > 0 else 'short',\n",
        "                    'pnl': pnl,\n",
        "                    'reason': reason,\n",
        "                    'date': len(self.trades)\n",
        "                }\n",
        "                \n",
        "                self.trades.append(trade_info)\n",
        "                self.risk_manager.update_trade_history(trade_info)\n",
        "                \n",
        "                self.position = 0\n",
        "                self.position_size = 0\n",
        "                self.entry_price = 0\n",
        "        \n",
        "        def _update_portfolio_value(self, current_price):\n",
        "            \"\"\"Update portfolio value\"\"\"\n",
        "            if self.position != 0:\n",
        "                pnl = (current_price - self.entry_price) * self.position * self.position_size\n",
        "                self.portfolio_value = self.capital + current_price * self.position_size + pnl\n",
        "            else:\n",
        "                self.portfolio_value = self.capital\n",
        "    \n",
        "    class EnhancedBacktestEngine:\n",
        "        \"\"\"Enhanced backtesting engine with realistic trading costs and slippage\"\"\"\n",
        "        \n",
        "        def __init__(self, initial_capital=100000, commission=0.001, slippage=0.0005, bid_ask_spread=0.0002):\n",
        "            self.initial_capital = initial_capital\n",
        "            self.commission = commission  # 0.1% commission\n",
        "            self.slippage = slippage      # 0.05% slippage\n",
        "            self.bid_ask_spread = bid_ask_spread  # 0.02% bid-ask spread\n",
        "        \n",
        "        def run_backtest(self, price_data, signals):\n",
        "            \"\"\"Run comprehensive backtest with realistic trading costs\"\"\"\n",
        "            portfolio_value = self.initial_capital\n",
        "            cash = self.initial_capital\n",
        "            shares = 0\n",
        "            trades = []\n",
        "            portfolio_values = []\n",
        "            daily_returns = []\n",
        "            \n",
        "            for i, (date, row) in enumerate(price_data.iterrows()):\n",
        "                if date in signals.index:\n",
        "                    signal = signals.loc[date, 'signal']\n",
        "                    position_size = signals.loc[date, 'position']\n",
        "                    price = row['Close']\n",
        "                    \n",
        "                    # Apply realistic execution costs\n",
        "                    if signal == 1 and shares == 0:  # Buy signal\n",
        "                        # Apply slippage and bid-ask spread\n",
        "                        execution_price = price * (1 + self.slippage + self.bid_ask_spread)\n",
        "                        shares_to_buy = (cash * position_size) / execution_price\n",
        "                        cost = shares_to_buy * execution_price * (1 + self.commission)\n",
        "                        \n",
        "                        if cost <= cash:\n",
        "                            shares = shares_to_buy\n",
        "                            cash -= cost\n",
        "                            trades.append({\n",
        "                                'date': date,\n",
        "                                'action': 'BUY',\n",
        "                                'price': execution_price,\n",
        "                                'shares': shares,\n",
        "                                'cost': cost,\n",
        "                                'commission': shares_to_buy * execution_price * self.commission,\n",
        "                                'slippage': shares_to_buy * execution_price * self.slippage,\n",
        "                                'spread': shares_to_buy * execution_price * self.bid_ask_spread\n",
        "                            })\n",
        "                    \n",
        "                    elif signal == -1 and shares > 0:  # Sell signal\n",
        "                        # Apply slippage and bid-ask spread\n",
        "                        execution_price = price * (1 - self.slippage - self.bid_ask_spread)\n",
        "                        proceeds = shares * execution_price * (1 - self.commission)\n",
        "                        cash += proceeds\n",
        "                        trades.append({\n",
        "                            'date': date,\n",
        "                            'action': 'SELL',\n",
        "                            'price': execution_price,\n",
        "                            'shares': shares,\n",
        "                            'proceeds': proceeds,\n",
        "                            'commission': shares * execution_price * self.commission,\n",
        "                            'slippage': shares * execution_price * self.slippage,\n",
        "                            'spread': shares * execution_price * self.bid_ask_spread\n",
        "                        })\n",
        "                        shares = 0\n",
        "                \n",
        "                # Calculate portfolio value\n",
        "                portfolio_value = cash + (shares * row['Close'] if shares > 0 else 0)\n",
        "                portfolio_values.append(portfolio_value)\n",
        "                \n",
        "                # Calculate daily return\n",
        "                if i > 0:\n",
        "                    daily_return = (portfolio_value - portfolio_values[i-1]) / portfolio_values[i-1]\n",
        "                    daily_returns.append(daily_return)\n",
        "                else:\n",
        "                    daily_returns.append(0)\n",
        "            \n",
        "            # Calculate returns\n",
        "            returns = pd.Series(daily_returns, index=price_data.index)\n",
        "            \n",
        "            # Calculate trading costs summary\n",
        "            total_commission = sum([t.get('commission', 0) for t in trades])\n",
        "            total_slippage = sum([t.get('slippage', 0) for t in trades])\n",
        "            total_spread = sum([t.get('spread', 0) for t in trades])\n",
        "            total_trading_costs = total_commission + total_slippage + total_spread\n",
        "            \n",
        "            return {\n",
        "                'total_return': (portfolio_value - self.initial_capital) / self.initial_capital,\n",
        "                'portfolio_value': portfolio_value,\n",
        "                'trades': trades,\n",
        "                'final_cash': cash,\n",
        "                'final_shares': shares,\n",
        "                'returns': returns,\n",
        "                'portfolio_values': portfolio_values,\n",
        "                'trading_costs': {\n",
        "                    'total_commission': total_commission,\n",
        "                    'total_slippage': total_slippage,\n",
        "                    'total_spread': total_spread,\n",
        "                    'total_costs': total_trading_costs,\n",
        "                    'cost_percentage': total_trading_costs / self.initial_capital\n",
        "                }\n",
        "            }\n",
        "    \n",
        "    class ComprehensivePerformanceAnalyzer:\n",
        "        \"\"\"Enhanced performance analyzer with comprehensive metrics and advanced analytics\"\"\"\n",
        "        \n",
        "        def calculate_portfolio_performance(self, backtest_results, benchmark_returns=None):\n",
        "            \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "            returns = backtest_results['returns']\n",
        "            \n",
        "            # Basic metrics\n",
        "            total_return = (1 + returns).prod() - 1\n",
        "            annualized_return = (1 + returns).prod() ** (252 / len(returns)) - 1\n",
        "            volatility = returns.std() * np.sqrt(252)\n",
        "            sharpe_ratio = annualized_return / volatility if volatility > 0 else 0\n",
        "            \n",
        "            # Risk metrics\n",
        "            negative_returns = returns[returns < 0]\n",
        "            sortino_ratio = annualized_return / (negative_returns.std() * np.sqrt(252)) if len(negative_returns) > 0 else 0\n",
        "            \n",
        "            # Drawdown analysis\n",
        "            cumulative = (1 + returns).cumprod()\n",
        "            running_max = cumulative.expanding().max()\n",
        "            drawdown = (cumulative - running_max) / running_max\n",
        "            max_drawdown = drawdown.min()\n",
        "            calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
        "            \n",
        "            # Drawdown duration analysis\n",
        "            drawdown_duration = self._calculate_drawdown_duration(drawdown)\n",
        "            max_drawdown_duration = drawdown_duration.max()\n",
        "            \n",
        "            # Win/Loss analysis\n",
        "            winning_trades = returns[returns > 0]\n",
        "            losing_trades = returns[returns < 0]\n",
        "            win_rate = len(winning_trades) / len(returns) if len(returns) > 0 else 0\n",
        "            avg_win = winning_trades.mean() if len(winning_trades) > 0 else 0\n",
        "            avg_loss = losing_trades.mean() if len(losing_trades) > 0 else 0\n",
        "            profit_factor = abs(winning_trades.sum() / losing_trades.sum()) if len(losing_trades) > 0 and losing_trades.sum() != 0 else 0\n",
        "            \n",
        "            # Advanced risk metrics\n",
        "            var_95 = returns.quantile(0.05)  # Value at Risk (95%)\n",
        "            cvar_95 = returns[returns <= var_95].mean()  # Conditional VaR\n",
        "            tail_ratio = abs(returns.quantile(0.95)) / abs(returns.quantile(0.05)) if returns.quantile(0.05) != 0 else 0\n",
        "            \n",
        "            # Skewness and Kurtosis\n",
        "            skewness = returns.skew()\n",
        "            kurtosis = returns.kurtosis()\n",
        "            \n",
        "            # Benchmark comparison\n",
        "            alpha = 0\n",
        "            beta = 0\n",
        "            information_ratio = 0\n",
        "            tracking_error = 0\n",
        "            if benchmark_returns is not None:\n",
        "                excess_returns = returns - benchmark_returns\n",
        "                alpha = excess_returns.mean() * 252\n",
        "                beta = returns.cov(benchmark_returns) / benchmark_returns.var() if benchmark_returns.var() > 0 else 0\n",
        "                information_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(252) if excess_returns.std() > 0 else 0\n",
        "                tracking_error = excess_returns.std() * np.sqrt(252)\n",
        "            \n",
        "            # Trading costs analysis\n",
        "            trading_costs = backtest_results.get('trading_costs', {})\n",
        "            cost_impact = trading_costs.get('cost_percentage', 0)\n",
        "            \n",
        "            return {\n",
        "                'total_return': total_return,\n",
        "                'annualized_return': annualized_return,\n",
        "                'volatility': volatility,\n",
        "                'sharpe_ratio': sharpe_ratio,\n",
        "                'sortino_ratio': sortino_ratio,\n",
        "                'max_drawdown': max_drawdown,\n",
        "                'calmar_ratio': calmar_ratio,\n",
        "                'max_drawdown_duration': max_drawdown_duration,\n",
        "                'win_rate': win_rate,\n",
        "                'avg_win': avg_win,\n",
        "                'avg_loss': avg_loss,\n",
        "                'profit_factor': profit_factor,\n",
        "                'var_95': var_95,\n",
        "                'cvar_95': cvar_95,\n",
        "                'tail_ratio': tail_ratio,\n",
        "                'skewness': skewness,\n",
        "                'kurtosis': kurtosis,\n",
        "                'alpha': alpha,\n",
        "                'beta': beta,\n",
        "                'information_ratio': information_ratio,\n",
        "                'tracking_error': tracking_error,\n",
        "                'cost_impact': cost_impact,\n",
        "                'cumulative_returns': cumulative,\n",
        "                'drawdown': drawdown,\n",
        "                'rolling_sharpe': returns.rolling(window=252).mean() / returns.rolling(window=252).std() * np.sqrt(252),\n",
        "                'monthly_returns': returns.resample('M').apply(lambda x: (1 + x).prod() - 1),\n",
        "                'annual_returns': returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)\n",
        "            }\n",
        "        \n",
        "        def _calculate_drawdown_duration(self, drawdown):\n",
        "            \"\"\"Calculate drawdown duration periods\"\"\"\n",
        "            duration = []\n",
        "            current_duration = 0\n",
        "            \n",
        "            for dd in drawdown:\n",
        "                if dd < 0:\n",
        "                    current_duration += 1\n",
        "                else:\n",
        "                    if current_duration > 0:\n",
        "                        duration.append(current_duration)\n",
        "                    current_duration = 0\n",
        "            \n",
        "            if current_duration > 0:\n",
        "                duration.append(current_duration)\n",
        "            \n",
        "            return pd.Series(duration) if duration else pd.Series([0])\n",
        "        \n",
        "        def create_comprehensive_dashboard(self, strategy_returns, benchmark_returns=None, strategy_name=\"Multi-Factor Strategy\"):\n",
        "            \"\"\"Create comprehensive performance dashboard with advanced analytics\"\"\"\n",
        "            \n",
        "            # Calculate metrics\n",
        "            strategy_metrics = self.calculate_portfolio_performance(\n",
        "                {'returns': strategy_returns}, benchmark_returns\n",
        "            )\n",
        "            \n",
        "            # Create subplots with correct specifications for tables\n",
        "            fig = make_subplots(\n",
        "                rows=4, cols=2,\n",
        "                subplot_titles=[\n",
        "                    'Cumulative Returns Comparison',\n",
        "                    'Rolling Sharpe Ratio',\n",
        "                    'Drawdown Analysis',\n",
        "                    'Monthly Returns Heatmap',\n",
        "                    'Risk-Return Scatter',\n",
        "                    'Performance Metrics Table',\n",
        "                    'Annual Returns Comparison',\n",
        "                    'Risk Metrics Summary'\n",
        "                ],\n",
        "                specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                       [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                       [{\"secondary_y\": False}, {\"type\": \"table\"}],\n",
        "                       [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
        "            )\n",
        "            \n",
        "            # 1. Cumulative Returns\n",
        "            strategy_cumulative = (1 + strategy_returns).cumprod()\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=strategy_returns.index, y=strategy_cumulative.values,\n",
        "                          name=f'{strategy_name}', line=dict(color='blue', width=2)),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            \n",
        "            if benchmark_returns is not None:\n",
        "                benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=benchmark_returns.index, y=benchmark_cumulative.values,\n",
        "                              name='Benchmark (S&P 500)', line=dict(color='red', width=2)),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "            \n",
        "            # 2. Rolling Sharpe Ratio\n",
        "            rolling_sharpe = strategy_returns.rolling(window=252).mean() / strategy_returns.rolling(window=252).std() * np.sqrt(252)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=strategy_returns.index, y=rolling_sharpe.values,\n",
        "                          name='Rolling Sharpe', line=dict(color='green')),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            \n",
        "            # 3. Drawdown Analysis\n",
        "            cumulative = (1 + strategy_returns).cumprod()\n",
        "            running_max = cumulative.expanding().max()\n",
        "            drawdown = (cumulative - running_max) / running_max\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=strategy_returns.index, y=drawdown.values,\n",
        "                          name='Drawdown', fill='tonexty', line=dict(color='red')),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            \n",
        "            # 4. Monthly Returns Heatmap\n",
        "            monthly_returns = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
        "            monthly_pivot = monthly_returns.groupby([monthly_returns.index.year, monthly_returns.index.month]).first().unstack()\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Heatmap(z=monthly_pivot.values,\n",
        "                          x=monthly_pivot.columns,\n",
        "                          y=monthly_pivot.index,\n",
        "                          colorscale='RdYlGn',\n",
        "                          name='Monthly Returns'),\n",
        "                row=2, col=2\n",
        "            )\n",
        "            \n",
        "            # 5. Risk-Return Scatter\n",
        "            if benchmark_returns is not None:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=[strategy_metrics['volatility']], y=[strategy_metrics['annualized_return']],\n",
        "                              mode='markers', marker=dict(size=15, color='blue'),\n",
        "                              name=f'{strategy_name}'),\n",
        "                    row=3, col=1\n",
        "                )\n",
        "                benchmark_metrics = self.calculate_portfolio_performance({'returns': benchmark_returns})\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=[benchmark_metrics['volatility']], y=[benchmark_metrics['annualized_return']],\n",
        "                              mode='markers', marker=dict(size=15, color='red'),\n",
        "                              name='Benchmark'),\n",
        "                    row=3, col=1\n",
        "                )\n",
        "            \n",
        "            # 6. Performance Metrics Table (row 3, col 2)\n",
        "            metrics_data = [\n",
        "                ['Total Return', f\"{strategy_metrics['total_return']:.2%}\"],\n",
        "                ['Annualized Return', f\"{strategy_metrics['annualized_return']:.2%}\"],\n",
        "                ['Volatility', f\"{strategy_metrics['volatility']:.2%}\"],\n",
        "                ['Sharpe Ratio', f\"{strategy_metrics['sharpe_ratio']:.2f}\"],\n",
        "                ['Sortino Ratio', f\"{strategy_metrics['sortino_ratio']:.2f}\"],\n",
        "                ['Max Drawdown', f\"{strategy_metrics['max_drawdown']:.2%}\"],\n",
        "                ['Calmar Ratio', f\"{strategy_metrics['calmar_ratio']:.2f}\"],\n",
        "                ['Win Rate', f\"{strategy_metrics['win_rate']:.2%}\"],\n",
        "                ['Profit Factor', f\"{strategy_metrics['profit_factor']:.2f}\"],\n",
        "                ['VaR (95%)', f\"{strategy_metrics['var_95']:.2%}\"],\n",
        "                ['CVaR (95%)', f\"{strategy_metrics['cvar_95']:.2%}\"],\n",
        "                ['Skewness', f\"{strategy_metrics['skewness']:.2f}\"],\n",
        "                ['Kurtosis', f\"{strategy_metrics['kurtosis']:.2f}\"]\n",
        "            ]\n",
        "            \n",
        "            if benchmark_returns is not None:\n",
        "                metrics_data.extend([\n",
        "                    ['Alpha', f\"{strategy_metrics['alpha']:.2%}\"],\n",
        "                    ['Beta', f\"{strategy_metrics['beta']:.2f}\"],\n",
        "                    ['Information Ratio', f\"{strategy_metrics['information_ratio']:.2f}\"],\n",
        "                    ['Tracking Error', f\"{strategy_metrics['tracking_error']:.2%}\"]\n",
        "                ])\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Table(\n",
        "                    header=dict(values=['Metric', 'Value'], fill_color='lightblue'),\n",
        "                    cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "                ),\n",
        "                row=3, col=2\n",
        "            )\n",
        "            \n",
        "            # 7. Annual Returns Comparison (row 4, col 1)\n",
        "            annual_returns = strategy_returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)\n",
        "            fig.add_trace(\n",
        "                go.Bar(x=annual_returns.index.year, y=annual_returns.values,\n",
        "                      name='Annual Returns', marker_color='blue'),\n",
        "                row=4, col=1\n",
        "            )\n",
        "            \n",
        "            if benchmark_returns is not None:\n",
        "                benchmark_annual = benchmark_returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)\n",
        "                fig.add_trace(\n",
        "                    go.Bar(x=benchmark_annual.index.year, y=benchmark_annual.values,\n",
        "                          name='Benchmark Annual', marker_color='red'),\n",
        "                    row=4, col=1\n",
        "                )\n",
        "            \n",
        "            # 8. Risk Metrics Summary (row 4, col 2)\n",
        "            risk_metrics_data = [\n",
        "                ['Max Drawdown Duration', f\"{strategy_metrics['max_drawdown_duration']:.0f} days\"],\n",
        "                ['Tail Ratio', f\"{strategy_metrics['tail_ratio']:.2f}\"],\n",
        "                ['Average Win', f\"{strategy_metrics['avg_win']:.2%}\"],\n",
        "                ['Average Loss', f\"{strategy_metrics['avg_loss']:.2%}\"],\n",
        "                ['Cost Impact', f\"{strategy_metrics['cost_impact']:.2%}\"]\n",
        "            ]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Table(\n",
        "                    header=dict(values=['Risk Metric', 'Value'], fill_color='lightcoral'),\n",
        "                    cells=dict(values=list(zip(*risk_metrics_data)), fill_color='white')\n",
        "                ),\n",
        "                row=4, col=2\n",
        "            )\n",
        "            \n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                height=1600,\n",
        "                title_text=f\"{strategy_name} - Comprehensive Performance Dashboard\",\n",
        "                showlegend=True\n",
        "            )\n",
        "            \n",
        "            return fig, strategy_metrics\n",
        "        \n",
        "        def calculate_technical_metrics(self, returns, benchmark_returns=None):\n",
        "            \"\"\"Calculate technical analysis metrics for strategy comparison\"\"\"\n",
        "            metrics = self.calculate_portfolio_performance({'returns': returns}, benchmark_returns)\n",
        "            \n",
        "            # Add technical-specific metrics\n",
        "            technical_metrics = {\n",
        "                'sharpe_ratio': metrics['sharpe_ratio'],\n",
        "                'sortino_ratio': metrics['sortino_ratio'],\n",
        "                'calmar_ratio': metrics['calmar_ratio'],\n",
        "                'max_drawdown': metrics['max_drawdown'],\n",
        "                'volatility': metrics['volatility'],\n",
        "                'win_rate': metrics['win_rate'],\n",
        "                'profit_factor': metrics['profit_factor'],\n",
        "                'var_95': metrics['var_95'],\n",
        "                'cvar_95': metrics['cvar_95'],\n",
        "                'tail_ratio': metrics['tail_ratio'],\n",
        "                'skewness': metrics['skewness'],\n",
        "                'kurtosis': metrics['kurtosis']\n",
        "            }\n",
        "            \n",
        "            if benchmark_returns is not None:\n",
        "                technical_metrics.update({\n",
        "                    'alpha': metrics['alpha'],\n",
        "                    'beta': metrics['beta'],\n",
        "                    'information_ratio': metrics['information_ratio'],\n",
        "                    'tracking_error': metrics['tracking_error']\n",
        "                })\n",
        "            \n",
        "            return technical_metrics\n",
        "        \n",
        "        def create_model_performance_analysis(self, model_history, X_test, y_test, model_name=\"CNN+LSTM\"):\n",
        "            \"\"\"Create comprehensive model performance analysis dashboard\"\"\"\n",
        "            \n",
        "            # Extract training history\n",
        "            train_loss = model_history.history['loss']\n",
        "            val_loss = model_history.history['val_loss']\n",
        "            train_mae = model_history.history['mae']\n",
        "            val_mae = model_history.history['val_mae']\n",
        "            \n",
        "            # Make predictions\n",
        "            predictions = model_history.model.predict(X_test, verbose=0)\n",
        "            predictions = predictions.flatten()\n",
        "            \n",
        "            # Calculate prediction metrics\n",
        "            mse = np.mean((y_test - predictions) ** 2)\n",
        "            mae = np.mean(np.abs(y_test - predictions))\n",
        "            rmse = np.sqrt(mse)\n",
        "            mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "            \n",
        "            # Calculate R-squared\n",
        "            ss_res = np.sum((y_test - predictions) ** 2)\n",
        "            ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
        "            \n",
        "            # Create subplots\n",
        "            fig = make_subplots(\n",
        "                rows=2, cols=2,\n",
        "                subplot_titles=[\n",
        "                    'Training History - Loss',\n",
        "                    'Training History - MAE',\n",
        "                    'Predictions vs Actual',\n",
        "                    'Model Performance Metrics'\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            # 1. Training Loss\n",
        "            epochs = range(1, len(train_loss) + 1)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=epochs, y=train_loss, name='Train Loss', line=dict(color='blue')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=epochs, y=val_loss, name='Validation Loss', line=dict(color='red')),\n",
        "                row=1, col=1\n",
        "            )\n",
        "            \n",
        "            # 2. Training MAE\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=epochs, y=train_mae, name='Train MAE', line=dict(color='blue'), showlegend=False),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=epochs, y=val_mae, name='Validation MAE', line=dict(color='red'), showlegend=False),\n",
        "                row=1, col=2\n",
        "            )\n",
        "            \n",
        "            # 3. Predictions vs Actual\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=y_test, y=predictions, mode='markers', name='Predictions',\n",
        "                          marker=dict(color='blue', size=4)),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            \n",
        "            # Add perfect prediction line\n",
        "            min_val = min(y_test.min(), predictions.min())\n",
        "            max_val = max(y_test.max(), predictions.max())\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=[min_val, max_val], y=[min_val, max_val], \n",
        "                          mode='lines', name='Perfect Prediction',\n",
        "                          line=dict(color='red', dash='dash')),\n",
        "                row=2, col=1\n",
        "            )\n",
        "            \n",
        "            # 4. Performance Metrics Table\n",
        "            metrics_data = [\n",
        "                ['MSE', f\"{mse:.6f}\"],\n",
        "                ['MAE', f\"{mae:.6f}\"],\n",
        "                ['RMSE', f\"{rmse:.6f}\"],\n",
        "                ['MAPE', f\"{mape:.2f}%\"],\n",
        "                ['R²', f\"{r_squared:.4f}\"],\n",
        "                ['Training Samples', f\"{len(X_test):,}\"],\n",
        "                ['Model Parameters', f\"{model_history.model.count_params():,}\"]\n",
        "            ]\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Table(\n",
        "                    header=dict(values=['Metric', 'Value'], fill_color='lightblue'),\n",
        "                    cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "            \n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                height=800,\n",
        "                title_text=f\"{model_name} - Model Performance Analysis\",\n",
        "                showlegend=True\n",
        "            )\n",
        "            \n",
        "            # Update axes labels\n",
        "            fig.update_xaxes(title_text=\"Epochs\", row=1, col=1)\n",
        "            fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
        "            fig.update_xaxes(title_text=\"Epochs\", row=1, col=2)\n",
        "            fig.update_yaxes(title_text=\"MAE\", row=1, col=2)\n",
        "            fig.update_xaxes(title_text=\"Actual Values\", row=2, col=1)\n",
        "            fig.update_yaxes(title_text=\"Predicted Values\", row=2, col=1)\n",
        "            \n",
        "            # Prepare metrics dictionary\n",
        "            model_metrics = {\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'rmse': rmse,\n",
        "                'mape': mape,\n",
        "                'r_squared': r_squared,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_mae': train_mae,\n",
        "                'val_mae': val_mae,\n",
        "                'predictions': predictions,\n",
        "                'actual': y_test\n",
        "            }\n",
        "            \n",
        "            return fig, model_metrics\n",
        "    \n",
        "    # Initialize improved classes\n",
        "    print(\"✅ Enhanced implementations created!\")\n",
        "    print(\"📊 Advanced Features implemented:\")\n",
        "    print(\"  • Multi-Factor Ensemble Signal Generation (Trend, Momentum, Volatility, Volume, Regime)\")\n",
        "    print(\"  • Advanced Technical Indicators (Williams %R, Stochastic, ATR, CCI, MFI, OBV)\")\n",
        "    print(\"  • Market Regime Detection with Multi-Timeframe Analysis\")\n",
        "    print(\"  • Kelly Criterion Position Sizing with Dynamic Risk Management\")\n",
        "    print(\"  • Enhanced CNN+LSTM Model with Regularization and Better Architecture\")\n",
        "    print(\"  • Realistic Backtesting with Commissions, Slippage, and Bid-Ask Spreads\")\n",
        "    print(\"  • Comprehensive Performance Analysis with Advanced Risk Metrics\")\n",
        "    print(\"  • Dynamic Stop-Loss and Take-Profit Based on Volatility and Regime\")\n",
        "    print(\"  • Pattern Recognition (Hammer, Shooting Star, Engulfing)\")\n",
        "    print(\"  • Advanced Risk Metrics (VaR, CVaR, Tail Ratio, Skewness, Kurtosis)\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize improved data collector and indicator engine\n",
        "collector = MarketDataCollector()\n",
        "indicator_engine = TechnicalIndicatorEngine()\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "# Define symbols to trade\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
        "\n",
        "# Collect historical data\n",
        "print(\"📊 Collecting historical data with improved error handling...\")\n",
        "data = {}\n",
        "for symbol in symbols:\n",
        "    try:\n",
        "        df = collector.get_historical_data(symbol, period='2y', interval='1d')\n",
        "        if df is not None and len(df) > 0:\n",
        "            data[symbol] = df\n",
        "            print(f\"✓ {symbol}: {len(df)} records\")\n",
        "        else:\n",
        "            print(f\"✗ {symbol}: No data received\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {symbol}: Error - {e}\")\n",
        "\n",
        "print(f\"\\nData collection complete! Collected data for {len(data)} symbols.\")\n",
        "\n",
        "# Process data with advanced technical indicators and feature engineering\n",
        "if data:\n",
        "    print(\"\\n🔧 Processing data with advanced technical indicators and feature engineering...\")\n",
        "    processed_data = {}\n",
        "    \n",
        "    for symbol, df in data.items():\n",
        "        try:\n",
        "            # Step 1: Calculate comprehensive technical indicators\n",
        "            print(f\"  📈 Calculating advanced technical indicators for {symbol}...\")\n",
        "            df_with_indicators = indicator_engine.calculate_all_indicators(df)\n",
        "            \n",
        "            # Step 2: Advanced feature engineering with market regime detection\n",
        "            print(f\"  🧠 Creating advanced features with market regime detection for {symbol}...\")\n",
        "            df_with_features = feature_engineer.create_features(df_with_indicators)\n",
        "            \n",
        "            processed_data[symbol] = df_with_features\n",
        "            print(f\"✓ {symbol}: Advanced technical indicators and features calculated\")\n",
        "            \n",
        "            # Display feature summary\n",
        "            feature_count = len([col for col in df_with_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])\n",
        "            print(f\"  📊 {symbol}: {feature_count} features created\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ {symbol}: Error processing - {e}\")\n",
        "            processed_data[symbol] = df  # Use original data if processing fails\n",
        "\n",
        "    print(f\"\\n✅ Data processing complete! Processed {len(processed_data)} symbols.\")\n",
        "    \n",
        "    # Display sample processed data with new features\n",
        "    sample_symbol = list(processed_data.keys())[0]\n",
        "    sample_data = processed_data[sample_symbol]\n",
        "    \n",
        "    print(f\"\\n📋 Sample processed data for {sample_symbol}:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show key technical indicators\n",
        "    key_indicators = ['Close', 'SMA_20', 'SMA_50', 'RSI', 'MACD', 'BB_Position', 'ATR_Ratio', 'Williams_R', 'Stoch_K', 'CCI', 'MFI', 'Market_Regime']\n",
        "    available_indicators = [col for col in key_indicators if col in sample_data.columns]\n",
        "    \n",
        "    if available_indicators:\n",
        "        print(\"Advanced Technical Indicators:\")\n",
        "        print(sample_data[available_indicators].tail())\n",
        "    \n",
        "    # Show feature summary\n",
        "    print(f\"\\n📊 Feature Summary for {sample_symbol}:\")\n",
        "    print(f\"Total features: {len(sample_data.columns)}\")\n",
        "    print(f\"Technical indicators: {len([col for col in sample_data.columns if col.startswith(('SMA', 'EMA', 'RSI', 'MACD', 'BB', 'Stoch', 'Williams', 'ATR', 'CCI', 'MFI', 'OBV'))])}\")\n",
        "    print(f\"Price features: {len([col for col in sample_data.columns if col.startswith(('Returns', 'Momentum', 'Price', 'Volatility'))])}\")\n",
        "    print(f\"Market regime features: {len([col for col in sample_data.columns if col.startswith('Regime')])}\")\n",
        "    print(f\"Pattern recognition: {len([col for col in sample_data.columns if col.startswith(('Doji', 'Hammer', 'Shooting', 'Engulfing'))])}\")\n",
        "    \n",
        "    # Show market regime distribution\n",
        "    if 'Market_Regime' in sample_data.columns:\n",
        "        regime_counts = sample_data['Market_Regime'].value_counts()\n",
        "        print(f\"\\n🎯 Market Regime Distribution:\")\n",
        "        for regime, count in regime_counts.items():\n",
        "            percentage = (count / len(sample_data)) * 100\n",
        "            print(f\"  {regime}: {count} days ({percentage:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No data collected. Check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train improved CNN+LSTM model with GPU and advanced features\n",
        "print(\"🤖 Training improved CNN+LSTM model with advanced architecture...\")\n",
        "\n",
        "# Prepare training data from processed data with comprehensive features\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    print(\"📊 Preparing training data from collected market data with advanced features...\")\n",
        "    \n",
        "    # Combine all data for training\n",
        "    all_data = []\n",
        "    for symbol, df in processed_data.items():\n",
        "        df['symbol'] = symbol\n",
        "        all_data.append(df)\n",
        "    \n",
        "    combined_data = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"Combined dataset shape: {combined_data.shape}\")\n",
        "    \n",
        "    # Select comprehensive features for training\n",
        "    feature_columns = [\n",
        "        # Price features\n",
        "        'Returns', 'Log_Returns', 'Price_Ratio', 'High_Low_Ratio', 'Close_Open_Ratio',\n",
        "        # Volatility features\n",
        "        'Volatility_5', 'Volatility_20', 'Volatility_Ratio',\n",
        "        # Momentum features\n",
        "        'Momentum_5', 'Momentum_10', 'Momentum_20',\n",
        "        # Trend features\n",
        "        'Trend_Strength', 'Price_vs_SMA20', 'Price_vs_SMA50',\n",
        "        # Advanced technical indicators\n",
        "        'RSI', 'RSI_6', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
        "        'BB_Position', 'BB_Width', 'BB_Squeeze', 'BB_Expansion',\n",
        "        'Stoch_K', 'Stoch_D', 'Williams_R', 'ATR_Ratio',\n",
        "        'CCI', 'MFI', 'OBV',\n",
        "        # Volume features\n",
        "        'Volume_Ratio', 'Volume_Change', 'Price_Volume_Trend',\n",
        "        # Technical indicator features\n",
        "        'RSI_Overbought', 'RSI_Oversold', 'MACD_Bullish', 'MACD_Bearish',\n",
        "        # Market regime features\n",
        "        'Regime_Bull', 'Regime_Bear', 'Regime_Sideways', 'Regime_High_Vol',\n",
        "        'Regime_Strength', 'Regime_Change',\n",
        "        # Pattern recognition\n",
        "        'Doji', 'Hammer', 'Shooting_Star', 'Engulfing',\n",
        "        # Lagged features\n",
        "        'Returns_Lag_1', 'Returns_Lag_2', 'Returns_Lag_3', 'Returns_Lag_5',\n",
        "        'Volume_Ratio_Lag_1', 'Volume_Ratio_Lag_2', 'Volume_Ratio_Lag_3', 'Volume_Ratio_Lag_5'\n",
        "    ]\n",
        "    \n",
        "    # Filter available features\n",
        "    available_features = [col for col in feature_columns if col in combined_data.columns]\n",
        "    print(f\"📈 Available features for training: {len(available_features)}\")\n",
        "    print(f\"Features: {available_features[:10]}...\")  # Show first 10 features\n",
        "    \n",
        "    if len(available_features) >= 10:  # Need at least 10 features for robust training\n",
        "        # Initialize improved model with correct number of features\n",
        "        cnn_lstm = CNNLSTMModel(\n",
        "            time_steps=60,\n",
        "            n_features=len(available_features),\n",
        "            learning_rate=0.0001  # Lower learning rate for better convergence\n",
        "        )\n",
        "        \n",
        "        # Build improved model\n",
        "        model = cnn_lstm.build_model()\n",
        "        print(f\"🏗️ Improved model built with {model.count_params():,} parameters\")\n",
        "        print(f\"📊 Model expects {len(available_features)} input features\")\n",
        "        \n",
        "        # IMMEDIATE FIX: Replace the problematic line with this robust version\n",
        "        print(\"🔧 Applying immediate fix for data type issues...\")\n",
        "        \n",
        "        # Convert all features to numeric first\n",
        "        X_data_numeric = []\n",
        "        for i, feature in enumerate(available_features):\n",
        "            if feature in combined_data.columns:\n",
        "                # Convert to numeric, coercing errors to NaN\n",
        "                numeric_series = pd.to_numeric(combined_data[feature], errors='coerce')\n",
        "                X_data_numeric.append(numeric_series.values)\n",
        "            else:\n",
        "                print(f\"⚠️ Feature {feature} not found in data\")\n",
        "                # Create array of NaNs with same length\n",
        "                X_data_numeric.append(np.full(len(combined_data), np.nan))\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        X_data = np.array(X_data_numeric).T\n",
        "        \n",
        "        # Handle target variable\n",
        "        y_data = pd.to_numeric(combined_data['Close'].shift(-1), errors='coerce').values\n",
        "        \n",
        "        print(f\"📊 Data types after conversion:\")\n",
        "        print(f\"  X_data shape: {X_data.shape}, dtype: {X_data.dtype}\")\n",
        "        print(f\"  y_data shape: {y_data.shape}, dtype: {y_data.dtype}\")\n",
        "        \n",
        "        # Now safely remove NaN values\n",
        "        print(\"🧹 Cleaning data and removing NaN values...\")\n",
        "        \n",
        "        # Check for NaN values safely\n",
        "        x_nan_mask = np.isnan(X_data).any(axis=1)\n",
        "        y_nan_mask = np.isnan(y_data)\n",
        "        \n",
        "        # Combine masks\n",
        "        valid_indices = ~(x_nan_mask | y_nan_mask)\n",
        "        \n",
        "        print(f\"📊 Data cleaning summary:\")\n",
        "        print(f\"  Total samples: {len(X_data)}\")\n",
        "        print(f\"  Samples with NaN in features: {x_nan_mask.sum()}\")\n",
        "        print(f\"  Samples with NaN in target: {y_nan_mask.sum()}\")\n",
        "        print(f\"  Valid samples after cleaning: {valid_indices.sum()}\")\n",
        "        \n",
        "        # Apply cleaning\n",
        "        X_data = X_data[valid_indices]\n",
        "        y_data = y_data[valid_indices]\n",
        "        \n",
        "        print(f\"📊 Data after cleaning: {len(X_data)} samples\")\n",
        "        \n",
        "        # Verify we have enough data for training\n",
        "        if len(X_data) < cnn_lstm.time_steps + 100:  # Need at least time_steps + 100 samples\n",
        "            print(f\"❌ Insufficient data after cleaning: {len(X_data)} samples\")\n",
        "            print(f\"   Need at least {cnn_lstm.time_steps + 100} samples for training\")\n",
        "            print(\"   Consider:\")\n",
        "            print(\"   1. Using more symbols in data collection\")\n",
        "            print(\"   2. Extending the data collection period\")\n",
        "            print(\"   3. Reducing the time_steps parameter\")\n",
        "            print(\"⚠️ Skipping model training due to insufficient data\")\n",
        "        else:\n",
        "            print(f\"✅ Sufficient data available: {len(X_data)} samples\")\n",
        "            \n",
        "            # Reshape for LSTM (samples, time_steps, features)\n",
        "            n_samples = len(X_data) - cnn_lstm.time_steps + 1\n",
        "            # Reshape for LSTM (samples, time_steps, features)\n",
        "            n_samples = len(X_data) - cnn_lstm.time_steps + 1\n",
        "            X_reshaped = np.zeros((n_samples, cnn_lstm.time_steps, len(available_features)))\n",
        "            y_reshaped = np.zeros((n_samples, 1))\n",
        "            \n",
        "            for i in range(n_samples):\n",
        "                X_reshaped[i] = X_data[i:i+cnn_lstm.time_steps]\n",
        "                y_reshaped[i] = y_data[i+cnn_lstm.time_steps-1]\n",
        "            \n",
        "            # Split data\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X_reshaped, y_reshaped, test_size=0.2, random_state=42, shuffle=False\n",
        "            )\n",
        "            \n",
        "            print(f\"📊 Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "            print(f\"📊 Test data shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "            \n",
        "            # Verify dimensions match\n",
        "            print(f\"🔍 Model input shape: (batch_size, {cnn_lstm.time_steps}, {cnn_lstm.n_features})\")\n",
        "            print(f\"🔍 Actual data shape: {X_train.shape}\")\n",
        "            \n",
        "            if X_train.shape[2] == cnn_lstm.n_features:\n",
        "                print(\"✅ Dimensions match! Proceeding with improved training...\")\n",
        "                \n",
        "                # Train improved model with early stopping and learning rate reduction\n",
        "                history = cnn_lstm.train(\n",
        "                    X_train, y_train,\n",
        "                    epochs=50,  # More epochs with early stopping\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2\n",
        "                )\n",
        "                \n",
        "                print(\"🎉 Improved CNN+LSTM training complete!\")\n",
        "                \n",
        "                # Plot comprehensive training history\n",
        "                plt.figure(figsize=(15, 5))\n",
        "                \n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "                plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "                plt.title('Model Loss (Huber Loss)')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                \n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.plot(history.history['mae'], label='Training MAE', color='blue')\n",
        "                plt.plot(history.history['val_mae'], label='Validation MAE', color='red')\n",
        "                plt.title('Model MAE')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('MAE')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                \n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.plot(history.history['mse'], label='Training MSE', color='blue')\n",
        "                plt.plot(history.history['val_mse'], label='Validation MSE', color='red')\n",
        "                plt.title('Model MSE')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('MSE')\n",
        "                plt.legend()\n",
        "                plt.grid(True)\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                \n",
        "                # Evaluate improved model\n",
        "                test_loss, test_mae, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "                print(f\"\\n📊 Improved Model Evaluation:\")\n",
        "                print(f\"Test Loss (Huber): {test_loss:.6f}\")\n",
        "                print(f\"Test MAE: {test_mae:.6f}\")\n",
        "                print(f\"Test MSE: {test_mse:.6f}\")\n",
        "                \n",
        "                # Calculate R² score\n",
        "                from sklearn.metrics import r2_score\n",
        "                predictions = model.predict(X_test)\n",
        "                r2 = r2_score(y_test, predictions)\n",
        "                print(f\"R² Score: {r2:.4f}\")\n",
        "                \n",
        "                # Model quality assessment\n",
        "                if r2 > 0.7:\n",
        "                    print(\"🎉 Excellent model performance (R² > 0.7)\")\n",
        "                elif r2 > 0.5:\n",
        "                    print(\"✅ Good model performance (R² > 0.5)\")\n",
        "                elif r2 > 0.3:\n",
        "                    print(\"⚠️ Moderate model performance (R² > 0.3)\")\n",
        "                else:\n",
        "                    print(\"❌ Poor model performance (R² < 0.3)\")\n",
        "                \n",
        "                # Store model and data for later use\n",
        "                trained_model = model\n",
        "                model_history = history\n",
        "                \n",
        "            else:\n",
        "                print(f\"❌ Dimension mismatch! Model expects {cnn_lstm.n_features} features but data has {X_train.shape[2]}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"⚠️ Insufficient features for training. Need at least 10 features.\")\n",
        "        print(f\"Available features: {len(available_features)}\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ No processed data available. Please run the data collection and processing cells first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALTERNATIVE FIX: If you're still getting the TypeError, run this cell instead\n",
        "# This provides a direct solution to the data type issue\n",
        "\n",
        "print(\"🔧 ALTERNATIVE FIX: Direct data type conversion...\")\n",
        "\n",
        "# Get the problematic data\n",
        "if 'combined_data' in locals() and 'available_features' in locals():\n",
        "    print(\"📊 Applying direct fix to data...\")\n",
        "    \n",
        "    # Method 1: Direct conversion using pandas\n",
        "    try:\n",
        "        # Select only numeric columns\n",
        "        numeric_columns = []\n",
        "        for feature in available_features:\n",
        "            if feature in combined_data.columns:\n",
        "                # Try to convert to numeric\n",
        "                try:\n",
        "                    pd.to_numeric(combined_data[feature], errors='raise')\n",
        "                    numeric_columns.append(feature)\n",
        "                except:\n",
        "                    print(f\"⚠️ Skipping non-numeric feature: {feature}\")\n",
        "        \n",
        "        print(f\"📈 Using {len(numeric_columns)} numeric features out of {len(available_features)}\")\n",
        "        \n",
        "        # Create clean numeric data\n",
        "        X_data = combined_data[numeric_columns].values.astype(np.float64)\n",
        "        y_data = combined_data['Close'].shift(-1).values.astype(np.float64)\n",
        "        \n",
        "        # Remove NaN values\n",
        "        mask = ~(np.isnan(X_data).any(axis=1) | np.isnan(y_data))\n",
        "        X_data = X_data[mask]\n",
        "        y_data = y_data[mask]\n",
        "        \n",
        "        print(f\"✅ Successfully created clean data:\")\n",
        "        print(f\"  X_data shape: {X_data.shape}, dtype: {X_data.dtype}\")\n",
        "        print(f\"  y_data shape: {y_data.shape}, dtype: {y_data.dtype}\")\n",
        "        print(f\"  Valid samples: {len(X_data)}\")\n",
        "        \n",
        "        # Update the available features to match what we actually used\n",
        "        available_features = numeric_columns\n",
        "        print(f\"📊 Updated available features: {len(available_features)}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in alternative fix: {e}\")\n",
        "        print(\"Please check your data and try again.\")\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ No data available. Please run the data collection and processing cells first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear any cached model saving code and restart\n",
        "print(\"🔄 Clearing cached model saving code...\")\n",
        "print(\"If you get git errors, please:\")\n",
        "print(\"1. Go to Runtime > Restart Runtime\")\n",
        "print(\"2. Run all cells from the beginning\")\n",
        "print(\"3. The model saving now only uses Google Drive (no git operations)\")\n",
        "\n",
        "# Clear any cached variables that might cause issues\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Remove any cached modules that might cause issues\n",
        "modules_to_clear = ['subprocess', 'os', 'shutil']\n",
        "for module in modules_to_clear:\n",
        "    if module in sys.modules:\n",
        "        del sys.modules[module]\n",
        "\n",
        "print(\"✅ Cache cleared. Model saving will now only use Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Your Work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models to Google Drive for permanent storage\n",
        "# Note: Using .keras format instead of .h5 for better compatibility\n",
        "\n",
        "# 1. Save locally first (using modern .keras format)\n",
        "cnn_lstm.save_model('cnn_lstm_model.keras')\n",
        "print(\"✓ Model saved locally in .keras format\")\n",
        "\n",
        "# 2. Save to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your trading models\n",
        "drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Save model to Google Drive with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_model_path = f'{drive_folder}/cnn_lstm_model_{timestamp}.keras'\n",
        "shutil.copy('cnn_lstm_model.keras', drive_model_path)\n",
        "print(f\"✓ Model saved to Google Drive: {drive_model_path}\")\n",
        "\n",
        "print(\"\\n🎉 Model saved in 2 locations:\")\n",
        "print(\"1. Local Colab environment (cnn_lstm_model.keras)\")\n",
        "print(\"2. Google Drive (permanent storage)\")\n",
        "print(\"\\n💡 Note: Using .keras format instead of .h5 for better compatibility\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Saved Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed Model Loading Function with Compatibility Handling\n",
        "\n",
        "def load_model_from_location_fixed(location_type, model_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained model from different storage locations with compatibility handling\n",
        "    \n",
        "    Args:\n",
        "        location_type: 'local', 'drive', 'github', or 'url'\n",
        "        model_path: Path to the model file (optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    if location_type == 'local':\n",
        "        # Load from local Colab environment\n",
        "        if model_path is None:\n",
        "            # Try .keras format first, then .h5\n",
        "            if os.path.exists('cnn_lstm_model.keras'):\n",
        "                model_path = 'cnn_lstm_model.keras'\n",
        "            elif os.path.exists('cnn_lstm_model.h5'):\n",
        "                model_path = 'cnn_lstm_model.h5'\n",
        "            else:\n",
        "                print(\"No model found locally\")\n",
        "                return None\n",
        "        \n",
        "        try:\n",
        "            model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"✓ Model loaded from local: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading model from local: {e}\")\n",
        "            return None\n",
        "        \n",
        "    elif location_type == 'drive':\n",
        "        # Load from Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        if model_path is None:\n",
        "            # List available models in Drive\n",
        "            drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "            if os.path.exists(drive_folder):\n",
        "                # Look for both .keras and .h5 files\n",
        "                keras_models = [f for f in os.listdir(drive_folder) if f.endswith('.keras')]\n",
        "                h5_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "                all_models = keras_models + h5_models\n",
        "                \n",
        "                if all_models:\n",
        "                    # Prefer .keras files, but use .h5 if that's all we have\n",
        "                    if keras_models:\n",
        "                        model_path = os.path.join(drive_folder, keras_models[-1])\n",
        "                        print(f\"Available .keras models: {keras_models}\")\n",
        "                    else:\n",
        "                        model_path = os.path.join(drive_folder, h5_models[-1])\n",
        "                        print(f\"Available .h5 models: {h5_models}\")\n",
        "                        print(\"⚠️ Loading .h5 model - may have compatibility issues\")\n",
        "                else:\n",
        "                    print(\"No models found in Google Drive\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Trading_Strategy_ML folder not found in Google Drive\")\n",
        "                return None\n",
        "        \n",
        "        try:\n",
        "            # Try loading with custom objects to handle compatibility issues\n",
        "            custom_objects = {\n",
        "                'mse': tf.keras.metrics.mean_squared_error,\n",
        "                'mae': tf.keras.metrics.mean_absolute_error,\n",
        "                'accuracy': tf.keras.metrics.accuracy\n",
        "            }\n",
        "            model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "            print(f\"✓ Model loaded from Google Drive: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading model from Google Drive: {e}\")\n",
        "            print(\"💡 Try training a new model with the current TensorFlow version\")\n",
        "            return None\n",
        "    \n",
        "    else:\n",
        "        print(\"Invalid location_type. Use 'local' or 'drive'\")\n",
        "        return None\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Example: Load the latest model from Google Drive with compatibility handling\n",
        "print(\"Loading model from Google Drive with compatibility handling...\")\n",
        "loaded_model = load_model_from_location_fixed('drive')\n",
        "\n",
        "if loaded_model is not None:\n",
        "    print(f\"Model summary:\")\n",
        "    loaded_model.summary()\n",
        "else:\n",
        "    print(\"No model found. Train a model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Technical Benchmarks and Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Technical Benchmarks and Performance Analysis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TradingBenchmark:\n",
        "    \"\"\"Comprehensive trading strategy benchmark and analysis\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.benchmarks = {}\n",
        "        self.results = {}\n",
        "        \n",
        "    def calculate_technical_metrics(self, returns, benchmark_returns=None):\n",
        "        \"\"\"Calculate comprehensive technical metrics\"\"\"\n",
        "        metrics = {}\n",
        "        \n",
        "        # Basic metrics\n",
        "        metrics['total_return'] = (1 + returns).prod() - 1\n",
        "        metrics['annualized_return'] = (1 + returns).prod() ** (252 / len(returns)) - 1\n",
        "        metrics['volatility'] = returns.std() * np.sqrt(252)\n",
        "        metrics['sharpe_ratio'] = metrics['annualized_return'] / metrics['volatility'] if metrics['volatility'] > 0 else 0\n",
        "        \n",
        "        # Risk metrics\n",
        "        negative_returns = returns[returns < 0]\n",
        "        metrics['sortino_ratio'] = metrics['annualized_return'] / (negative_returns.std() * np.sqrt(252)) if len(negative_returns) > 0 else 0\n",
        "        \n",
        "        # Drawdown analysis\n",
        "        cumulative = (1 + returns).cumprod()\n",
        "        running_max = cumulative.expanding().max()\n",
        "        drawdown = (cumulative - running_max) / running_max\n",
        "        metrics['max_drawdown'] = drawdown.min()\n",
        "        metrics['calmar_ratio'] = metrics['annualized_return'] / abs(metrics['max_drawdown']) if metrics['max_drawdown'] != 0 else 0\n",
        "        \n",
        "        # Win/Loss analysis\n",
        "        winning_trades = returns[returns > 0]\n",
        "        losing_trades = returns[returns < 0]\n",
        "        metrics['win_rate'] = len(winning_trades) / len(returns) if len(returns) > 0 else 0\n",
        "        metrics['avg_win'] = winning_trades.mean() if len(winning_trades) > 0 else 0\n",
        "        metrics['avg_loss'] = losing_trades.mean() if len(losing_trades) > 0 else 0\n",
        "        metrics['profit_factor'] = abs(winning_trades.sum() / losing_trades.sum()) if len(losing_trades) > 0 and losing_trades.sum() != 0 else 0\n",
        "        \n",
        "        # Benchmark comparison\n",
        "        if benchmark_returns is not None:\n",
        "            excess_returns = returns - benchmark_returns\n",
        "            metrics['alpha'] = excess_returns.mean() * 252\n",
        "            metrics['beta'] = returns.cov(benchmark_returns) / benchmark_returns.var() if benchmark_returns.var() > 0 else 0\n",
        "            metrics['information_ratio'] = excess_returns.mean() / excess_returns.std() * np.sqrt(252) if excess_returns.std() > 0 else 0\n",
        "            metrics['tracking_error'] = excess_returns.std() * np.sqrt(252)\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def create_performance_dashboard(self, strategy_returns, benchmark_returns=None, strategy_name=\"Trading Strategy\"):\n",
        "        \"\"\"Create comprehensive performance dashboard\"\"\"\n",
        "        \n",
        "        # Calculate metrics\n",
        "        strategy_metrics = self.calculate_technical_metrics(strategy_returns, benchmark_returns)\n",
        "        \n",
        "        # Create subplots with correct specs for table\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Cumulative Returns Comparison',\n",
        "                'Rolling Sharpe Ratio',\n",
        "                'Drawdown Analysis',\n",
        "                'Monthly Returns Heatmap',\n",
        "                'Risk-Return Scatter',\n",
        "                'Performance Metrics Table'\n",
        "            ],\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
        "        )\n",
        "        \n",
        "        # 1. Cumulative Returns\n",
        "        strategy_cumulative = (1 + strategy_returns).cumprod()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=strategy_cumulative.values,\n",
        "                      name=f'{strategy_name}', line=dict(color='blue', width=2)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        if benchmark_returns is not None:\n",
        "            benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=benchmark_returns.index, y=benchmark_cumulative.values,\n",
        "                          name='Benchmark (S&P 500)', line=dict(color='red', width=2)),\n",
        "                row=1, col=1\n",
        "            )\n",
        "        \n",
        "        # 2. Rolling Sharpe Ratio\n",
        "        rolling_sharpe = strategy_returns.rolling(window=252).mean() / strategy_returns.rolling(window=252).std() * np.sqrt(252)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=rolling_sharpe.values,\n",
        "                      name='Rolling Sharpe', line=dict(color='green')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Drawdown Analysis\n",
        "        cumulative = (1 + strategy_returns).cumprod()\n",
        "        running_max = cumulative.expanding().max()\n",
        "        drawdown = (cumulative - running_max) / running_max\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=strategy_returns.index, y=drawdown.values,\n",
        "                      name='Drawdown', fill='tonexty', line=dict(color='red')),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Monthly Returns Heatmap\n",
        "        monthly_returns = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
        "        monthly_pivot = monthly_returns.groupby([monthly_returns.index.year, monthly_returns.index.month]).first().unstack()\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Heatmap(z=monthly_pivot.values,\n",
        "                      x=monthly_pivot.columns,\n",
        "                      y=monthly_pivot.index,\n",
        "                      colorscale='RdYlGn',\n",
        "                      name='Monthly Returns'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 5. Risk-Return Scatter\n",
        "        if benchmark_returns is not None:\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=[strategy_metrics['volatility']], y=[strategy_metrics['annualized_return']],\n",
        "                          mode='markers', marker=dict(size=15, color='blue'),\n",
        "                          name=f'{strategy_name}'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "            benchmark_metrics = self.calculate_technical_metrics(benchmark_returns)\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=[benchmark_metrics['volatility']], y=[benchmark_metrics['annualized_return']],\n",
        "                          mode='markers', marker=dict(size=15, color='red'),\n",
        "                          name='Benchmark'),\n",
        "                row=3, col=1\n",
        "            )\n",
        "        \n",
        "        # 6. Performance Metrics Table\n",
        "        metrics_data = [\n",
        "            ['Total Return', f\"{strategy_metrics['total_return']:.2%}\"],\n",
        "            ['Annualized Return', f\"{strategy_metrics['annualized_return']:.2%}\"],\n",
        "            ['Volatility', f\"{strategy_metrics['volatility']:.2%}\"],\n",
        "            ['Sharpe Ratio', f\"{strategy_metrics['sharpe_ratio']:.2f}\"],\n",
        "            ['Sortino Ratio', f\"{strategy_metrics['sortino_ratio']:.2f}\"],\n",
        "            ['Max Drawdown', f\"{strategy_metrics['max_drawdown']:.2%}\"],\n",
        "            ['Calmar Ratio', f\"{strategy_metrics['calmar_ratio']:.2f}\"],\n",
        "            ['Win Rate', f\"{strategy_metrics['win_rate']:.2%}\"],\n",
        "            ['Profit Factor', f\"{strategy_metrics['profit_factor']:.2f}\"]\n",
        "        ]\n",
        "        \n",
        "        if benchmark_returns is not None:\n",
        "            metrics_data.extend([\n",
        "                ['Alpha', f\"{strategy_metrics['alpha']:.2%}\"],\n",
        "                ['Beta', f\"{strategy_metrics['beta']:.2f}\"],\n",
        "                ['Information Ratio', f\"{strategy_metrics['information_ratio']:.2f}\"]\n",
        "            ])\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Table(\n",
        "                header=dict(values=['Metric', 'Value'], fill_color='lightblue'),\n",
        "                cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "            ),\n",
        "            row=3, col=2\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=1200,\n",
        "            title_text=f\"{strategy_name} - Performance Dashboard\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        return fig, strategy_metrics\n",
        "    \n",
        "    def create_model_performance_analysis(self, model_history, X_test, y_test, model_name=\"CNN+LSTM\"):\n",
        "        \"\"\"Create model performance analysis\"\"\"\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                'Training History',\n",
        "                'Prediction vs Actual',\n",
        "                'Residuals Analysis',\n",
        "                'Model Performance Metrics'\n",
        "            ],\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
        "        )\n",
        "        \n",
        "        # 1. Training History\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=model_history.history['loss'], name='Training Loss', line=dict(color='blue')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(y=model_history.history['val_loss'], name='Validation Loss', line=dict(color='red')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Predictions vs Actual\n",
        "        predictions = model_history.model.predict(X_test)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test.flatten(), y=predictions.flatten(),\n",
        "                      mode='markers', name='Predictions vs Actual',\n",
        "                      marker=dict(color='blue', opacity=0.6)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Add perfect prediction line\n",
        "        min_val = min(y_test.min(), predictions.min())\n",
        "        max_val = max(y_test.max(), predictions.max())\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                      mode='lines', name='Perfect Prediction',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Residuals Analysis\n",
        "        residuals = y_test.flatten() - predictions.flatten()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=predictions.flatten(), y=residuals.flatten(),\n",
        "                      mode='markers', name='Residuals',\n",
        "                      marker=dict(color='green', opacity=0.6)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Model Metrics Table\n",
        "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "        \n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "        \n",
        "        metrics_data = [\n",
        "            ['MSE', f\"{mse:.6f}\"],\n",
        "            ['RMSE', f\"{rmse:.6f}\"],\n",
        "            ['MAE', f\"{mae:.6f}\"],\n",
        "            ['R² Score', f\"{r2:.4f}\"],\n",
        "            ['Training Samples', f\"{len(X_test):,}\"],\n",
        "            ['Model Parameters', f\"{model_history.model.count_params():,}\"]\n",
        "        ]\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Table(\n",
        "                header=dict(values=['Metric', 'Value'], fill_color='lightgreen'),\n",
        "                cells=dict(values=list(zip(*metrics_data)), fill_color='white')\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"{model_name} - Model Performance Analysis\",\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        return fig, {'mse': mse, 'mae': mae, 'r2': r2, 'rmse': rmse}\n",
        "\n",
        "# Initialize comprehensive benchmark analyzer\n",
        "benchmark_analyzer = ComprehensivePerformanceAnalyzer()\n",
        "\n",
        "print(\"✅ Comprehensive performance analyzer initialized!\")\n",
        "print(\"Available methods:\")\n",
        "print(\"- calculate_portfolio_performance()\")\n",
        "print(\"- create_comprehensive_dashboard()\")\n",
        "print(\"- Advanced risk metrics: VaR, CVaR, Tail Ratio, Skewness, Kurtosis\")\n",
        "print(\"- Benchmark comparison: Alpha, Beta, Information Ratio, Tracking Error\")\n",
        "print(\"- Trading cost analysis and drawdown duration analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Sample Trading Strategy Results for Benchmarking\n",
        "print(\"📊 Generating sample trading strategy results for benchmarking...\")\n",
        "\n",
        "# Create sample strategy returns (simulate a trading strategy)\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    # Use real data if available\n",
        "    sample_symbol = list(processed_data.keys())[0]\n",
        "    sample_data = processed_data[sample_symbol]\n",
        "    \n",
        "    # Generate strategy returns based on technical indicators\n",
        "    strategy_returns = []\n",
        "    for i in range(1, len(sample_data)):\n",
        "        # Simple strategy: Buy when RSI < 30, Sell when RSI > 70\n",
        "        current_rsi = sample_data['RSI'].iloc[i] if 'RSI' in sample_data.columns else 50\n",
        "        price_change = sample_data['Close'].iloc[i] / sample_data['Close'].iloc[i-1] - 1\n",
        "        \n",
        "        if current_rsi < 30:  # Oversold - Buy\n",
        "            strategy_returns.append(price_change * 1.0)  # Full position\n",
        "        elif current_rsi > 70:  # Overbought - Sell\n",
        "            strategy_returns.append(-price_change * 0.5)  # Short position\n",
        "        else:  # Hold\n",
        "            strategy_returns.append(0)\n",
        "    \n",
        "    strategy_returns = pd.Series(strategy_returns, index=sample_data.index[1:])\n",
        "    \n",
        "else:\n",
        "    # Generate synthetic strategy returns for demonstration\n",
        "    np.random.seed(42)\n",
        "    dates = pd.date_range(start='2022-01-01', end='2024-01-01', freq='D')\n",
        "    strategy_returns = pd.Series(np.random.normal(0.0005, 0.02, len(dates)), index=dates)\n",
        "\n",
        "# Generate benchmark returns (S&P 500 simulation)\n",
        "np.random.seed(123)\n",
        "benchmark_dates = strategy_returns.index\n",
        "benchmark_returns = pd.Series(np.random.normal(0.0003, 0.015, len(benchmark_dates)), index=benchmark_dates)\n",
        "\n",
        "print(f\"✅ Generated strategy returns: {len(strategy_returns)} days\")\n",
        "print(f\"✅ Generated benchmark returns: {len(benchmark_returns)} days\")\n",
        "print(f\"📈 Strategy total return: {(1 + strategy_returns).prod() - 1:.2%}\")\n",
        "print(f\"📈 Benchmark total return: {(1 + benchmark_returns).prod() - 1:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Comprehensive Performance Dashboard\n",
        "print(\"🎯 Creating comprehensive performance dashboard with advanced analytics...\")\n",
        "\n",
        "# Generate comprehensive performance dashboard\n",
        "performance_fig, strategy_metrics = benchmark_analyzer.create_comprehensive_dashboard(\n",
        "    strategy_returns, \n",
        "    benchmark_returns, \n",
        "    \"Multi-Factor Ensemble Trading Strategy\"\n",
        ")\n",
        "\n",
        "# Display the dashboard\n",
        "performance_fig.show()\n",
        "\n",
        "# Print detailed metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 COMPREHENSIVE PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"📈 Returns:\")\n",
        "print(f\"  • Total Return: {strategy_metrics['total_return']:.2%}\")\n",
        "print(f\"  • Annualized Return: {strategy_metrics['annualized_return']:.2%}\")\n",
        "\n",
        "print(f\"\\n📊 Risk Metrics:\")\n",
        "print(f\"  • Volatility: {strategy_metrics['volatility']:.2%}\")\n",
        "print(f\"  • Sharpe Ratio: {strategy_metrics['sharpe_ratio']:.2f}\")\n",
        "print(f\"  • Sortino Ratio: {strategy_metrics['sortino_ratio']:.2f}\")\n",
        "\n",
        "print(f\"\\n📉 Drawdown Analysis:\")\n",
        "print(f\"  • Max Drawdown: {strategy_metrics['max_drawdown']:.2%}\")\n",
        "print(f\"  • Calmar Ratio: {strategy_metrics['calmar_ratio']:.2f}\")\n",
        "print(f\"  • Max Drawdown Duration: {strategy_metrics['max_drawdown_duration']:.0f} days\")\n",
        "\n",
        "print(f\"\\n🎯 Trading Performance:\")\n",
        "print(f\"  • Win Rate: {strategy_metrics['win_rate']:.2%}\")\n",
        "print(f\"  • Average Win: {strategy_metrics['avg_win']:.2%}\")\n",
        "print(f\"  • Average Loss: {strategy_metrics['avg_loss']:.2%}\")\n",
        "print(f\"  • Profit Factor: {strategy_metrics['profit_factor']:.2f}\")\n",
        "\n",
        "print(f\"\\n📊 Advanced Risk Metrics:\")\n",
        "print(f\"  • VaR (95%): {strategy_metrics['var_95']:.2%}\")\n",
        "print(f\"  • CVaR (95%): {strategy_metrics['cvar_95']:.2%}\")\n",
        "print(f\"  • Tail Ratio: {strategy_metrics['tail_ratio']:.2f}\")\n",
        "print(f\"  • Skewness: {strategy_metrics['skewness']:.2f}\")\n",
        "print(f\"  • Kurtosis: {strategy_metrics['kurtosis']:.2f}\")\n",
        "\n",
        "if 'alpha' in strategy_metrics and strategy_metrics['alpha'] != 0:\n",
        "    print(f\"\\n📊 Benchmark Comparison:\")\n",
        "    print(f\"  • Alpha: {strategy_metrics['alpha']:.2%}\")\n",
        "    print(f\"  • Beta: {strategy_metrics['beta']:.2f}\")\n",
        "    print(f\"  • Information Ratio: {strategy_metrics['information_ratio']:.2f}\")\n",
        "    print(f\"  • Tracking Error: {strategy_metrics['tracking_error']:.2%}\")\n",
        "\n",
        "print(f\"\\n💰 Trading Costs:\")\n",
        "print(f\"  • Cost Impact: {strategy_metrics['cost_impact']:.2%}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Multi-Factor Trading Strategy Demonstration\n",
        "print(\"🚀 Demonstrating Enhanced Multi-Factor Trading Strategy...\")\n",
        "\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    # Use the first symbol for demonstration\n",
        "    demo_symbol = list(processed_data.keys())[0]\n",
        "    demo_data = processed_data[demo_symbol].dropna()\n",
        "    \n",
        "    print(f\"📊 Running enhanced strategy on {demo_symbol} with {len(demo_data)} data points\")\n",
        "    \n",
        "    # Initialize enhanced components\n",
        "    enhanced_risk_manager = RiskManager(\n",
        "        max_position_size=0.8,\n",
        "        stop_loss=0.05,\n",
        "        take_profit=0.15,\n",
        "        max_drawdown=0.20\n",
        "    )\n",
        "    \n",
        "    enhanced_strategy = MultiFactorTradingStrategy(\n",
        "        initial_capital=100000,\n",
        "        risk_manager=enhanced_risk_manager\n",
        "    )\n",
        "    \n",
        "    enhanced_backtest = EnhancedBacktestEngine(\n",
        "        initial_capital=100000,\n",
        "        commission=0.001,  # 0.1% commission\n",
        "        slippage=0.0005,   # 0.05% slippage\n",
        "        bid_ask_spread=0.0002  # 0.02% spread\n",
        "    )\n",
        "    \n",
        "    # Generate enhanced signals using multi-factor ensemble\n",
        "    print(\"🧠 Generating multi-factor ensemble signals...\")\n",
        "    enhanced_signals = enhanced_strategy.generate_advanced_signals(demo_data)\n",
        "    \n",
        "    # Run enhanced backtest with realistic costs\n",
        "    print(\"📈 Running enhanced backtest with realistic trading costs...\")\n",
        "    enhanced_results = enhanced_backtest.run_backtest(demo_data, enhanced_signals)\n",
        "    \n",
        "    # Calculate enhanced performance metrics\n",
        "    enhanced_performance = benchmark_analyzer.calculate_portfolio_performance(enhanced_results)\n",
        "    \n",
        "    # Display enhanced results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯 ENHANCED MULTI-FACTOR STRATEGY RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"📊 Strategy Performance:\")\n",
        "    print(f\"  • Total Return: {enhanced_performance['total_return']:.2%}\")\n",
        "    print(f\"  • Annualized Return: {enhanced_performance['annualized_return']:.2%}\")\n",
        "    print(f\"  • Sharpe Ratio: {enhanced_performance['sharpe_ratio']:.2f}\")\n",
        "    print(f\"  • Max Drawdown: {enhanced_performance['max_drawdown']:.2%}\")\n",
        "    print(f\"  • Win Rate: {enhanced_performance['win_rate']:.2%}\")\n",
        "    print(f\"  • Profit Factor: {enhanced_performance['profit_factor']:.2f}\")\n",
        "    \n",
        "    print(f\"\\n📊 Advanced Risk Metrics:\")\n",
        "    print(f\"  • Sortino Ratio: {enhanced_performance['sortino_ratio']:.2f}\")\n",
        "    print(f\"  • Calmar Ratio: {enhanced_performance['calmar_ratio']:.2f}\")\n",
        "    print(f\"  • VaR (95%): {enhanced_performance['var_95']:.2%}\")\n",
        "    print(f\"  • CVaR (95%): {enhanced_performance['cvar_95']:.2%}\")\n",
        "    print(f\"  • Tail Ratio: {enhanced_performance['tail_ratio']:.2f}\")\n",
        "    \n",
        "    print(f\"\\n💰 Trading Costs Analysis:\")\n",
        "    trading_costs = enhanced_results['trading_costs']\n",
        "    print(f\"  • Total Commission: ${trading_costs['total_commission']:.2f}\")\n",
        "    print(f\"  • Total Slippage: ${trading_costs['total_slippage']:.2f}\")\n",
        "    print(f\"  • Total Spread: ${trading_costs['total_spread']:.2f}\")\n",
        "    print(f\"  • Total Costs: ${trading_costs['total_costs']:.2f}\")\n",
        "    print(f\"  • Cost Impact: {trading_costs['cost_percentage']:.2%}\")\n",
        "    \n",
        "    print(f\"\\n📈 Trade Statistics:\")\n",
        "    trades = enhanced_results['trades']\n",
        "    print(f\"  • Total Trades: {len(trades)}\")\n",
        "    print(f\"  • Buy Trades: {len([t for t in trades if t['action'] == 'BUY'])}\")\n",
        "    print(f\"  • Sell Trades: {len([t for t in trades if t['action'] == 'SELL'])}\")\n",
        "    \n",
        "    # Strategy quality assessment\n",
        "    print(f\"\\n🎯 Strategy Quality Assessment:\")\n",
        "    if enhanced_performance['sharpe_ratio'] > 1.5:\n",
        "        print(\"  ✅ Excellent Sharpe Ratio (>1.5)\")\n",
        "    elif enhanced_performance['sharpe_ratio'] > 1.0:\n",
        "        print(\"  ✅ Good Sharpe Ratio (>1.0)\")\n",
        "    elif enhanced_performance['sharpe_ratio'] > 0.5:\n",
        "        print(\"  ⚠️ Moderate Sharpe Ratio (>0.5)\")\n",
        "    else:\n",
        "        print(\"  ❌ Poor Sharpe Ratio (<0.5)\")\n",
        "    \n",
        "    if enhanced_performance['max_drawdown'] > -0.15:\n",
        "        print(\"  ✅ Acceptable Drawdown (<15%)\")\n",
        "    elif enhanced_performance['max_drawdown'] > -0.25:\n",
        "        print(\"  ⚠️ Moderate Drawdown (<25%)\")\n",
        "    else:\n",
        "        print(\"  ❌ High Drawdown (>25%)\")\n",
        "    \n",
        "    if enhanced_performance['win_rate'] > 0.55:\n",
        "        print(\"  ✅ Good Win Rate (>55%)\")\n",
        "    elif enhanced_performance['win_rate'] > 0.45:\n",
        "        print(\"  ⚠️ Moderate Win Rate (>45%)\")\n",
        "    else:\n",
        "        print(\"  ❌ Poor Win Rate (<45%)\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No processed data available. Please run the data collection and processing cells first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Performance Analysis (if model was trained)\n",
        "if 'history' in locals() and 'X_test' in locals() and 'y_test' in locals():\n",
        "    print(\"🤖 Creating model performance analysis...\")\n",
        "    \n",
        "    # Generate model performance analysis\n",
        "    model_fig, model_metrics = benchmark_analyzer.create_model_performance_analysis(\n",
        "        history, X_test, y_test, \"CNN+LSTM Model\"\n",
        "    )\n",
        "    \n",
        "    # Display the analysis\n",
        "    model_fig.show()\n",
        "    \n",
        "    # Print model metrics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🤖 MODEL PERFORMANCE METRICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"📊 Accuracy Metrics:\")\n",
        "    print(f\"  • MSE: {model_metrics['mse']:.6f}\")\n",
        "    print(f\"  • RMSE: {model_metrics['rmse']:.6f}\")\n",
        "    print(f\"  • MAE: {model_metrics['mae']:.6f}\")\n",
        "    print(f\"  • R² Score: {model_metrics['r2']:.4f}\")\n",
        "    \n",
        "    print(f\"\\n📈 Model Quality:\")\n",
        "    if model_metrics['r2'] > 0.7:\n",
        "        print(\"  ✅ Excellent model performance (R² > 0.7)\")\n",
        "    elif model_metrics['r2'] > 0.5:\n",
        "        print(\"  ✅ Good model performance (R² > 0.5)\")\n",
        "    elif model_metrics['r2'] > 0.3:\n",
        "        print(\"  ⚠️ Moderate model performance (R² > 0.3)\")\n",
        "    else:\n",
        "        print(\"  ❌ Poor model performance (R² < 0.3)\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No trained model found. Train a model first to see model performance analysis.\")\n",
        "    print(\"💡 Run the model training cell to generate model performance metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Technical Analysis and Comparisons\n",
        "print(\"📊 Creating additional technical analysis...\")\n",
        "\n",
        "# Create comparison with different strategies\n",
        "strategies = {\n",
        "    'CNN+LSTM Strategy': strategy_returns,\n",
        "    'Buy & Hold': benchmark_returns,\n",
        "    'Random Strategy': pd.Series(np.random.normal(0.0002, 0.025, len(strategy_returns)), index=strategy_returns.index)\n",
        "}\n",
        "\n",
        "# Calculate metrics for all strategies\n",
        "comparison_metrics = {}\n",
        "for name, returns in strategies.items():\n",
        "    comparison_metrics[name] = benchmark_analyzer.calculate_technical_metrics(returns, benchmark_returns)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for strategy_name, metrics in comparison_metrics.items():\n",
        "    comparison_data.append([\n",
        "        strategy_name,\n",
        "        f\"{metrics['total_return']:.2%}\",\n",
        "        f\"{metrics['annualized_return']:.2%}\",\n",
        "        f\"{metrics['volatility']:.2%}\",\n",
        "        f\"{metrics['sharpe_ratio']:.2f}\",\n",
        "        f\"{metrics['max_drawdown']:.2%}\",\n",
        "        f\"{metrics['win_rate']:.2%}\"\n",
        "    ])\n",
        "\n",
        "# Create comparison visualization\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=[\n",
        "        'Strategy Comparison Table',\n",
        "        'Risk-Return Scatter',\n",
        "        'Cumulative Returns Comparison',\n",
        "        'Rolling Sharpe Comparison'\n",
        "    ],\n",
        "    specs=[[{\"type\": \"table\"}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# 1. Comparison Table\n",
        "fig.add_trace(\n",
        "    go.Table(\n",
        "        header=dict(values=['Strategy', 'Total Return', 'Annual Return', 'Volatility', 'Sharpe', 'Max DD', 'Win Rate'],\n",
        "                   fill_color='lightblue'),\n",
        "        cells=dict(values=list(zip(*comparison_data)), fill_color='white')\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# 2. Risk-Return Scatter\n",
        "colors = ['blue', 'red', 'green']\n",
        "for i, (strategy_name, metrics) in enumerate(comparison_metrics.items()):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[metrics['volatility']], y=[metrics['annualized_return']],\n",
        "                  mode='markers', marker=dict(size=15, color=colors[i]),\n",
        "                  name=strategy_name),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "# 3. Cumulative Returns Comparison\n",
        "for i, (strategy_name, returns) in enumerate(strategies.items()):\n",
        "    cumulative = (1 + returns).cumprod()\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=returns.index, y=cumulative.values,\n",
        "                  name=strategy_name, line=dict(color=colors[i])),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# 4. Rolling Sharpe Comparison\n",
        "for i, (strategy_name, returns) in enumerate(strategies.items()):\n",
        "    rolling_sharpe = returns.rolling(window=252).mean() / returns.rolling(window=252).std() * np.sqrt(252)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=returns.index, y=rolling_sharpe.values,\n",
        "                  name=f'{strategy_name} Sharpe', line=dict(color=colors[i])),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=1000,\n",
        "    title_text=\"Strategy Comparison Analysis\",\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Print strategy ranking\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🏆 STRATEGY RANKING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Rank by Sharpe ratio\n",
        "ranked_strategies = sorted(comparison_metrics.items(), key=lambda x: x[1]['sharpe_ratio'], reverse=True)\n",
        "\n",
        "for i, (strategy_name, metrics) in enumerate(ranked_strategies):\n",
        "    rank_emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else \"📊\"\n",
        "    print(f\"{rank_emoji} #{i+1}: {strategy_name}\")\n",
        "    print(f\"   Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
        "    print(f\"   Annual Return: {metrics['annualized_return']:.2%}\")\n",
        "    print(f\"   Max Drawdown: {metrics['max_drawdown']:.2%}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Management and Versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model management and versioning utilities\n",
        "\n",
        "def list_saved_models():\n",
        "    \"\"\"List all saved models in different locations\"\"\"\n",
        "    \n",
        "    print(\"🔍 Searching for saved models...\")\n",
        "    \n",
        "    # Check local\n",
        "    if os.path.exists('cnn_lstm_model.h5'):\n",
        "        print(\"📁 Local: cnn_lstm_model.h5\")\n",
        "    \n",
        "    # Check Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        if os.path.exists(drive_folder):\n",
        "            drive_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            if drive_models:\n",
        "                print(f\"☁️ Google Drive ({len(drive_models)} models):\")\n",
        "                for model in sorted(drive_models):\n",
        "                    print(f\"   - {model}\")\n",
        "            else:\n",
        "                print(\"☁️ Google Drive: No models found\")\n",
        "        else:\n",
        "            print(\"☁️ Google Drive: Trading_Strategy_ML folder not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"☁️ Google Drive: Error accessing - {e}\")\n",
        "    \n",
        "    # Check GitHub repository\n",
        "    if os.path.exists('models'):\n",
        "        git_models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        if git_models:\n",
        "            print(f\"🐙 GitHub ({len(git_models)} models):\")\n",
        "            for model in sorted(git_models):\n",
        "                print(f\"   - {model}\")\n",
        "        else:\n",
        "            print(\"🐙 GitHub: No models found in models/ directory\")\n",
        "    else:\n",
        "        print(\"🐙 GitHub: models/ directory not found\")\n",
        "\n",
        "def create_model_backup():\n",
        "    \"\"\"Create a backup of the current model with metadata\"\"\"\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Create backup with metadata\n",
        "    backup_info = {\n",
        "        'timestamp': timestamp,\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'model_type': 'CNN+LSTM',\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'description': 'Trading Strategy ML Model'\n",
        "    }\n",
        "    \n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'model_metadata_{timestamp}.json', 'w') as f:\n",
        "        json.dump(backup_info, f, indent=2)\n",
        "    \n",
        "    print(f\"📋 Model metadata saved: model_metadata_{timestamp}.json\")\n",
        "    print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"🕒 Backup timestamp: {timestamp}\")\n",
        "\n",
        "def cleanup_old_models(keep_last_n=5):\n",
        "    \"\"\"Clean up old models, keeping only the last N versions\"\"\"\n",
        "    \n",
        "    print(f\"🧹 Cleaning up old models (keeping last {keep_last_n})...\")\n",
        "    \n",
        "    # Clean Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        \n",
        "        if os.path.exists(drive_folder):\n",
        "            models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            models.sort(reverse=True)  # Sort by name (newest first)\n",
        "            \n",
        "            if len(models) > keep_last_n:\n",
        "                models_to_delete = models[keep_last_n:]\n",
        "                for model in models_to_delete:\n",
        "                    os.remove(os.path.join(drive_folder, model))\n",
        "                    print(f\"🗑️ Deleted old model: {model}\")\n",
        "                print(f\"✅ Kept {keep_last_n} latest models in Google Drive\")\n",
        "            else:\n",
        "                print(f\"✅ Google Drive has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning Google Drive: {e}\")\n",
        "    \n",
        "    # Clean local models directory\n",
        "    if os.path.exists('models'):\n",
        "        models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        models.sort(reverse=True)\n",
        "        \n",
        "        if len(models) > keep_last_n:\n",
        "            models_to_delete = models[keep_last_n:]\n",
        "            for model in models_to_delete:\n",
        "                os.remove(os.path.join('models', model))\n",
        "                print(f\"🗑️ Deleted old model: {model}\")\n",
        "            print(f\"✅ Kept {keep_last_n} latest models locally\")\n",
        "        else:\n",
        "            print(f\"✅ Local models directory has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "\n",
        "# Run model management functions\n",
        "print(\"=\" * 50)\n",
        "print(\"📊 MODEL MANAGEMENT DASHBOARD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "list_saved_models()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "create_model_backup()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "cleanup_old_models(keep_last_n=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
