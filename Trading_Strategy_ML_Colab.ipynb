{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trading Strategy ML - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Multi-Factor Momentum Trading Strategy with ML Enhancement on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- Real-time data collection\n",
        "- Advanced ML models (CNN+LSTM)\n",
        "- Comprehensive backtesting\n",
        "- Performance analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Enable GPU memory growth\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        for gpu in tf.config.list_physical_devices('GPU'):\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q yfinance alpha-vantage pandas-datareader\n",
        "!pip install -q TA-Lib\n",
        "!pip install -q tensorflow-gpu\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q scikit-learn xgboost optuna lightgbm\n",
        "!pip install -q backtrader zipline-reloaded arch quantlib\n",
        "!pip install -q plotly matplotlib seaborn streamlit\n",
        "!pip install -q empyrical ffn pyfolio\n",
        "!pip install -q python-dotenv requests tqdm joblib\n",
        "\n",
        "print(\"All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/CatalinMoldova/trading-strategy-ml.git\n",
        "\n",
        "# Change to the project directory\n",
        "%cd trading-strategy-ml\n",
        "\n",
        "# Install project requirements\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "print(\"Repository cloned and requirements installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import project modules\n",
        "from data_pipeline.market_data_collector import MarketDataCollector\n",
        "from data_pipeline.indicator_engine import IndicatorEngine\n",
        "from data_pipeline.feature_engineer import FeatureEngineer\n",
        "from ml_models.cnn_lstm_model import CNNLSTMModel\n",
        "from ml_models.random_forest_model import RandomForestModel\n",
        "from ml_models.ensemble_predictor import EnsemblePredictor\n",
        "from strategy.signal_generator import SignalGenerator\n",
        "from strategy.position_sizer import PositionSizer\n",
        "from strategy.risk_manager import RiskManager\n",
        "from backtesting.backtest_engine import BacktestEngine\n",
        "from backtesting.performance_analyzer import PerformanceAnalyzer\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data collector\n",
        "collector = MarketDataCollector()\n",
        "\n",
        "# Define symbols to trade\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
        "\n",
        "# Collect historical data\n",
        "print(\"Collecting historical data...\")\n",
        "data = {}\n",
        "for symbol in symbols:\n",
        "    try:\n",
        "        df = collector.get_historical_data(symbol, period='2y', interval='1d')\n",
        "        data[symbol] = df\n",
        "        print(f\"‚úì {symbol}: {len(df)} records\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó {symbol}: Error - {e}\")\n",
        "\n",
        "print(f\"\\nData collection complete! Collected data for {len(data)} symbols.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN+LSTM model with GPU\n",
        "print(\"Training CNN+LSTM model...\")\n",
        "\n",
        "# Initialize model\n",
        "cnn_lstm = CNNLSTMModel(\n",
        "    time_steps=60,\n",
        "    n_features=20,  # Adjust based on your features\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Build model\n",
        "model = cnn_lstm.build_model()\n",
        "print(f\"Model built with {model.count_params()} parameters\")\n",
        "\n",
        "# Prepare training data (simplified example)\n",
        "# In practice, you'd use your actual processed data\n",
        "X_train = np.random.randn(1000, 60, 20)  # Example data\n",
        "y_train = np.random.randn(1000, 1)        # Example targets\n",
        "\n",
        "# Train model\n",
        "history = cnn_lstm.train(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Reduced for demo\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"CNN+LSTM training complete!\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Your Work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models to multiple locations for permanent storage\n",
        "\n",
        "# 1. Save locally first\n",
        "cnn_lstm.save_model('cnn_lstm_model.h5')\n",
        "print(\"‚úì Model saved locally\")\n",
        "\n",
        "# 2. Save to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your trading models\n",
        "drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Save model to Google Drive with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_model_path = f'{drive_folder}/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', drive_model_path)\n",
        "print(f\"‚úì Model saved to Google Drive: {drive_model_path}\")\n",
        "\n",
        "# 3. Save to GitHub (push to repository)\n",
        "import subprocess\n",
        "\n",
        "# Configure git (if not already done)\n",
        "subprocess.run(['git', 'config', '--global', 'user.email', 'your-email@example.com'], check=False)\n",
        "subprocess.run(['git', 'config', '--global', 'user.name', 'CatalinMoldova'], check=False)\n",
        "\n",
        "# Create models directory in git\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Copy model to git directory\n",
        "git_model_path = f'models/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', git_model_path)\n",
        "\n",
        "# Add, commit, and push to GitHub\n",
        "subprocess.run(['git', 'add', git_model_path], check=True)\n",
        "subprocess.run(['git', 'commit', '-m', f'Add trained CNN+LSTM model - {timestamp}'], check=True)\n",
        "subprocess.run(['git', 'push', 'origin', 'main'], check=True)\n",
        "print(f\"‚úì Model pushed to GitHub: {git_model_path}\")\n",
        "\n",
        "# 4. Download to local machine as backup\n",
        "from google.colab import files\n",
        "files.download('cnn_lstm_model.h5')\n",
        "print(\"‚úì Model downloaded to your local machine\")\n",
        "\n",
        "print(\"\\nüéâ Model saved in 4 locations:\")\n",
        "print(\"1. Local Colab environment\")\n",
        "print(\"2. Google Drive (permanent)\")\n",
        "print(\"3. GitHub repository (permanent)\")\n",
        "print(\"4. Your local machine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Saved Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models from different storage locations\n",
        "\n",
        "def load_model_from_location(location_type, model_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained model from different storage locations\n",
        "    \n",
        "    Args:\n",
        "        location_type: 'local', 'drive', 'github', or 'url'\n",
        "        model_path: Path to the model file (optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    if location_type == 'local':\n",
        "        # Load from local Colab environment\n",
        "        if model_path is None:\n",
        "            model_path = 'cnn_lstm_model.h5'\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"‚úì Model loaded from local: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'drive':\n",
        "        # Load from Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        if model_path is None:\n",
        "            # List available models in Drive\n",
        "            drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "            if os.path.exists(drive_folder):\n",
        "                models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(drive_folder, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in Google Drive\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Trading_Strategy_ML folder not found in Google Drive\")\n",
        "                return None\n",
        "        \n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"‚úì Model loaded from Google Drive: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'github':\n",
        "        # Load from GitHub (if model is in repository)\n",
        "        if model_path is None:\n",
        "            models_dir = 'models'\n",
        "            if os.path.exists(models_dir):\n",
        "                models = [f for f in os.listdir(models_dir) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(models_dir, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in GitHub repository\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Models directory not found\")\n",
        "                return None\n",
        "        \n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"‚úì Model loaded from GitHub: {model_path}\")\n",
        "        \n",
        "    elif location_type == 'url':\n",
        "        # Load from URL (if you have a direct link)\n",
        "        if model_path is None:\n",
        "            print(\"Please provide a URL to the model file\")\n",
        "            return None\n",
        "        \n",
        "        import urllib.request\n",
        "        local_path = 'downloaded_model.h5'\n",
        "        urllib.request.urlretrieve(model_path, local_path)\n",
        "        model = tf.keras.models.load_model(local_path)\n",
        "        print(f\"‚úì Model loaded from URL: {model_path}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"Invalid location_type. Use 'local', 'drive', 'github', or 'url'\")\n",
        "        return None\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Example: Load the latest model from Google Drive\n",
        "print(\"Loading model from Google Drive...\")\n",
        "loaded_model = load_model_from_location('drive')\n",
        "\n",
        "if loaded_model is not None:\n",
        "    print(f\"Model summary:\")\n",
        "    loaded_model.summary()\n",
        "else:\n",
        "    print(\"No model found. Train a model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Management and Versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model management and versioning utilities\n",
        "\n",
        "def list_saved_models():\n",
        "    \"\"\"List all saved models in different locations\"\"\"\n",
        "    \n",
        "    print(\"üîç Searching for saved models...\")\n",
        "    \n",
        "    # Check local\n",
        "    if os.path.exists('cnn_lstm_model.h5'):\n",
        "        print(\"üìÅ Local: cnn_lstm_model.h5\")\n",
        "    \n",
        "    # Check Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        if os.path.exists(drive_folder):\n",
        "            drive_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            if drive_models:\n",
        "                print(f\"‚òÅÔ∏è Google Drive ({len(drive_models)} models):\")\n",
        "                for model in sorted(drive_models):\n",
        "                    print(f\"   - {model}\")\n",
        "            else:\n",
        "                print(\"‚òÅÔ∏è Google Drive: No models found\")\n",
        "        else:\n",
        "            print(\"‚òÅÔ∏è Google Drive: Trading_Strategy_ML folder not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚òÅÔ∏è Google Drive: Error accessing - {e}\")\n",
        "    \n",
        "    # Check GitHub repository\n",
        "    if os.path.exists('models'):\n",
        "        git_models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        if git_models:\n",
        "            print(f\"üêô GitHub ({len(git_models)} models):\")\n",
        "            for model in sorted(git_models):\n",
        "                print(f\"   - {model}\")\n",
        "        else:\n",
        "            print(\"üêô GitHub: No models found in models/ directory\")\n",
        "    else:\n",
        "        print(\"üêô GitHub: models/ directory not found\")\n",
        "\n",
        "def create_model_backup():\n",
        "    \"\"\"Create a backup of the current model with metadata\"\"\"\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Create backup with metadata\n",
        "    backup_info = {\n",
        "        'timestamp': timestamp,\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'model_type': 'CNN+LSTM',\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'description': 'Trading Strategy ML Model'\n",
        "    }\n",
        "    \n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'model_metadata_{timestamp}.json', 'w') as f:\n",
        "        json.dump(backup_info, f, indent=2)\n",
        "    \n",
        "    print(f\"üìã Model metadata saved: model_metadata_{timestamp}.json\")\n",
        "    print(f\"üìä TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"üïí Backup timestamp: {timestamp}\")\n",
        "\n",
        "def cleanup_old_models(keep_last_n=5):\n",
        "    \"\"\"Clean up old models, keeping only the last N versions\"\"\"\n",
        "    \n",
        "    print(f\"üßπ Cleaning up old models (keeping last {keep_last_n})...\")\n",
        "    \n",
        "    # Clean Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        \n",
        "        if os.path.exists(drive_folder):\n",
        "            models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            models.sort(reverse=True)  # Sort by name (newest first)\n",
        "            \n",
        "            if len(models) > keep_last_n:\n",
        "                models_to_delete = models[keep_last_n:]\n",
        "                for model in models_to_delete:\n",
        "                    os.remove(os.path.join(drive_folder, model))\n",
        "                    print(f\"üóëÔ∏è Deleted old model: {model}\")\n",
        "                print(f\"‚úÖ Kept {keep_last_n} latest models in Google Drive\")\n",
        "            else:\n",
        "                print(f\"‚úÖ Google Drive has {len(models)} models (‚â§ {keep_last_n}, no cleanup needed)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cleaning Google Drive: {e}\")\n",
        "    \n",
        "    # Clean local models directory\n",
        "    if os.path.exists('models'):\n",
        "        models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        models.sort(reverse=True)\n",
        "        \n",
        "        if len(models) > keep_last_n:\n",
        "            models_to_delete = models[keep_last_n:]\n",
        "            for model in models_to_delete:\n",
        "                os.remove(os.path.join('models', model))\n",
        "                print(f\"üóëÔ∏è Deleted old model: {model}\")\n",
        "            print(f\"‚úÖ Kept {keep_last_n} latest models locally\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Local models directory has {len(models)} models (‚â§ {keep_last_n}, no cleanup needed)\")\n",
        "\n",
        "# Run model management functions\n",
        "print(\"=\" * 50)\n",
        "print(\"üìä MODEL MANAGEMENT DASHBOARD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "list_saved_models()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "create_model_backup()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "cleanup_old_models(keep_last_n=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
