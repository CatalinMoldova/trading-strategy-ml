{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CatalinMoldova/trading-strategy-ml/blob/main/Trading_Strategy_ML_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN8NtChgEuMq"
      },
      "source": [
        "# Trading Strategy ML - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs the Multi-Factor Momentum Trading Strategy with ML Enhancement on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- Real-time data collection\n",
        "- Advanced ML models (CNN+LSTM)\n",
        "- Comprehensive backtesting\n",
        "- Performance analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHyTw17kEuMr"
      },
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-fyMpC3EuMr",
        "outputId": "9cda9bf6-515a-4490-fb21-1a653cb00162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# Enable GPU memory growth\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    try:\n",
        "        for gpu in tf.config.list_physical_devices('GPU'):\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU memory growth error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3evn_eRBEuMr",
        "outputId": "4736f7c5-d430-49de-896b-c9f821611f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q yfinance alpha-vantage pandas-datareader\n",
        "!pip install -q TA-Lib\n",
        "!pip install -q tensorflow-gpu\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q scikit-learn xgboost optuna lightgbm\n",
        "!pip install -q backtrader zipline-reloaded arch quantlib\n",
        "!pip install -q plotly matplotlib seaborn streamlit\n",
        "!pip install -q empyrical ffn pyfolio\n",
        "!pip install -q python-dotenv requests tqdm joblib redis\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUBXSDVZEuMr",
        "outputId": "b2ebf2d9-0d1e-4a19-991c-607b58e13a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'trading-strategy-ml'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 62 (delta 6), reused 59 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (62/62), 173.87 KiB | 6.00 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/trading-strategy-ml\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 7)) (1.16.1)\n",
            "Requirement already satisfied: yfinance>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 10)) (0.2.65)\n",
            "Requirement already satisfied: alpha-vantage>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pandas-datareader>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: TA-Lib>=0.4.25 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 15)) (0.6.7)\n",
            "Collecting tensorflow-gpu>=2.10.0 (from -r requirements_colab.txt (line 18))\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Repository cloned and requirements installed!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository (replace with your GitHub URL)\n",
        "!git clone https://github.com/CatalinMoldova/trading-strategy-ml.git\n",
        "\n",
        "# Change to the project directory\n",
        "%cd trading-strategy-ml\n",
        "\n",
        "# Install project requirements\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "print(\"Repository cloned and requirements installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jn1bOREEuMr"
      },
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "KwJy05TqEuMr",
        "outputId": "f7e94c83-6b37-4f37-9675-558499ef8a24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'redis'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4169353624.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Import project modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarket_data_collector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarketDataCollector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicator_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndicatorEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_engineer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureEngineer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/trading-strategy-ml/src/data_pipeline/market_data_collector.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malpha_vantage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Configure logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'redis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project src to path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Import project modules\n",
        "from data_pipeline.market_data_collector import MarketDataCollector\n",
        "from data_pipeline.indicator_engine import IndicatorEngine\n",
        "from data_pipeline.feature_engineer import FeatureEngineer\n",
        "from ml_models.cnn_lstm_model import CNNLSTMModel\n",
        "from ml_models.random_forest_model import RandomForestModel\n",
        "from ml_models.ensemble_predictor import EnsemblePredictor\n",
        "from strategy.signal_generator import SignalGenerator\n",
        "from strategy.position_sizer import PositionSizer\n",
        "from strategy.risk_manager import RiskManager\n",
        "from backtesting.backtest_engine import BacktestEngine\n",
        "from backtesting.performance_analyzer import PerformanceAnalyzer\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stBRwePWEuMs"
      },
      "source": [
        "## 3. Data Collection and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGEAg3wuEuMs"
      },
      "outputs": [],
      "source": [
        "# Initialize data collector\n",
        "collector = MarketDataCollector()\n",
        "\n",
        "# Define symbols to trade\n",
        "symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'NFLX']\n",
        "\n",
        "# Collect historical data\n",
        "print(\"Collecting historical data...\")\n",
        "data = {}\n",
        "for symbol in symbols:\n",
        "    try:\n",
        "        df = collector.get_historical_data(symbol, period='2y', interval='1d')\n",
        "        data[symbol] = df\n",
        "        print(f\"✓ {symbol}: {len(df)} records\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ {symbol}: Error - {e}\")\n",
        "\n",
        "print(f\"\\nData collection complete! Collected data for {len(data)} symbols.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7R2ziJHEuMs"
      },
      "source": [
        "## 4. Model Training with GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4B6xH9QEuMs"
      },
      "outputs": [],
      "source": [
        "# Train CNN+LSTM model with GPU\n",
        "print(\"Training CNN+LSTM model...\")\n",
        "\n",
        "# Initialize model\n",
        "cnn_lstm = CNNLSTMModel(\n",
        "    time_steps=60,\n",
        "    n_features=20,  # Adjust based on your features\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Build model\n",
        "model = cnn_lstm.build_model()\n",
        "print(f\"Model built with {model.count_params()} parameters\")\n",
        "\n",
        "# Prepare training data (simplified example)\n",
        "# In practice, you'd use your actual processed data\n",
        "X_train = np.random.randn(1000, 60, 20)  # Example data\n",
        "y_train = np.random.randn(1000, 1)        # Example targets\n",
        "\n",
        "# Train model\n",
        "history = cnn_lstm.train(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Reduced for demo\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"CNN+LSTM training complete!\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktOby3UFEuMs"
      },
      "source": [
        "## 5. Save Your Work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90fP77RpEuMs"
      },
      "outputs": [],
      "source": [
        "# Save models to multiple locations for permanent storage\n",
        "\n",
        "# 1. Save locally first\n",
        "cnn_lstm.save_model('cnn_lstm_model.h5')\n",
        "print(\"✓ Model saved locally\")\n",
        "\n",
        "# 2. Save to Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your trading models\n",
        "drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# Save model to Google Drive with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_model_path = f'{drive_folder}/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', drive_model_path)\n",
        "print(f\"✓ Model saved to Google Drive: {drive_model_path}\")\n",
        "\n",
        "# 3. Save to GitHub (push to repository)\n",
        "import subprocess\n",
        "\n",
        "# Configure git (if not already done)\n",
        "subprocess.run(['git', 'config', '--global', 'user.email', 'your-email@example.com'], check=False)\n",
        "subprocess.run(['git', 'config', '--global', 'user.name', 'CatalinMoldova'], check=False)\n",
        "\n",
        "# Create models directory in git\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Copy model to git directory\n",
        "git_model_path = f'models/cnn_lstm_model_{timestamp}.h5'\n",
        "shutil.copy('cnn_lstm_model.h5', git_model_path)\n",
        "\n",
        "# Add, commit, and push to GitHub\n",
        "subprocess.run(['git', 'add', git_model_path], check=True)\n",
        "subprocess.run(['git', 'commit', '-m', f'Add trained CNN+LSTM model - {timestamp}'], check=True)\n",
        "subprocess.run(['git', 'push', 'origin', 'main'], check=True)\n",
        "print(f\"✓ Model pushed to GitHub: {git_model_path}\")\n",
        "\n",
        "# 4. Download to local machine as backup\n",
        "from google.colab import files\n",
        "files.download('cnn_lstm_model.h5')\n",
        "print(\"✓ Model downloaded to your local machine\")\n",
        "\n",
        "print(\"\\n🎉 Model saved in 4 locations:\")\n",
        "print(\"1. Local Colab environment\")\n",
        "print(\"2. Google Drive (permanent)\")\n",
        "print(\"3. GitHub repository (permanent)\")\n",
        "print(\"4. Your local machine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7WA4AT7EuMs"
      },
      "source": [
        "## 6. Load Saved Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYpaKzqXEuMs"
      },
      "outputs": [],
      "source": [
        "# Load models from different storage locations\n",
        "\n",
        "def load_model_from_location(location_type, model_path=None):\n",
        "    \"\"\"\n",
        "    Load a trained model from different storage locations\n",
        "\n",
        "    Args:\n",
        "        location_type: 'local', 'drive', 'github', or 'url'\n",
        "        model_path: Path to the model file (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    if location_type == 'local':\n",
        "        # Load from local Colab environment\n",
        "        if model_path is None:\n",
        "            model_path = 'cnn_lstm_model.h5'\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from local: {model_path}\")\n",
        "\n",
        "    elif location_type == 'drive':\n",
        "        # Load from Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        if model_path is None:\n",
        "            # List available models in Drive\n",
        "            drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "            if os.path.exists(drive_folder):\n",
        "                models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(drive_folder, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in Google Drive\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Trading_Strategy_ML folder not found in Google Drive\")\n",
        "                return None\n",
        "\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from Google Drive: {model_path}\")\n",
        "\n",
        "    elif location_type == 'github':\n",
        "        # Load from GitHub (if model is in repository)\n",
        "        if model_path is None:\n",
        "            models_dir = 'models'\n",
        "            if os.path.exists(models_dir):\n",
        "                models = [f for f in os.listdir(models_dir) if f.endswith('.h5')]\n",
        "                if models:\n",
        "                    model_path = os.path.join(models_dir, models[-1])  # Load latest\n",
        "                    print(f\"Available models: {models}\")\n",
        "                else:\n",
        "                    print(\"No models found in GitHub repository\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Models directory not found\")\n",
        "                return None\n",
        "\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"✓ Model loaded from GitHub: {model_path}\")\n",
        "\n",
        "    elif location_type == 'url':\n",
        "        # Load from URL (if you have a direct link)\n",
        "        if model_path is None:\n",
        "            print(\"Please provide a URL to the model file\")\n",
        "            return None\n",
        "\n",
        "        import urllib.request\n",
        "        local_path = 'downloaded_model.h5'\n",
        "        urllib.request.urlretrieve(model_path, local_path)\n",
        "        model = tf.keras.models.load_model(local_path)\n",
        "        print(f\"✓ Model loaded from URL: {model_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid location_type. Use 'local', 'drive', 'github', or 'url'\")\n",
        "        return None\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example: Load the latest model from Google Drive\n",
        "print(\"Loading model from Google Drive...\")\n",
        "loaded_model = load_model_from_location('drive')\n",
        "\n",
        "if loaded_model is not None:\n",
        "    print(f\"Model summary:\")\n",
        "    loaded_model.summary()\n",
        "else:\n",
        "    print(\"No model found. Train a model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8PCTg9VEuMt"
      },
      "source": [
        "## 7. Model Management and Versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q7AeMIOEuMt"
      },
      "outputs": [],
      "source": [
        "# Model management and versioning utilities\n",
        "\n",
        "def list_saved_models():\n",
        "    \"\"\"List all saved models in different locations\"\"\"\n",
        "\n",
        "    print(\"🔍 Searching for saved models...\")\n",
        "\n",
        "    # Check local\n",
        "    if os.path.exists('cnn_lstm_model.h5'):\n",
        "        print(\"📁 Local: cnn_lstm_model.h5\")\n",
        "\n",
        "    # Check Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "        if os.path.exists(drive_folder):\n",
        "            drive_models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            if drive_models:\n",
        "                print(f\"☁️ Google Drive ({len(drive_models)} models):\")\n",
        "                for model in sorted(drive_models):\n",
        "                    print(f\"   - {model}\")\n",
        "            else:\n",
        "                print(\"☁️ Google Drive: No models found\")\n",
        "        else:\n",
        "            print(\"☁️ Google Drive: Trading_Strategy_ML folder not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"☁️ Google Drive: Error accessing - {e}\")\n",
        "\n",
        "    # Check GitHub repository\n",
        "    if os.path.exists('models'):\n",
        "        git_models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        if git_models:\n",
        "            print(f\"🐙 GitHub ({len(git_models)} models):\")\n",
        "            for model in sorted(git_models):\n",
        "                print(f\"   - {model}\")\n",
        "        else:\n",
        "            print(\"🐙 GitHub: No models found in models/ directory\")\n",
        "    else:\n",
        "        print(\"🐙 GitHub: models/ directory not found\")\n",
        "\n",
        "def create_model_backup():\n",
        "    \"\"\"Create a backup of the current model with metadata\"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Create backup with metadata\n",
        "    backup_info = {\n",
        "        'timestamp': timestamp,\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'model_type': 'CNN+LSTM',\n",
        "        'training_date': datetime.now().isoformat(),\n",
        "        'description': 'Trading Strategy ML Model'\n",
        "    }\n",
        "\n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'model_metadata_{timestamp}.json', 'w') as f:\n",
        "        json.dump(backup_info, f, indent=2)\n",
        "\n",
        "    print(f\"📋 Model metadata saved: model_metadata_{timestamp}.json\")\n",
        "    print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"🕒 Backup timestamp: {timestamp}\")\n",
        "\n",
        "def cleanup_old_models(keep_last_n=5):\n",
        "    \"\"\"Clean up old models, keeping only the last N versions\"\"\"\n",
        "\n",
        "    print(f\"🧹 Cleaning up old models (keeping last {keep_last_n})...\")\n",
        "\n",
        "    # Clean Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_folder = '/content/drive/MyDrive/Trading_Strategy_ML'\n",
        "\n",
        "        if os.path.exists(drive_folder):\n",
        "            models = [f for f in os.listdir(drive_folder) if f.endswith('.h5')]\n",
        "            models.sort(reverse=True)  # Sort by name (newest first)\n",
        "\n",
        "            if len(models) > keep_last_n:\n",
        "                models_to_delete = models[keep_last_n:]\n",
        "                for model in models_to_delete:\n",
        "                    os.remove(os.path.join(drive_folder, model))\n",
        "                    print(f\"🗑️ Deleted old model: {model}\")\n",
        "                print(f\"✅ Kept {keep_last_n} latest models in Google Drive\")\n",
        "            else:\n",
        "                print(f\"✅ Google Drive has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning Google Drive: {e}\")\n",
        "\n",
        "    # Clean local models directory\n",
        "    if os.path.exists('models'):\n",
        "        models = [f for f in os.listdir('models') if f.endswith('.h5')]\n",
        "        models.sort(reverse=True)\n",
        "\n",
        "        if len(models) > keep_last_n:\n",
        "            models_to_delete = models[keep_last_n:]\n",
        "            for model in models_to_delete:\n",
        "                os.remove(os.path.join('models', model))\n",
        "                print(f\"🗑️ Deleted old model: {model}\")\n",
        "            print(f\"✅ Kept {keep_last_n} latest models locally\")\n",
        "        else:\n",
        "            print(f\"✅ Local models directory has {len(models)} models (≤ {keep_last_n}, no cleanup needed)\")\n",
        "\n",
        "# Run model management functions\n",
        "print(\"=\" * 50)\n",
        "print(\"📊 MODEL MANAGEMENT DASHBOARD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "list_saved_models()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "create_model_backup()\n",
        "print(\"\\n\" + \"=\" * 30)\n",
        "cleanup_old_models(keep_last_n=3)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}